<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: javascript | James Thomas]]></title>
  <link href="http://jthomas.github.com/jthomas/blog/categories/javascript/atom.xml" rel="self"/>
  <link href="http://jthomas.github.com/jthomas/"/>
  <updated>2018-08-13T17:21:19+01:00</updated>
  <id>http://jthomas.github.com/jthomas/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Serverless Machine Learning With TensorFlow.js]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/"/>
    <updated>2018-08-13T12:16:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js</id>
    <content type="html"><![CDATA[<p>In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to use <a href="https://js.tensorflow.org/">TensorFlow.js</a> on Node.js to run <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d#file-script-js">visual recognition on images from the local filesystem</a>. TensorFlow.js is a JavaScript version of the open-source machine learning library from Google.</p>

<p>Once I had this working with a local Node.js script, my next idea was to convert it into a serverless function. Running this function on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> (<a href="https://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) would turn the script into my own visual recognition microservice.</p>

<p><img src="/images/tfjs-serverless/tf-js-example.gif" title="Serverless TensorFlow.js Function" ></p>

<p>Sounds easy, right? It's just a JavaScript library? So, zip it up and away we go... <strong><em>ahem</em></strong> 👊</p>

<p><em>Converting the image classification script to run in a serverless environment had the following challenges...</em></p>

<ul>
<li><strong>TensorFlow.js libraries need to be available in the runtime.</strong></li>
<li><strong>Native bindings for the library must be compiled against the platform architecture.</strong></li>
<li><strong>Models files need to be loaded from the filesystem.</strong></li>
</ul>


<p>Some of these issues were more challenging than others to fix! Let's start by looking at the details of each issue, before explaining how <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">Docker support</a> in Apache OpenWhisk can be used to resolve them all.</p>

<h2>Challenges</h2>

<h3>TensorFlow.js Libraries</h3>

<p>TensorFlow.js libraries are not included in the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js runtimes</a> provided by the Apache OpenWhisk.</p>

<p>External libraries <a href="http://jamesthom.as/blog/2016/11/28/npm-modules-in-openwhisk/">can be imported</a> into the runtime by deploying applications from a zip file. Custom <code>node_modules</code> folders included in the zip file will be extracted in the runtime. Zip files are limited to a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">maximum size of 48MB</a>.</p>

<h4>Library Size</h4>

<p>Running <code>npm install</code> for the TensorFlow.js libraries used revealed the first problem... the resulting <code>node_modules</code> directory was 175MB. 😱</p>

<p>Looking at the contents of this folder, the <code>tfjs-node</code> module compiles a <a href="https://github.com/tensorflow/tfjs-node/tree/master/src">native shared library</a> (<code>libtensorflow.so</code>) that is 135M. This means no amount of JavaScript minification is going to get those external dependencies under the magic 48 MB limit. 👎</p>

<h4>Native Dependencies</h4>

<p>The <code>libtensorflow.so</code> native shared library must be compiled using the platform runtime. Running <code>npm install</code>  locally automatically compiles native dependencies against the host platform. Local environments may use different CPU architectures (Mac vs Linux) or link against shared libraries not available in the serverless runtime.</p>

<h3>MobileNet Model Files</h3>

<p>TensorFlow models files <a href="https://js.tensorflow.org/tutorials/model-save-load.html">need loading from the filesystem</a> in Node.js. Serverless runtimes do provide a temporary filesystem inside the runtime environment. Files from deployment zip files are automatically extracted into this environment before invocations. There is no external access to this filesystem outside the lifecycle of the serverless function.</p>

<p>Models files for the MobileNet model were 16MB. If these files are included in the deployment package, it leaves 32MB for the rest of the application source code. Although the model files are small enough to include in the zip file, what about the TensorFlow.js libraries? Is this the end of the blog post? Not so fast....</p>

<p><strong>Apache OpenWhisk's support for custom runtimes provides a simple solution to all these issues!</strong></p>

<h2>Custom Runtimes</h2>

<p>Apache OpenWhisk uses Docker containers as the runtime environments for serverless functions (actions). All platform runtime images are <a href="https://hub.docker.com/r/openwhisk/">published on Docker Hub</a>, allowing developers to start these environments locally.</p>

<p>Developers can also <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">specify custom runtime images</a> when creating actions. These images must be publicly available on Docker Hub. Custom runtimes have to expose the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">same HTTP API</a> used by the platform for invoking actions.</p>

<p>Using platform runtime images as <a href="https://docs.docker.com/glossary/?term=parent%20image">parent images</a> makes it simple to build custom runtimes. Users can run commands during the Docker build to install additional libraries and other dependencies. The parent image already contains source files with the HTTP API service handling platform requests.</p>

<h3>TensorFlow.js Runtime</h3>

<p>Here is the Docker build file for the Node.js action runtime with additional TensorFlow.js dependencies.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs @tensorflow-models/mobilenet @tensorflow/tfjs-node jpeg-js</p>

<p>COPY mobilenet mobilenet
```</p>

<p><code>openwhisk/action-nodejs-v8:latest</code> is the Node.js action runtime image <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">published by OpenWhisk</a>.</p>

<p>TensorFlow libraries and other dependencies are installed using <code>npm install</code> in the build process. Native dependencies for the <code>@tensorflow/tfjs-node</code> library are automatically compiled for the correct platform by installing during the build process.</p>

<p>Since I'm building a new runtime, I've also added the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model files</a> to the image. Whilst not strictly necessary, removing them from the action zip file reduces deployment times.</p>

<p><strong><em>Want to skip the next step? Use this image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/"><code>jamesthomas/action-nodejs-v8:tfjs</code></a> rather than building your own.</em></strong></p>

<h3>Building The Runtime</h3>

<p><em>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to download model files from the public storage bucket.</em></p>

<ul>
<li>Download a version of the MobileNet model and place all files in the <code>mobilenet</code> directory.</li>
<li>Copy the Docker build file from above to a local file named <code>Dockerfile</code>.</li>
<li>Run the Docker <a href="https://docs.docker.com/engine/reference/commandline/build/">build command</a> to generate a local image.</li>
</ul>


<p><code>sh
docker build -t tfjs .
</code></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/tag/">Tag the local image</a> with a remote username and repository.</li>
</ul>


<p><code>sh
docker tag tfjs &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p><em>Replace <code>&lt;USERNAME&gt;</code> with your Docker Hub username.</em></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/push/">Push the local image</a> to Docker Hub</li>
</ul>


<p><code>sh
 docker push &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p>Once the image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/">is available</a> on Docker Hub, actions can be created using that runtime image. 😎</p>

<h2>Example Code</h2>

<p>This source code implements image classification as an OpenWhisk action. Image files are provided as a Base64 encoded string using the <code>image</code> property on the event parameters. Classification results are returned as the <code>results</code> property in the response.</p>

<script src="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5.js"></script>


<h3>Caching Loaded Models</h3>

<p>Serverless platforms initialise runtime environments on-demand to handle invocations. Once a runtime environment has been created, it will be <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">re-used for further invocations</a> with some limits. This improves performance by removing the initialisation delay ("cold start") from request processing.</p>

<p>Applications can exploit this behaviour by using global variables to maintain state across requests. This is often use to <a href="https://blog.rowanudell.com/database-connections-in-lambda/">cache opened database connections</a> or store initialisation data loaded from external systems.</p>

<p>I have used this pattern to <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L80-L82">cache the MobileNet model</a> used for classification. During cold invocations, the model is loaded from the filesystem and stored in a global variable. Warm invocations then use the existence of that global variable to skip the model loading process with further requests.</p>

<p>Caching the model reduces the time (and therefore cost) for classifications on warm invocations.</p>

<h3>Memory Leak</h3>

<p>Running the Node.js script from blog post on IBM Cloud Functions was possible with minimal modifications. Unfortunately, performance testing revealed a memory leak in the handler function. 😢</p>

<p><em>Reading more about <a href="https://js.tensorflow.org/tutorials/core-concepts.html">how TensorFlow.js works</a> on Node.js uncovered the issue...</em></p>

<p>TensorFlow.js's Node.js extensions use a native C++ library to execute the Tensors on a CPU or GPU engine. Memory allocated for Tensor objects in the native library is retained until the application explicitly releases it or the process exits. TensorFlow.js provides a <code>dispose</code> method on the individual objects to free allocated memory. There is also a <code>tf.tidy</code> method to automatically clean up all allocated objects within a frame.</p>

<p>Reviewing the code, tensors were being created as <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L51-L59">model input from images</a> on each request. These objects were not disposed before returning from the request handler. This meant native memory grew unbounded. Adding an explicit <code>dispose</code> call to free these objects before returning <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L91">fixed the issue</a>.</p>

<h3>Profiling &amp; Performance</h3>

<p>Action code records memory usage and elapsed time at different stages in classification process.</p>

<p>Recording <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L12-L20">memory usage</a> allows me to modify the maximum memory allocated to the function for optimal performance and cost. Node.js provides a <a href="https://nodejs.org/docs/v0.4.11/api/all.html#process.memoryUsage">standard library API</a> to retrieve memory usage for the current process. Logging these values allows me to inspect memory usage at different stages.</p>

<p>Timing <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L71">different tasks</a> in the classification process, i.e. model loading, image classification, gives me an insight into how efficient classification is compared to other methods. Node.js has a <a href="https://nodejs.org/api/console.html#console_console_time_label">standard library API</a> for timers to record and print elapsed time to the console.</p>

<h2>Demo</h2>

<h3>Deploy Action</h3>

<ul>
<li>Run the following command with the <a href="https://console.bluemix.net/openwhisk/learn/cli">IBM Cloud CLI</a> to create the action.</li>
</ul>


<p><code>sh
ibmcloud fn action create classify --docker &lt;IMAGE_NAME&gt; index.js
</code></p>

<p><em>Replace <code>&lt;IMAGE_NAME&gt;</code> with the public Docker Hub image identifier for the custom runtime. Use <code>jamesthomas/action-nodejs-v8:tfjs</code> if you haven't built this manually.</em></p>

<h3>Testing It Out</h3>

<ul>
<li>Download <a href="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG">this image</a> of a Panda from Wikipedia.</li>
</ul>


<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG"></p>

<p><code>sh
wget http://bit.ly/2JYSal9 -O panda.jpg
</code></p>

<ul>
<li>Invoke the action with the Base64 encoded image as an input parameter.</li>
</ul>


<p><code>sh
 ibmcloud fn action invoke classify -r -p image $(base64 panda.jpg)
</code></p>

<ul>
<li>Returned JSON message contains classification probabilities. 🐼🐼🐼</li>
</ul>


<p>```json
{
  "results":  [{</p>

<pre><code>className: 'giant panda, panda, panda bear, coon bear',
probability: 0.9993536472320557
</code></pre>

<p>  }]
}
```</p>

<h3>Activation Details</h3>

<ul>
<li>Retrieve logging output for the last activation to show performance data.</li>
</ul>


<p><code>sh
ibmcloud fn activation logs --last
</code></p>

<p><strong><em>Profiling and memory usage details are logged to stdout</em></strong></p>

<p><code>sh
prediction function called.
memory used: rss=150.46 MB, heapTotal=32.83 MB, heapUsed=20.29 MB, external=67.6 MB
loading image and model...
decodeImage: 74.233ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=40.63 MB
imageByteArray: 5.676ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=45.51 MB
imageToInput: 5.952ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.06 MB, external=45.51 MB
mn_model.classify: 274.805ms
memory used: rss=149.83 MB, heapTotal=24.33 MB, heapUsed=20.57 MB, external=45.51 MB
classification results: [...]
main: 356.639ms
memory used: rss=144.37 MB, heapTotal=24.33 MB, heapUsed=20.58 MB, external=45.51 MB
</code></p>

<p><code>main</code> is the total elapsed time for the action handler. <code>mn_model.classify</code> is the elapsed time for the image classification. Cold start requests print an extra log message with model loading time, <code>loadModel: 394.547ms</code>.</p>

<h2>Performance Results</h2>

<p>Invoking the <code>classify</code> action 1000 times for both cold and warm activations (using 256MB memory) generated the following performance results.</p>

<h3>warm invocations</h3>

<p><img src="/images/tfjs-serverless/warm-activations.png" title="Warm Activation Performance Results" ></p>

<p>Classifications took an average of <strong>316 milliseconds to process when using warm environments</strong>. Looking at the timing data, converting the Base64 encoded JPEG into the input tensor took around 100 milliseconds. Running the model classification task was in the 200 - 250 milliseconds range.</p>

<h3>cold invocations</h3>

<p><img src="/images/tfjs-serverless/cold-activations.png" title="Cold Activation Performance Results" ></p>

<p>Classifications took an average of <strong>1260 milliseconds to process when using cold environments</strong>. These requests incur penalties for initialising new runtime containers and loading models from the filesystem. Both of these tasks took around 400 milliseconds each.</p>

<p><em>One disadvantage of using custom runtime images in Apache OpenWhisk is the lack of <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">pre-warmed containers</a>. Pre-warming is used to reduce cold start times by starting runtime containers before they are needed. This is not supported for non-standard runtime images.</em></p>

<h3>classification cost</h3>

<p>IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">provides a free tier</a> of 400,000 GB/s per month. Each further second of execution is charged at $0.000017 per GB of memory allocated. Execution time is rounded up to the nearest 100ms.</p>

<p>If all activations were warm, a user could execute <strong>more than 4,000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 600,000 further invocations would cost just over $1.</p>

<p>If all activations were cold, a user could execute <strong>more than 1,2000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 180,000 further invocations would cost just over $1.</p>

<h2>Conclusion</h2>

<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>

<p>Getting a local script to run image classification was relatively simple, but converting to a serverless function came with more challenges! Apache OpenWhisk restricts the maximum application size to 50MB and native libraries dependencies were much larger than this limit.</p>

<p>Fortunately, Apache OpenWhisk's custom runtime support allowed us to resolve all these issues. By building a custom runtime with native dependencies and models files, those libraries can be used on the platform without including them in the deployment package.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Machine Learning In Node.js With TensorFlow.js]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/"/>
    <updated>2018-08-07T09:52:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js</id>
    <content type="html"><![CDATA[<p><a href="https://js.tensorflow.org/">TensorFlow.js</a> is a new version of the popular open-source library which brings deep learning to JavaScript. Developers can now define, train, and run machine learning models using the <a href="https://js.tensorflow.org/api/0.12.0/">high-level library API</a>.</p>

<p><a href="https://github.com/tensorflow/tfjs-models/">Pre-trained models</a> mean developers can now easily perform complex tasks like <a href="https://emojiscavengerhunt.withgoogle.com/">visual recognition</a>, <a href="https://magenta.tensorflow.org/demos/performance_rnn/index.html#2|2,0,1,0,1,1,0,1,0,1,0,1|1,1,1,1,1,1,1,1,1,1,1,1|1,1,1,1,1,1,1,1,1,1,1,1|false">generating music</a> or <a href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html">detecting human poses</a> with just a few lines of JavaScript.</p>

<p>Having started as a front-end library for web browsers, recent updates added <a href="https://github.com/tensorflow/tfjs-node">experimental support</a> for Node.js. This allows TensorFlow.js to be used in backend JavaScript applications without having to use Python.</p>

<p><em>Reading about the library, I wanted to test it out with a simple task...</em> 🧐</p>

<blockquote><p> <strong>Use TensorFlow.js to perform visual recognition on images using JavaScript from Node.js</strong></p></blockquote>

<p>Unfortunately, most of the <a href="https://js.tensorflow.org/#getting-started">documentation</a> and <a href="https://js.tensorflow.org/tutorials/webcam-transfer-learning.html">example code</a> provided uses the library in a browser. <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">Project utilities</a> provided to simplify loading and using pre-trained models have not yet been extended with Node.js support. Getting this working did end up with me spending a lot of time reading the Typescript source files for the library. 👎</p>

<p>However, after a few days' hacking, I managed to get <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d">this completed</a>! Hurrah! 🤩</p>

<p><em>Before we dive into the code, let's start with an overview of the different TensorFlow libraries.</em></p>

<h2>TensorFlow</h2>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> is an open-source software library for machine learning applications. TensorFlow can be used to implement neural networks and other deep learning algorithms.</p>

<p>Released by Google in November 2015, TensorFlow was originally a <a href="https://www.tensorflow.org/api_docs/python/">Python library</a>. It used either CPU or GPU-based computation for training and evaluating machine learning models. The library was initially designed to run on high-performance servers with expensive GPUs.</p>

<p>Recent updates have extended the software to run in resource-constrained environments like mobile devices and web browsers.</p>

<h3>TensorFlow Lite</h3>

<p><a href="https://www.tensorflow.org/mobile/tflite/">Tensorflow Lite</a>, a lightweight version of the library for mobile and embedded devices, was released in May 2017. This was accompanied by a new series of pre-trained deep learning models for vision recognition tasks, called <a href="https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNet</a>. MobileNet models were designed to work efficiently in resource-constrained environments like mobile devices.</p>

<h3>TensorFlow.js</h3>

<p>Following Tensorflow Lite, <a href="https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db">TensorFlow.js</a> was announced in March 2018. This version of the library was designed to run in the browser, building on an earlier project called <a href="https://twitter.com/deeplearnjs">deeplearn.js</a>. WebGL provides GPU access to the library. Developers use a JavaScript API to train, load and run models.</p>

<p>TensorFlow.js was recently extended to run on Node.js, using an <a href="https://github.com/tensorflow/tfjs-node">extension library</a> called <code>tfjs-node</code>.</p>

<p><em>The Node.js extension is an alpha release and still under active development.</em></p>

<h4>Importing Existing Models Into TensorFlow.js</h4>

<p>Existing TensorFlow and Keras models can be executed using the TensorFlow.js library. Models need converting to a new format <a href="https://github.com/tensorflow/tfjs-converter">using this tool</a> before execution. Pre-trained and converted models for image classification, pose detection and k-nearest neighbours are <a href="https://github.com/tensorflow/tfjs-models">available on Github</a>.</p>

<h2>Using TensorFlow.js in Node.js</h2>

<h3>Installing TensorFlow Libraries</h3>

<p>TensorFlow.js can be installed from the <a href="https://www.npmjs.com/">NPM registry</a>.</p>

<ul>
<li><code>@tensorflow/tfjs</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs">Core TensorFlow.js library</a></li>
<li><code>@tensorflow/tfjs-node</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs-node">TensorFlow.js Node.js extension</a></li>
<li><code>@tensorflow/tfjs-node-gpu</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs-node-gpu">TensorFlow.js Node.js extension with GPU support</a></li>
</ul>


<p><code>
npm install @tensorflow/tfjs @tensorflow/tfjs-node
// or...
npm install @tensorflow/tfjs @tensorflow/tfjs-node-gpu
</code></p>

<p>Both Node.js extensions use native dependencies which will be compiled on demand.</p>

<h3>Loading TensorFlow Libraries</h3>

<p>TensorFlow's <a href="https://js.tensorflow.org/api/0.12.0/">JavaScript API</a> is exposed from the core library. Extension modules to enable Node.js support do not expose additional APIs.</p>

<p><code>javascript
const tf = require('@tensorflow/tfjs')
// Load the binding (CPU computation)
require('@tensorflow/tfjs-node')
// Or load the binding (GPU computation)
require('@tensorflow/tfjs-node-gpu')
</code></p>

<h3>Loading TensorFlow Models</h3>

<p>TensorFlow.js provides an <a href="https://github.com/tensorflow/tfjs-models">NPM library</a> (<code>tfjs-models</code>) to ease loading pre-trained &amp; converted models for <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">image classification</a>, <a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet">pose detection</a> and <a href="https://github.com/tensorflow/tfjs-models/tree/master/knn-classifier">k-nearest neighbours</a>.</p>

<p>The <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model</a> used for image classification is a deep neural network trained to <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">identify 1000 different classes</a>.</p>

<p>In the project's README, the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet#via-npm">following example code</a> is used to load the model.</p>

<p>```javascript
import * as mobilenet from '@tensorflow-models/mobilenet';</p>

<p>// Load the model.
const model = await mobilenet.load();
```</p>

<p><strong>One of the first challenges I encountered was that this does not work on Node.js.</strong></p>

<p><code>
Error: browserHTTPRequest is not supported outside the web browser.
</code></p>

<p>Looking at the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L27">source code</a>, the <code>mobilenet</code> library is a wrapper around the underlying <code>tf.Model</code> class. When the <code>load()</code> method is called, it automatically downloads the correct model files from an external HTTP address and instantiates the TensorFlow model.</p>

<p>The Node.js extension does not yet support HTTP requests to dynamically retrieve models. Instead, models must be manually loaded from the filesystem.</p>

<p><em>After reading the source code for the library, I managed to create a work-around...</em></p>

<h4>Loading Models From a Filesystem</h4>

<p>Rather than calling the module's <code>load</code> method, if the <code>MobileNet</code> class is created manually, the auto-generated <code>path</code> variable which contains the HTTP address of the model can be overwritten with a local filesystem path. Having done this, calling the <code>load</code> method on the class instance will trigger the <a href="https://js.tensorflow.org/tutorials/model-save-load.html">filesystem loader class</a>, rather than trying to use the browser-based HTTP loader.</p>

<p><code>javascript
const path = "mobilenet/model.json"
const mn = new mobilenet.MobileNet(1, 1);
mn.path = `file://${path}`
await mn.load()
</code></p>

<p><strong>Awesome, it works!</strong></p>

<p><em>But how where do the models files come from?</em></p>

<h3>MobileNet Models</h3>

<p>Models for TensorFlow.js consist of two file types, a model configuration file stored in JSON and model weights in a binary format. Model weights are often sharded into multiple files for better caching by browsers.</p>

<p>Looking at the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L68-L76">automatic loading code</a> for MobileNet models, models configuration and weight shards are retrieved from a public storage bucket at this address.</p>

<p><code>
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v${version}_${alpha}_${size}/
</code></p>

<p>The template parameters in the URL refer to the model versions listed <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md#pre-trained-models">here</a>. Classification accuracy results for each version are also shown on that page.</p>

<p><em>According to the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L36">source code</a>, only MobileNet v1 models can be loaded using the <code>tensorflow-models/mobilenet</code> library.</em></p>

<p>The HTTP retrieval code loads the <code>model.json</code> file from this location and then recursively fetches all referenced model weights shards. These files are in the format <code>groupX-shard1of1</code>.</p>

<h4>Downloading Models Manually</h4>

<p>Saving all model files to a filesystem can be achieved by retrieving the model configuration file, parsing out the referenced weight files and downloading each weight file manually.</p>

<p><strong>I want to use the MobileNet V1 Module with 1.0 alpha value and image size of 224 pixels.</strong> This gives me the <a href="https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json">following URL</a> for the model configuration file.</p>

<p><code>
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json
</code></p>

<p>Once this file has been downloaded locally, I can use the <a href="https://stedolan.github.io/jq/"><code>jq</code> tool</a> to parse all the weight file names.</p>

<p><code>
$ cat model.json | jq -r ".weightsManifest[].paths[0]"
group1-shard1of1
group2-shard1of1
group3-shard1of1
...
</code></p>

<p>Using the <code>sed</code> tool, I can prefix these names with the HTTP URL to generate URLs for each weight file.</p>

<p><code>
$ cat model.json | jq -r ".weightsManifest[].paths[0]" | sed 's/^/https:\/\/storage.googleapis.com\/tfjs-models\/tfjs\/mobilenet_v1_1.0_224\//'
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group1-shard1of1
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group2-shard1of1
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group3-shard1of1
...
</code></p>

<p>Using the <code>parallel</code> and <code>curl</code> commands, I can then download all of these files to my local directory.</p>

<p><code>
cat model.json | jq -r ".weightsManifest[].paths[0]" | sed 's/^/https:\/\/storage.googleapis.com\/tfjs-models\/tfjs\/mobilenet_v1_1.0_224\//' |  parallel curl -O
</code></p>

<h3>Classifying Images</h3>

<p><a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet#via-npm">This example code</a> is provided by TensorFlow.js to demonstrate returning classifications for an image.</p>

<p>```javascript
const img = document.getElementById('img');</p>

<p>// Classify the image.
const predictions = await model.classify(img);
```</p>

<p><strong>This does not work on Node.js due to the lack of a DOM.</strong></p>

<p>The <code>classify</code> <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L143-L155">method</a> accepts numerous DOM elements (<code>canvas</code>, <code>video</code>, <code>image</code>) and will automatically retrieve and convert image bytes from these elements into a <a href="https://js.tensorflow.org/api/latest/index.html#tensor3d"><code>tf.Tensor3D</code> class</a> which is used as the input to the model. Alternatively, the <code>tf.Tensor3D</code> input can be passed directly.</p>

<p><strong>Rather than trying to use an external package to simulate a DOM element in Node.js, I found it easier to construct the <code>tf.Tensor3D</code> manually.</strong></p>

<h4>Generating Tensor3D from an Image</h4>

<p>Reading the <a href="https://github.com/tensorflow/tfjs-core/blob/master/src/kernels/backend_cpu.ts#L126-L140">source code</a> for the method used to turn DOM elements into Tensor3D classes, the following input parameters are used to generate the Tensor3D class.</p>

<p><code>javascript
const values = new Int32Array(image.height * image.width * numChannels);
// fill pixels with pixel channel bytes from image
const outShape = [image.height, image.width, numChannels];
const input = tf.tensor3d(values, outShape, 'int32');
</code></p>

<p><code>pixels</code> is a 2D array of type (Int32Array) which contains a sequential list of channel values for each pixel. <code>numChannels</code> is the number of channel values per pixel.</p>

<h4>Creating Input Values For JPEGs</h4>

<p>The <a href="https://www.npmjs.com/package/jpeg-js"><code>jpeg-js</code> library</a> is a pure javascript JPEG encoder and decoder for Node.js. Using this library the RGB values for each pixel can be extracted.</p>

<p><code>javascript
const pixels = jpeg.decode(buffer, true);
</code></p>

<p>This will return a <code>Uint8Array</code> with four channel values (<code>RGBA</code>) for each pixel (<code>width * height</code>). The MobileNet model only uses the three colour channels (<code>RGB</code>) for classification, ignoring the alpha channel. This code converts the four channel array into the correct three channel version.</p>

<p>```javascript
const numChannels = 3;
const numPixels = image.width * image.height;
const values = new Int32Array(numPixels * numChannels);</p>

<p>for (let i = 0; i &lt; numPixels; i++) {
  for (let channel = 0; channel &lt; numChannels; ++channel) {</p>

<pre><code>values[i * numChannels + channel] = pixels[i * 4 + channel];
</code></pre>

<p>  }
}
```</p>

<h4>MobileNet Models Input Requirements</h4>

<p>The <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md#mobilenet_v1">MobileNet model</a> being used classifies images of width and height 224 pixels. Input tensors must contain float values, between -1 and 1, for each of the three channels pixel values.</p>

<p>Input values for images of different dimensions needs to be re-sized before classification. Additionally, pixels values from the JPEG decoder are in the range <em>0 - 255</em>, rather than <em>-1 to 1</em>. These values also need converting prior to classification.</p>

<p><strong>TensorFlow.js has library methods to make this process easier but, fortunately for us, the <code>tfjs-models/mobilenet</code> library <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L103-L114">automatically handles</a> this issue!</strong> 👍</p>

<p>Developers can pass in Tensor3D inputs of type <code>int32</code>  and different dimensions to the  <code>classify</code> method and it converts the input to the correct format prior to classification. Which means there's nothing to do... Super 🕺🕺🕺.</p>

<h4>Obtaining Predictions</h4>

<p>MobileNet models in Tensorflow are trained to recognise entities from the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">top 1000 classes</a> in the <a href="http://image-net.org/">ImageNet</a> dataset. The models output the probabilities that each of those entities is in the image being classified.</p>

<p><em>The full list of trained classes for the model being used can be found in <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">this file</a>.</em></p>

<p>The <code>tfjs-models/mobilenet</code> library exposes a <code>classify</code> method on the <code>MobileNet</code> class to return the top X classes with highest probabilities from an image input.</p>

<p><code>javascript
const predictions = await mn_model.classify(input, 10);
</code></p>

<p><code>predictions</code> is an array of X classes and probabilities in the following format.</p>

<p><code>javascript
{
  className: 'panda',
  probability: 0.9993536472320557
}
</code></p>

<h2>Example</h2>

<p>Having worked how to use the TensorFlow.js library and MobileNet models on Node.js, <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d">this script</a> will classify an image given as a command-line argument.</p>

<h3>source code</h3>

<ul>
<li>Save this script file and package descriptor to local files.</li>
</ul>


<script src="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d.js"></script>


<h3>testing it out</h3>

<ul>
<li><p>Download the model files to a <code>mobilenet</code> directory using the instructions above.</p></li>
<li><p>Install the project dependencies using NPM
<code>
npm install
</code></p></li>
<li><p>Download a sample JPEG file to classify
<code>
wget http://bit.ly/2JYSal9 -O panda.jpg
</code>
<img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG"></p></li>
<li><p>Run the script with the model file and input image as arguments.
<code>
node script.js mobilenet/model.json panda.jpg
</code></p></li>
</ul>


<p><strong>If everything worked, the following output should be printed to the console.</strong></p>

<p>```javascript
classification results: [ {</p>

<pre><code>className: 'giant panda, panda, panda bear, coon bear',
probability: 0.9993536472320557 
</code></pre>

<p>} ]
```</p>

<p>The image is correctly classified as containing a Panda with 99.93% probability! 🐼🐼🐼</p>

<h2>Conclusion</h2>

<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>

<p>Having been released as a browser-based library, TensorFlow.js has now been extended to work on Node.js, although not all of the tools and utilities support the new runtime. With a few days' hacking, I was able to use the library with the MobileNet models for visual recognition on images from a local file.</p>

<p>Getting this working in the Node.js runtime means I now move on to my next idea... making this run inside a serverless function! Come back soon to read about my next adventure with TensorFlow.js. 👋</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Playing With OpenWhisk]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2016/04/22/openwhisk/"/>
    <updated>2016-04-22T15:36:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2016/04/22/openwhisk</id>
    <content type="html"><![CDATA[<p>IBM recently launched <a href="https://developer.ibm.com/openwhisk/">OpenWhisk</a>,
their new <a href="https://www.quora.com/What-is-Serverless-Computing">"serverless"</a>
compute platform.</p>

<p>This service allows developers to register small bits of
code that are executed on-demand in response to external events. The
"serverless" stack started in 2014, when Amazon launched
<a href="https://aws.amazon.com/lambda/">Lambda</a>, but is now set to be a major
technology trend in 2016 with IBM, Microsoft and Google all launching their own
solutions.</p>

<p>OpenWhisk is the first <a href="https://github.com/openwhisk/openwhisk">open-source "serverless" platform</a>. It supports running registered
actions in Node.js, Swift and even executing custom Docker containers.</p>

<p>Playing around with the technology recently, I've created two projects using the platform.</p>

<h2>OpenWhisk Client Library</h2>

<p>OpenWhisk exposes a <a href="https://github.com/openwhisk/openwhisk/blob/master/docs/reference.md#rest-api">RESTful API</a>
for interacting with the service. Wrapping this API with a
<a href="https://github.com/openwhisk/openwhisk-client-js">small client library</a> makes it easy for developers to interact with the service from JavaScript.</p>

<p>This library has been donated back to the OpenWhisk project and is <a href="https://www.npmjs.com/package/openwhisk">available through NPM</a>.</p>

<p><code>javascript
const openwhisk = require('openwhisk')
const ow = openwhisk({api: 'https://openwhisk.ng.bluemix.net/api/v1/', api_key: '...', namespace: '...'})
ow.actions.invoke({actionName: 'action'}).then(result =&gt; {
  // result is service response
})
</code></p>

<h2>Whiskify</h2>

<p>This <a href="https://github.com/jthomas/whiskify">project</a>, available through <a href="https://www.npmjs.com/package/openwhisk">NPM</a>, makes it easy to run arbitary JavaScript
functions as OpenWhisk actions.  Passing a reference to a JavaScript function
into the module, an OpenWhisk action is created using the function source.  The
module returns a new JavaScript function, that when executed, will call the
remote action and returns a Promise with the service response.</p>

<p>``` javascript
const whiskify = require('whiskify')({api: 'https://', api_key: '...', namespace: '...'})
const action = whiskify(function (item) { return item + 1; })</p>

<p>action(1).then(function (result) {
  // == 2
})</p>

<p>action.map([1, 2, 3, 4]).then(function (result) {
 // == [2, 3, 4, 5]
})</p>

<p>action.delete()
```</p>

<p>This project uses the client library above.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GeoPix Live Photos]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2015/07/16/geopix-live-photos/"/>
    <updated>2015-07-16T13:26:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2015/07/16/geopix-live-photos</id>
    <content type="html"><![CDATA[<p><a href="http://www.tricedesigns.com/about/">Andrew Trice</a> wrote a great sample
application for <a href="bluemix.net">IBM Bluemix</a> called <a href="http://www.tricedesigns.com/2015/03/27/geopix-a-native-ios-app-powered-by-ibm-mobilefirst-for-bluemix/">GeoPix</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/376h2yjnf6Q" frameborder="0" allowfullscreen></iframe>


<p><blockquote><p>GeoPix uses the IBM MobileFirst services to provide a native iOS application which allows users to capture images from their mobile phones, storing them on the local device with automatic syncing to the cloud when online.</p></p><p><p>Using a web application, the user can view their images over a map based upon their location when the photo was taken.</p><footer><strong>Andrew Trice</strong> <cite><a href='http://www.tricedesigns.com/2015/03/27/geopix-a-native-ios-app-powered-by-ibm-mobilefirst-for-bluemix/'>www.tricedesigns.com/2015/03/27/&hellip;</a></cite></footer></blockquote></p>

<p>I've been using the demonstration to highlight the <a href="https://console.ng.bluemix.net/solutions/mobilefirst">mobile capabilities</a> of IBM Bluemix and had an idea for an
enhancement...</p>

<p><strong><em>Could the web page update with new pictures without having to refresh the page?</em></strong></p>

<p>Looking at the <a href="https://github.com/IBM-Bluemix/MobileFirst-Offline-Apps">source code</a>, the web application
is a Node.js application using the <a href="http://leafletjs.com/">Leaflet</a> JavaScript library to create interactive
maps. Images captured from mobile devices are <a href="https://www.ng.bluemix.net/docs/services/data/index.html#replicate">synchronised</a>
to a remote <a href="http://couchdb.apache.org/">CouchDB</a> database. When the user visits the <a href="http://geopix-web.mybluemix.net">GeoPix</a> site, the application queries this database
for all mobile images and renders the HTML using the <a href="http://jade-lang.com/">Jade</a> templating language.</p>

<p>Adding support for live photos will require two new features...</p>

<ul>
<li><em>Triggering backend events when new photos are available</em></li>
<li><em>Sending these photos in real-time to the web page</em></li>
</ul>


<h2>Change Notifications Using CouchDB</h2>

<p>CouchDB comes with built-in support for listening to changes in a database, <a href="http://guide.couchdb.org/draft/notifications.html">change notifications</a>.
The <a href="http://docs.couchdb.org/en/latest/api/database/changes.html"><em>_changes</em> feed</a> for a database is an activity stream publishing all document modifications.</p>

<p>GeoPix uses the following CouchDB <a href="https://www.npmjs.com/package/cloudant">client library</a>, to interact with our database from NodeJS. This library provides an <a href="https://github.com/dscape/nano#nanodbfollowname-params-callback">API</a>
to start following database changes and register callbacks for updates.</p>

<p>Modifying our <a href="https://github.com/IBM-Bluemix/MobileFirst-Offline-Apps/blob/master/Node.js/app.js#L42-L51">application code</a>, upon connecting to the CouchDB database, we register a change notification
handler. We follow all changes that occur in the future (<em>since: "now"</em>) and include the full document contents
in the change event (<em>include_docs: true</em>).</p>

<p>``` javascript
Cloudant({account:credentials.username, password:credentials.password}, function(err, cloudant) {</p>

<pre><code>var geopix = cloudant.use(database);
var feed = geopix.follow({include_docs: true, since: "now"});

feed.on('change', function (change) {
  // ....we can now send this data to the web pages
});

feed.follow();
</code></pre>

<p>})
```</p>

<p><strong>Now, every time a user sync their local photos to the cloud, the registered callback will be executed.</strong></p>

<p><em>How do we send new photos to the web page over a real-time stream?</em></p>

<h2>Real-time Web with Socket.IO</h2>

<p>Introducing <a href="">Socket.IO</a>...</p>

<p><blockquote><p>Socket.IO enables real-time bidirectional event-based communication.<br/>It works on every platform, browser or device, focusing equally on reliability and speed.</p></blockquote></p>

<p>Sounds great!</p>

<p>By embedding this library into our application, we can open a real-time event stream between the server and client. This channel
will be used by the client to listen for new images and then update the page.</p>

<p>The library has great <a href="http://socket.io/docs/">documentation</a> and provides both <a href="http://socket.io/docs/server-api/">server</a> and <a href="http://socket.io/docs/client-api/">client</a> modules. It also integrates with <a href="http://expressjs.com">ExpressJS</a>, the web framework used in GeoPix.
Socket.IO can use either WebSocket or long-polling transport protocols.</p>

<p>Socket.IO supports running under ExpressJS with minimal configuration, here are the changes needed to start our real-time stream in GeoPix:</p>

<p>``` javascript
var express = require('express');
var app = express();
var server = require('http').Server(app);
var io = require('socket.io')(server);</p>

<p>// ...snipped out the app routes for express</p>

<p>io.on('connection', function (socket) {</p>

<pre><code>console.log('New Client WSS Connection.')
</code></pre>

<p>});</p>

<p>var port = (process.env.VCAP_APP_PORT || 3000);
server.listen(port);
```</p>

<p><em>When a document change event is fired, executing the handle we registered above, we want to send this data to all connected clients.</em></p>

<p>Using the <a href="http://socket.io/docs/server-api/#server#emit"><em>emit</em> call</a> from the server-side API will do this for us.</p>

<p>``` javascript
feed.on('change', function (change) {</p>

<pre><code>io.sockets.emit('image', change);
</code></pre>

<p>});
```</p>

<p><strong>Now we're sending changes to the clients, we need to modify the client-side to listen for events and update the page.</strong></p>

<p>Socket.IO provides a <a href="http://socket.io/download/">JavaScript client library</a> that exposes a simple API for listening to events from the server-side stream.
Once we've included the script tag pointing to the client library, we can register a callback for <em>image</em> events and update the DOM
with the new elements.</p>

<p>We're sending the full database document associated with each photo to the client. The raw image bytes are stored as an
<a href="https://wiki.apache.org/couchdb/HTTP_Document_API#Attachments">attachment</a>.</p>

<p>``` javascript
var socket = io(); // TIP: io() with no args does auto-discovery
socket.on('connect', function () {</p>

<pre><code>console.log('WSS Connected');

socket.on('image', function (image) { // TIP: you can avoid listening on `connect` and listen on events directly too!
    var attachment = Object.keys(image.doc._attachments)[0]
    var url = "/image/" + image.doc._id + "/" + attachment;
    add_new_image(url, image.doc.clientDate, 'latitude: '
        + image.doc.latitude + ', longitude: '
        + image.doc.longitude + ', altitude: '
        + image.doc.altitude);
});
</code></pre>

<p>});
```</p>

<p>...and that's it! Now our web pages will automatically update with new photos whenever the mobile application syncs with the cloud.</p>

<h2>CouchDB + Socket.IO = Real-time Awesome!</h2>

<p>Adding <em>real-time</em> photos to our application was amazingly simple by combining
CouchDB with Socket.IO.</p>

<p>CouchDB's <em>_changes</em> API provided an easy way to follow
all modifications to database documents in real-time. Socket.IO made the
configuration and management of real-time event streams between our server and
client straightforward.</p>

<p><em>With minimal code changes, we simply connected these two technologies to create
a real-time photo stream for our GeoPix application. <strong>Awesome</strong></em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Server Side Dijit]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2013/01/15/server-side-dijit/"/>
    <updated>2013-01-15T10:08:00+00:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2013/01/15/server-side-dijit</id>
    <content type="html"><![CDATA[<p>Modern Dojo applications often use declarative programming, annotating HTML
elements with custom attributes containing module identifiers, to declare widgets
and use client-side rendering with HTML templates to convert web pages into
JavaScript applications.</p>

<p><strong>Client-side rendering often comes with a major complaint, the dreaded
"pop-up effect".</strong></p>

<p><img src="/images/white_screen.png"></p>

<p>This happens because the HTML initially displayed
does not contain widget templates until after client-side rendering has
finished. Essentially, the application has to load twice, once to download all
the JS, CSS and HTML resources, then again, to render widgets client-side.</p>

<p>Usually this is hidden behind an overlay screen, which becomes especially
annoying in multi-page applications.</p>

<p><strong>So, what can we do?</strong></p>

<p>Templated widgets provide a good pattern for building re-usable application modules but client-side rendering can
provide a less ideal user experience.</p>

<p>Reading an article about the <a href="http://anyasq.com/79-im-a-technical-lead-on-the-google+-team">technology stack behind Google+</a>, Google
were using page widgets with templates supported by the <a href="https://developers.google.com/closure/library/">Closure framework</a>. However, they had
an interesting idea to overcome the client-side rendering issue...</p>

<p><blockquote><p>We often render our Closure templates server-side<br/>so the page renders before any JavaScript is loaded, then the JavaScript finds<br/>the right DOM nodes and hooks up event handlers, etc. to make it responsive.</p><footer><strong>Joseph Smarr</strong> <cite><a href='http://anyasq.com/79-im-a-technical-lead-on-the-google+-team'>anyasq.com/&hellip;</a></cite></footer></blockquote></p>

<p><strong>Could we use the same server-side rendering technique in Dojo applications?</strong></p>

<p>Doing a little investigation, Dojo's abstractions around widget rendering made it perfect
for server-side rendering.</p>

<p><strong>Tl;DR? Project source code is available on Github <a href="https://github.com/jthomas/server_side_dijit">here</a>.</strong></p>

<h2>Dijit Widget Lifecycle</h2>

<p>Dojo widgets inherit from the following base class,
<a href="http://dojotoolkit.org/reference-guide/1.8/dijit/_WidgetBase.html">dijit/_WidgetBase</a>,
which provides the widget lifecycle, which can be extended with custom implementations.</p>

<ul>
<li><strong>constructor</strong></li>
<li><strong>parameters</strong> are mixed into the widget instance</li>
<li><strong>postMixInProperties</strong> - Invoked before rendering occurs, and before any DOM nodes are created.</li>
<li><strong>buildRendering</strong> - Used to define the widget's DOM nodes</li>
<li><strong>setters are called</strong> - Custom attribute setters are called</li>
<li><strong>postCreate</strong> - Widget has been rendered.</li>
<li><strong>startup</strong> - Parsing and creation of any child widgets completed.</li>
</ul>


<p>All lifecycle methods are executed in linear order for each new widget instance.
Having clear abstractions around where and when the widget rendering
occurs in the lifecycle (buildRendering) makes extending simple.</p>

<p>Rendering widget templates is provided by an additional mixin,
<a href="http://dojotoolkit.org/reference-guide/1.8/dijit/_TemplatedMixin.html">dijit/_TemplatedMixin</a>.</p>

<p>There's also a further extension, <a href="http://dojotoolkit.org/reference-guide/1.8/dijit/_WidgetsInTemplateMixin.html">dijit/_WidgetsInTemplateMixin</a>,
for ensuring child widgets within the template are instantiated correctly during rendering.</p>

<p>If we provide a pre-rendered template within the page, the client-side
renderer will hook up that DOM node as the widget's DOM node, using a
custom lifecycle extension, rather than attempting to construct the HTML
template client-side.</p>

<p>We only need to modify the <em>buildRendering</em> phase, every
other lifecycle phase will run normally.</p>

<h2>Rendering Templates Server-Side</h2>

<p>Now we know where to hook up a pre-rendered template, how would we render the templates server-side?</p>

<p>We want to support server-side rendering with only minimal changes to an application.</p>

<h3> Running Dojo on NodeJS</h3>

<p>With the recent popularity of NodeJS, we have an excellent server-side
JavaScript environment. If we configure Dojo to run within this platform, we
should be able to construct page widgets server-side, delegating template
rendering to the same lifecycle used client-side.</p>

<p>This code below shows how to configure Dojo on NodeJS.</p>

<p>``` javascript Loading Dojo on NodeJS</p>

<pre><code>dojoConfig = {
    packages: [
        {name: "dojo", location: "./lib/dojo"},
        {name: "dijit", location: "./lib/dijit"}
    ],
};

require("./lib/dojo/dojo.js");
</code></pre>

<p>```</p>

<p>Once we've evaluated the dojo.js file within NodeJS, the AMD loader (<em>require/define</em>) is available through properties on the
<em>global</em> object. We can use these functions to load additional DTK or custom AMD modules. Accessing
page widgets using the AMD loader, we can execute the lifecycle methods to trigger template rendering, read the
rendered template and include the output within the application's HTML pages.</p>

<p><strong>Unfortunately, there's one thing missing... access to the DOM!</strong></p>

<h3>Simulating a Browser </h3>

<p>Dojo widgets need access to the <a href="http://en.wikipedia.org/wiki/Document_Object_Model">DOM</a> when rendering the static HTML template into live DOM nodes.
Running inside a NodeJS instance, rather than a browser, this API is missing.</p>

<p>Luckily, there's a pure-JavaScript implementation of a DOM, which can be executed within NodeJS, called <a href="https://github.com/tmpvar/jsdom">JSDOM</a>.</p>

<p>Importing this package within our application simulates those APIs, allowing page widgets to render normally and, more importantly, letting
us access the live DOM nodes which result from widget rendering.</p>

<p>Finally, creating Dojo widgets within our fake browser environment triggered a
few issues, due to the configuration used with the NodeJS loader.</p>

<p>The code snippet below shows how we initialise a server-side DOM and fix those configuration issues.</p>

<p>``` javascript Server-Side DOM with Dojo
var jsdom = require("jsdom").jsdom,</p>

<pre><code>document = jsdom("&lt;html&gt;&lt;/html&gt;"),
window = document.createWindow();
</code></pre>

<p>var has = global.require("dojo/has"),</p>

<pre><code>win = global.require("dojo/_base/window"),
</code></pre>

<p>// Manually add event listener test as this was only included in
// the "host-browser" profile.
has.add("dom-addeventlistener", !!document.addEventListener);
has.add("dom-attributes-explicit", true);</p>

<p>// Fix global property to point to "window"
win.global = window;
```</p>

<p><em>Now we can successfully create widgets on the server-side, how do we know which
widgets to create for an application?</em></p>

<h3>Declarative Dojo Applications</h3>

<p>Dojo provides a mechanism to convert HTML elements, annotated with module identifiers, into page widgets at runtime.</p>

<p>Using the <a href="http://dojotoolkit.org/reference-guide/1.8/dojo/parser.html">dojo/parser</a>
module, once the page has loaded, it will automatically instantiate the widgets, passing in
parameters and other attributes defined in the markup.</p>

<p>An example of declarative widget declaration is shown below.</p>

<p>``` html Declarative widgets
<select name="state" data-dojo-type="dijit/form/Select"></p>

<pre><code>&lt;option value="TN"&gt;Tennessee&lt;/option&gt;
&lt;option value="VA" selected="selected"&gt;Virginia&lt;/option&gt;
&lt;option value="WA"&gt;Washington&lt;/option&gt;
&lt;option value="FL"&gt;Florida&lt;/option&gt;
&lt;option value="CA"&gt;California&lt;/option&gt;
</code></pre>

<p></select>
```</p>

<p>Application pages using declarative markup can easily be scanned to find application widgets that are needed. As we're able to
run AMD modules server-side, we can simply use the existing Dojo parser with our server-side DOM to do the hard work for us!</p>

<h3>Server-side Parsing</h3>

<p>For a sample page we want to pre-render, we inject the HTML source into our DOM and run the parser over the current instance. Once the parser
has finished, the server-side DOM will contain the rendered templates for each widget.</p>

<p>``` javascript Using dojo/parser with JSDOM
var parser = global.require("dojo/parser"),</p>

<pre><code>source = "... page html goes here ...";
</code></pre>

<p>// Overwrite finished document contents
// with new source and run parser over the DOM.
document.write(source);
parser.parse(document);</p>

<p>source = document.innerHTML;
```</p>

<p>Using JSDOM like this, script tags within the page aren't evaluated, letting us handle the module loading
and parsing externally in NodeJS.</p>

<p>However, this presented a challenge as module dependencies declared in these
script tags were ignored, leaving the parser to instantiate declarative widgets from modules which hadn't been loaded.</p>

<p><em>Luckily, in the Dojo 1.8 release, the parser was enhanced to automatically load any missing module dependencies during the parsing phase.
Phew...</em></p>

<p>Finally, once a widget's template has been rendered, any other operations
performed by the parser are unnecessary.  Creating a "lite" parser which
removed these code paths, which also provided a place for the extensions
described later, was started from a copy of the existing parser.</p>

<p>Using the AMD "aliases" configuration, this module transparently replaced the existing parser during server-side rendering.</p>

<h3>Mixins For Pre-Rendering</h3>

<p>Rendering widgets server-side, using NodeJS and JSDOM, works for simple widgets but what happens when you use
layout widgets, which rely on accessing the browser's layout properties? What if you have separate code paths for different browsers
which affect the template string?</p>

<p>There are numerous scenarios where we rely on data that's impractical to simulate
within our fake browser.</p>

<p><em>So, how do we pre-render these widgets? We don't!</em></p>

<p>Ignoring these widgets, which leaves them to render normally client-side.</p>

<p>Identifying widgets to render server-side takes advantage of a new declarative
parameter used by the parser since 1.8, <em>data-dojo-mixins</em>. This parameter
allows additional modules to be mixed into the declarative class instance by
the parser.</p>

<p>Using this parameter with a custom module,
<em>server_side/_TemplatedMixin</em>, on widgets to be pre-rendered, as shown below,
make identification easy. Additionally, this class
will contain the lifecycle extensions that modifies client-side rendering.</p>

<p>``` html Custom Declarative Mixins</p>

<div data-dojo-type="dijit/CalendarLite" data-dojo-mixins="server_side/_TemplatedMixin"></div>


<p>```</p>

<h3>Automating Rendering</h3>

<p><strong>Now we've identified the mechanism for server-side rendering, how can we automate this process
for all application pages?</strong></p>

<p><a href="https://github.com/senchalabs/connect">Connect</a> is <em>"an extensible HTTP server framework for
node, providing high performance plugins known as middleware"</em>.</p>

<p>Using this framework as our HTTP server means we can write a custom middleware plugin
that will automatically parse, pre-render and serve all our application pages.</p>

<p>Connect plugins are functions that accept three parameters, the request and
response objects, along with a callback to signal this plugin's work has
finished. Each registered plugin will be executed for each request.</p>

<p>We've decomposed the library into two files, server_side.js, which exposes a
valid express plugin, and render.js, which provides a simple interface for the
server-side rendering, described above. The complete version of the code for both modules is included below.</p>

<p>``` javascript server_side.js
var render = require('./render.js');</p>

<p>module.exports = function (config) {</p>

<pre><code>// Create AMD packages from module configuration.
var page = render({
    dojo: config.dojo + "/dojo",
    dijit: config.dojo + "/dijit",
    server_side: __dirname + "/../public/js/server_side"
});

return function (req, res, next) {
    var ignore = function (accept) {
        return accept.indexOf("text/html") === -1;
    };

    // Only hook into text/html requests....
    if (ignore(req.headers.accept)) {
        return next();
    }

    var write = res.write,
        end = res.end,
        buffer = "";

    // We need entire page contents, not just the chunks.
    // Proxy original methods while we're buffering.
    res.write = function (chunk, encoding) {
        buffer = buffer.concat(chunk);
        return true;
    };

    res.end = function (chunk, encoding) {
        if (chunk) {
            res.write(chunk);
        }

        // Fix content-length, we now have more data to send.
        var rendered = page(buffer);
        res.setHeader("Content-Length", rendered.length);

        return end.call(res, rendered, encoding);
    };

    next();
};
</code></pre>

<p>};
```</p>

<p>``` javascript render.js
var jsdom = require("jsdom").jsdom,</p>

<pre><code>document = jsdom("&lt;html&gt;&lt;/html&gt;"),
window = document.createWindow();
</code></pre>

<p>module.exports = function (packages) {</p>

<pre><code>// Fix window objects in global scope.
global.document = document;
global.navigator = window.navigator;
global.window = window;

var amd_packages = Object.keys(packages).map(function (key) {
    return { name: key, location: packages[key] };
});

// Deliberately create global "dojoConfig" variable.
dojoConfig = {
    packages: amd_packages,
    // _WidgetsInTemplateMixin call parser directly to instantiate children. 
    // We need it to use our custom parser so use AMD-remapping magic!
    aliases: [["dojo/parser", "server_side/parser"]],
    deps: ["server_side/parser", "dojo/has", "dojo/_base/window", "server_side/registry"]
};

require(packages.dojo + "/dojo.js");

// Once Dojo has been evalulated, require &amp; define methods 
// from AMD API as exposed as properties on "global" object.

var has = global.require("dojo/has"),
    win = global.require("dojo/_base/window"),
    registry = global.require("server_side/registry"),
    parser = global.require("server_side/parser");

// Now we need to manually fix a few things to make Dojo 
// simulate running in a browser.

// Manually add event listener test as this was only included in 
// the "host-browser" profile.
has.add("dom-addeventlistener", !!document.addEventListener);
has.add("dom-attributes-explicit", true);

// Fix global property to point to "window" 
win.global = window;

return function (source) {
    // Clear any previously rendered widgets from registry,
    // simulate fresh page load.
    registry.reset();

    // Overwrite finished document contents
    // with new source and run parser over the DOM.
    document.write(source);
    parser.parse(document);

    return document.innerHTML;
};
</code></pre>

<p>};
```</p>

<p>Using this new plugin in an application is demonstrated in the code below, which
serves the "public" directory as the application's source root.</p>

<p>``` javascript Server-side Rendering Application
var connect = require('connect'),</p>

<pre><code>server_side = require('../lib/server_side');
</code></pre>

<p>var app = connect()
  .use(connect.directory(<strong>dirname + '/public', { icons: true }))
  .use(server_side({dojo: process.env.DOJO_SOURCE}))
  .use("/dojo", connect.static(process.env.DOJO_SOURCE))
  .use("/server_side", connect.static(</strong>dirname + '/../public/js/server_side'))
  .use(connect.static(__dirname + '/public'))
  .listen(3000);
```</p>

<h2>Using Server-Side Rendered Templates</h2>

<p>Once the pre-rendered page has been returned to the browser, the normal client-side
parsing will take place to instantiate the page widgets. For widgets whose templates are
included within the page, we need to ensure the normal client-side rendering is bypassed.</p>

<p>In this scenario, we connect the widget's <em>domNode</em> property to the DOM node that the
declarative widget was instantiated from.</p>

<h3>Extending buildRendering</h3>

<p>Adding a HTML template to your widget is achieved by inheriting from
<em>dijit/_TemplatedMixin</em>, which provides the "buildRendering" implementation to
convert a HTML string stored under "templateString" into live DOM nodes.</p>

<p>Although we want to skip creating DOM nodes from the template, there are other steps, e.g. attaching event handlers, which must be ran normally.
Using a custom mixin to identify declarative widgets for server-side rendering, <em>server_side/_TemplatedMixin</em>, also provides
the extension point to modify the rendering process.</p>

<p>Overwriting the default implementation of "buildRendering" through this mixin led
to unresolvable issues.</p>

<p>We're forced to call any super-class "buildRendering" implementations, through
"this.inherited(arguments)", to ensure any custom code paths that also extend this method are executed.
However, this will reach the original <em>dijit/_TemplatedMixin</em> module, which we need to skip.</p>

<p>Monkey-patching the _TemplatedMixin prototype became the easiest solution.</p>

<p>Once our custom mixin is loaded,
we overwrite "buildRendering" which a new implementation. Using a custom flag, provided by our mixin, we check
whether to continue with the normal code path for client-side rendering, otherwise we run our stripped down version.</p>

<p>``` javascript Monkey-patching _TemplatedMixin</p>

<pre><code>var br = _TemplatedMixin.prototype.buildRendering,
    fc = _TemplatedMixin.prototype._fillContent;

// Stripped down of the original function source below.
_TemplatedMixin.prototype.buildRendering = function () {
    if (!this.serverSide) {
        return br.call(this);
    }

    // Source DOM node already the pre-rendered template nodes.
    var node = this.srcNodeRef;

    node.removeAttribute("data-dojo-type");

    // Call down to _Widget.buildRendering() to get base classes assigned
    _WidgetBase.prototype.buildRendering.call(this);

    this._attachTemplateNodes(node, function(n,p){ return n.getAttribute(p); });

    this._beforeFillContent();      // hook for _WidgetsInTemplateMixin

    // Don't pass srcRefNode reference as it doesn't exist.
    this._fillContent();
};

// Override to turn into a no-op, we don't want to attach source
// ref nodes client side as it's been done on the server.
_TemplatedMixin.prototype._fillContent = function () {
    if (!this.serverSide) {
        return fc.apply(this, arguments);
    }
};
</code></pre>

<p>```</p>

<p>We performed the same trick for the <em>fillContent</em> method due to similar issues, along with a new implementation
of <em>attachTemplateNodes</em> in the mixin.</p>

<p>With this minimal change to the client-side rendering process, widgets pick up their templates from the existing page and are
instantiated normally. Hooking up template nodes as properties on the parent, attaching event handlers and setting data bindings
behaves as expected.</p>

<h3>Putting It Together</h3>

<p><strong>Using our custom middleware for server-side rendering, along with our client-side rendering modifications,
users accessing pages will see the templated widgets straight away, removing the "double-rendering" effect
and the need for loading screens.</strong></p>

<p><img src="/images/pre_rendered.png"></p>

<p><em>This image above the same widgets rendered client-side and server-side when the page loads, but before
client-side rendering has finished.</em></p>

<p>Server-side rendering also comes with client-side performance benefits,
reducing the number of costly DOM operations performed during application loading.
This may be especially useful for low-power devices with mobile browsers.</p>

<p>Extending, rather than replacing, the normal Dojo rendering lifecycle allows us to transparently delegate rendering
to the client-side for unsupported widgets. Excellent abstractions already provided for the lifecycle in the toolkit make
the extension conceptually simple.</p>

<p>There are restrictions that come with this implementation, discussed below, but working within these
constraints it is possible for the majority of templated widgets to be rendered server-side.</p>

<h2>Source Code</h2>

<p>All source code for the project lives on Github <a href="https://github.com/jthomas/server_side_dijit">here</a>.
Feel free to file issues, patches and comments at the project home page.</p>

<p>Once you have checked out the project code, run the following command to
start a test application comparing client-side and server-side rendering side
by side.</p>

<p><code>sh
$ export DOJO_SOURCE=/path/to/dojo-release-1.8.0-src
$ npm start
</code></p>

<p>Once the server has started, visit <a href="http://localhost:3000">http://localhost:3000</a>.</p>

<p>You can also install the module as an NPM package, <a href="https://npmjs.org/package/server_side_dijit">server_side_dijit</a>,
and use the plugin within your existing Connect application.</p>

<h2>Issues</h2>

<p>We've already mentioned potential pitfalls which restrict server-side
rendering. These include widgets that use browser dimensions to dynamically
calculate sizing e.g. layout managers, use client-side resources to construct
templates e.g. reading cookie data, expect access to remote resources e.g
XHR'ing session details, and many, many more.</p>

<p>Letting those widgets default to client-side template rendering provides a safe fallback.</p>

<p>Discovering which existing Dojo widgets can support server-side rendering requires manual
testing. Within the project directory, under the "/test/public" location, we've started
collecting test pages which demonstrate those widgets which are known to work. Looking at those
pages should provide a good indication of the current level of support.</p>
]]></content>
  </entry>
  
</feed>
