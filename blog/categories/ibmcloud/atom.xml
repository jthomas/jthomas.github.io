<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ibmcloud | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/ibmcloud/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2019-05-10T16:19:44+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Provisioning IBM Cloud Services With Terraform]]></title>
    <link href="http://jamesthom.as/blog/2019/01/25/provisioning-ibm-cloud-services-with-terraform/"/>
    <updated>2019-01-25T10:09:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/01/25/provisioning-ibm-cloud-services-with-terraform</id>
    <content type="html"><![CDATA[<p><strong>This blog post will teach you how to provision <a href="https://cloud.ibm.com/catalog">applications services</a> on <a href="https://cloud.ibm.com/">IBM Cloud</a> with <a href="https://www.terraform.io/">Terraform</a>.</strong></p>

<p>Terraform is an open-source "<a href="https://en.wikipedia.org/wiki/Infrastructure_as_code">infrastructure-as-code</a>" tool. It allows cloud resources to be defined using a <a href="https://www.terraform.io/docs/configuration/syntax.html">declarative configuration file</a>. The <a href="https://www.terraform.io/docs/commands/index.html">Terraform CLI</a> then uses this file to automatically provision and maintain cloud infrastructure needed by your application. This allows the creation of reproducible environments in the cloud across your application life cycle.</p>

<p>IBM Cloud created an <a href="https://github.com/IBM-Cloud/terraform-provider-ibm">official provider plugin</a> for Terraform. This allows IBM Cloud services to be <a href="https://ibm-cloud.github.io/tf-ibm-docs/">declared</a> in Terraform configuration files. This is a much better approach than using the CLI or IBM Cloud UI to create application services manually.</p>

<p><strong>The following steps needed to set up Terraform with IBM Cloud will be explained.</strong></p>

<ul>
<li><em>Install Terraform CLI tools and IBM Cloud Provider Plugin.</em></li>
<li><em>Create API keys for platform access.</em></li>
<li><em>Terraform configuration for IBM Cloud services.</em></li>
<li><em>Terraform CLI commands to provision IBM Cloud services.</em></li>
</ul>


<p>Ready? Let's go! 😎😎😎</p>

<h2>Install Terraform</h2>

<ul>
<li><a href="https://www.terraform.io/intro/getting-started/install.html">Download and install</a> Terraform for your system.</li>
</ul>


<p>Once installed, the <code>terraform</code> command will be available.</p>

<p><code>
$ terraform
Usage: terraform [-version] [-help] &lt;command&gt; [args]
...
</code></p>

<h2>Install IBM Cloud Terraform Plugin</h2>

<ul>
<li>Download the IBM Cloud Terraform plugin binary from the <a href="https://github.com/IBM-Cloud/terraform-provider-ibm/releases">Github releases page</a>.</li>
<li>Unzip the release archive to extract the plugin binary (<code>terraform-provider-ibm_vX.Y.Z</code>).</li>
<li>Move the binary into the <a href="https://www.terraform.io/docs/configuration/providers.html#third-party-plugins">Terraform plugins directory</a> for the platform.

<ul>
<li><em>Linux/Unix/OS X:</em> <code>~/.terraform.d/plugins</code></li>
<li><em>Windows:</em> <code>%APPDATA%\terraform.d\plugins</code></li>
</ul>
</li>
</ul>


<h2>IBM Cloud Authentication Credentials</h2>

<p>IBM Cloud's Terraform provider plugin needs authentication credentials to interact with the platform. This is best handled by creating an API key and exporting as an environment variable. API keys can be created from the <a href="https://console.bluemix.net/docs/cli/reference/ibmcloud/download_cli.html#install_use">IBM Cloud CLI</a> or the <a href="https://cloud.ibm.com/iam#/users">web site</a>.</p>

<h3>using the cli</h3>

<ul>
<li>Run the <a href="https://console.bluemix.net/docs/cli/reference/ibmcloud/cli_api_policy.html#ibmcloud_iam_api_key_create">following command</a> to generate an API key.</li>
</ul>


<p><code>
ibmcloud iam api-key-create terraform-api-key
</code></p>

<p>The <code>apikey</code> property in the JSON output is the API key value.</p>

<p>```
{</p>

<pre><code>"name": "terraform-api-key",
"description": "...",
"apikey": "xxx-yyy-zzz",
"createdAt": "...",
"locked": false,
"uuid": "..."
</code></pre>

<p>}
```</p>

<p><em>Store this value securely. API keys cannot be retrieved after creation!</em></p>

<h3>using the web site.</h3>

<ul>
<li>From the <a href="https://cloud.ibm.com/iam#/users">IAM Users page</a>, select a user account.</li>
<li>Under the "<em>API keys</em>" table, click the "<em>Create an IBM Cloud API Key</em>" button.</li>
<li>Give the key a name and (optional) description.</li>
<li>Make a note of the API key value returned. API keys cannot be retrieved after creation.</li>
</ul>


<h3>exporting as an environment variable</h3>

<ul>
<li>Expose the API key as an environment variable to provide credentials to Terraform.</li>
</ul>


<p><code>
export BM_API_KEY=API_KEY_VALUE
</code></p>

<h2>Terraform configuration</h2>

<p>We can now start to write configuration files to describe IBM Cloud services we want to provision. Terraform configuration files are human-readable text files, ending with the <code>.tf</code> extension, which contain <a href="https://github.com/hashicorp/hcl">HashiCorp Configuration Language</a> (HCL) syntax.</p>

<p>IBM Cloud platform services come in two flavours: IAM managed resource instances and older Cloud Foundry-based service instances. This is due to the history of IBM Cloud starting as Bluemix, a Cloud Foundry-based cloud platform. Both platform services types can be provisioned using Terraform.</p>

<p>Most IBM Cloud platform services are available today as "<strong>resource instances</strong>".</p>

<h3>create new configuration file</h3>

<ul>
<li>Create a new <code>infra.tf</code> file which contains the following syntax.</li>
</ul>


<p><code>
provider "ibm" {}
</code></p>

<h3>add resource instances</h3>

<p><a href="https://ibm-cloud.github.io/tf-ibm-docs/v0.14.1/r/resource_instance.html">Resource instances</a> can be added to the configuration file as follows.</p>

<p><code>
resource "ibm_resource_instance" "resource_instance_name" {
  name              = "test"
  service           = "service-id"
  plan              = "service-plan"
  location          = "region-info"
}
</code></p>

<ul>
<li><code>resource_instance_name</code> - identifier for this service in the configuration, referenced by service keys.</li>
<li><code>name</code> - user-provided service name used by the platform to identify service.</li>
<li><code>service</code> - service identifier on the platform (can be found in the service documentation page).</li>
<li><code>plan</code> - service plan used for billing.</li>
<li><code>location</code> - cloud region used during service provisioning.</li>
</ul>


<p>Here is an example of provisioning a <a href="https://cloud.ibm.com/catalog/services/cloudant">Cloudant</a> database using the <code>ibm_resource_instance</code> configuration.</p>

<p><code>
resource "ibm_resource_instance" "cloudant" {
  name              = "my-cloudant-db"
  service           = "cloudantnosqldb"
  plan              = "lite"
  location          = "us-south"
}
</code></p>

<p><em>Other parameters are supported for resource configuration, see the <a href="https://ibm-cloud.github.io/tf-ibm-docs/v0.14.1/r/resource_instance.html">docs</a> for more details...</em></p>

<h3>add resource keys</h3>

<p>Applications accessing resource instances need service credentials. Access keys can also be provisioned using <a href="https://ibm-cloud.github.io/tf-ibm-docs/v0.14.1/r/resource_key.html">Terraform configuration</a>.</p>

<p><code>
resource "ibm_resource_key" "resource_key_name" {
  name                 = "my-key-name"
  role                 = "&lt;IAM_ROLE&gt;"
  resource_instance_id = "${ibm_resource_instance.resource_instance_name.id}"
}
</code></p>

<ul>
<li><code>name</code> - user-provided key name used by the platform to identify the credentials.</li>
<li><code>role</code> - IBM Cloud <a href="https://cloud.ibm.com/docs/iam/users_roles.html#iamusermanrol">IAM roles</a> (as supported by the service, e.g. Writer or Reader).</li>
</ul>


<p>Here is an example of provisioning a resource key for the <a href="https://cloud.ibm.com/catalog/services/cloudant">Cloudant</a> example from above.</p>

<p><code>
resource "ibm_resource_key" "cloudant_key" {
  name                  = "my-db-key"
  role                  = "Manager"
  resource_instance_id  = "${ibm_resource_instance.cloudant.id}"
}
</code></p>

<h3>(optional) add services instances to configuration</h3>

<p>Use the <a href="https://ibm-cloud.github.io/tf-ibm-docs/v0.14.1/r/service_instance.html">following configuration</a> to provision older Cloud Foundry services.</p>

<p><code>
resource "ibm_service_instance" "service_instance_name" {
  name       = "test"
  space_guid = "cf-space-guid"
  service    = "service-id"
  plan       = "service-plan"
}
</code></p>

<ul>
<li><code>service_instance_name</code> - identifier for this service in the configuration, referenced by service keys.</li>
<li><code>name</code> - user-provided service name used by the platform to identify the service.</li>
<li><code>service</code> - service identifier on the platform (can be found in the service documentation page).</li>
<li><code>plan</code> - service plan used for billing.</li>
</ul>


<h3>(optional) add service instance keys</h3>

<p>Applications accessing service instances need service credentials. Service keys can also be provisioned using <a href="https://ibm-cloud.github.io/tf-ibm-docs/v0.14.1/r/service_key.html">Terraform configuration</a>.</p>

<p><code>
resource "ibm_service_key" "service_key_name" {
  name                 = "my-key-name"
  service_instance_guid = "${ibm_service_instance.service_instance_name.id}"
}
</code></p>

<ul>
<li><code>name</code> - user-provided key name used by the platform to identify the credentials.</li>
<li><code>service_instance_guid</code> - Service instance GUID.</li>
</ul>


<h3>add output configuration</h3>

<p>Accessing service keys and other service details is handled with <code>output</code> <a href="https://www.terraform.io/docs/configuration/outputs.html">configuration</a> in Terraform files.</p>

<p><code>
output "app_credentials" {
  value = "${ibm_resource_key.resource_key_name.credentials}"
}
</code></p>

<p>Output values can be logged to the console using the <a href="https://www.terraform.io/docs/commands/output.html">Terraform CLI</a>.</p>

<p>Here is an example of accessing Cloudant credentials provisioned in the example above.</p>

<p><code>
output "cloudant_credentials" {
  value = "${ibm_resource_key.cloudant_key.credentials}"
}
</code></p>

<h2>Run Terraform commands</h2>

<p>Having finished the configuration file to describe our applications services, the Terraform CLI can now provision those services!</p>

<ul>
<li><a href="https://www.terraform.io/docs/commands/init.html">Initialise</a> the terraform project.</li>
</ul>


<p><code>
terraform init
</code></p>

<ul>
<li><a href="https://www.terraform.io/docs/commands/validate.html">Validate</a> the configuration file for syntax errors.</li>
</ul>


<p><code>
terraform validate
</code></p>

<ul>
<li><a href="https://www.terraform.io/docs/commands/plan.html">Display</a> the platform changes to be executed on the configuration file.</li>
</ul>


<p><code>
terraform plan
</code></p>

<p><em>Here is the example output from running that command with the Cloudant database example.</em></p>

<p>```
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be
persisted to local or remote state storage.</p>

<hr />

<p>An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create</p>

<p>Terraform will perform the following actions:</p>

<ul>
<li><p>ibm_resource_instance.cloudant
  id:                   <computed>
  location:             "us-south"
  name:                 "my-cloudant-db"
  plan:                 "lite"
  service:              "cloudantnosqldb"
  status:               <computed></p></li>
<li><p>ibm_resource_key.cloudant_key
  id:                   <computed>
  credentials.%:        <computed>
  name:                 "my-db-key"
  parameters.%:         <computed>
  resource_instance_id: "${ibm_resource_instance.cloudant.id}"
  role:                 "Manager"
  status:               <computed></p></li>
</ul>


<p>Plan: 2 to add, 0 to change, 0 to destroy.</p>

<hr />

<p>```</p>

<ul>
<li><a href="https://www.terraform.io/docs/commands/apply.html">Execute</a> the planned changes using <code>apply</code>.</li>
</ul>


<p><code>
terraform apply -auto-approve
</code></p>

<p>Terraform will now provision the platform services, resources keys and output credentials to the console.</p>

<p><em>Here is the example output from running that command with the Cloudant database example.</em></p>

<p>```
ibm_resource_instance.cloudant: Creating...
  location: "" => "us-south"
  name:     "" => "my-cloudant-db"
  plan:     "" => "lite"
  service:  "" => "cloudantnosqldb"
  status:   "" => "<computed>"
ibm_resource_instance.cloudant: Still creating... (10s elapsed)
ibm_resource_instance.cloudant: Still creating... (20s elapsed)
ibm_resource_instance.cloudant: Creation complete after 21s (ID: ...)
ibm_resource_key.cloudant_key: Creating...
  credentials.%:        "" => "<computed>"
  name:                 "" => "my-db-key"
  parameters.%:         "" => "<computed>"
  resource_instance_id: "" => "crn:v1:bluemix:public:cloudantnosqldb:us-south:a/...::"
  role:                 "" => "Manager"
  status:               "" => "<computed>"
ibm_resource_key.cloudant_key: Creation complete after 8s (ID: ...)</p>

<p>Apply complete! Resources: 2 added, 0 changed, 0 destroyed.</p>

<p>Outputs:</p>

<p>cloudant_credentials = {
  apikey = <API_KEY_VALUE>
  host = <DB_HOST>
  ...
}
```</p>

<p><strong>API keys from the <code>cloudant_credentials</code> output section can be used applications to interact with the provisioned database! 👏👏👏</strong></p>

<h2>Conclusion</h2>

<p>Provisioning cloud services using Terraform is a great way to manage application resources on IBM Cloud.</p>

<p>Applications resources are defined in a declarative configuration file, following the "infrastructure-as-code" approach to managing cloud environments. This configuration is maintained in the application's source code repository to enable reproducible environments.</p>

<p>IBM Cloud provides an official provider plugin for Terraform. This allows IBM Cloud services to be defined through custom configuration primitives. Developers can then use the Terraform CLI to provision new resources and extract service keys needed to access those services. 💯💯💯</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Loosely-coupled Serverless Functions With Apache Openwhisk]]></title>
    <link href="http://jamesthom.as/blog/2019/01/18/loosely-coupled-serverless-functions-with-openwhisk/"/>
    <updated>2019-01-18T15:10:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/01/18/loosely-coupled-serverless-functions-with-openwhisk</id>
    <content type="html"><![CDATA[<p>Just like software engineering, <a href="https://medium.com/@PaulDJohnston/serverless-best-practices-b3c97d551535">best practices for serverless applications</a> advise keeping functions small and focused on a single task, aka "<a href="https://en.wikipedia.org/wiki/Unix_philosophy#Do_One_Thing_and_Do_It_Well">do one thing and do it well</a>". Small single-purpose functions are easier to develop, test and debug. 👍</p>

<p><strong>But what happens when you need execute multiple asynchronous tasks (implemented as separate functions) from an incoming event, like an API request?</strong> 🤔</p>

<h2>Functions Calling Functions?</h2>

<p>Functions can invoke other functions directly, using asynchronous calls through the client SDK. This works at the cost of introducing <a href="https://en.wikipedia.org/wiki/Coupling_%28computer_programming%29">tighter coupling</a> between functions, which is generally avoided in software engineering! Disadvantages of this approach include...</p>

<ul>
<li><em>Functions which call other functions can be more difficult to test. Test cases needs to mock out the client SDK to remove side-effects during unit or integration tests.</em></li>
<li><em>It can lead to repetitive code if you want to fire multiple tasks with the same event. Each invocation needs to manually handle error conditions and re-tries on network or other issues, which complicates the business logic.</em></li>
<li><em>Modifying the functions being invoked cannot be changed dynamically. The function doing the invoking has to be re-deployed with updated code.</em></li>
</ul>


<p><a href="https://twitter.com/PaulDJohnston">Some people</a> have even labelled "<em>functions calling functions</em>" an <a href="https://medium.com/@PaulDJohnston/serverless-best-practices-b3c97d551535">anti-pattern</a> in serverless development! 😱</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Most common Serverless mistake?<br><br>Functions calling other functions<br><br>Why do people make this mistake?<br><br>Because people assume they should build functions like microservices and then use them in a similar way.<br><br>Causes no end of problems</p>&mdash; Serverless / Green Data Advocate (@PaulDJohnston) <a href="https://twitter.com/PaulDJohnston/status/1085106548270088193?ref_src=twsrc%5Etfw">January 15, 2019</a></blockquote>


<p><strong>Hmmm... so what should we do?</strong></p>

<p>Apache OpenWhisk has an awesome feature to help with this problem, triggers and rules! 👏</p>

<h2>OpenWhisk Triggers &amp; Rules</h2>

<p>Triggers and Rules in OpenWhisk are similar to the <a href="https://en.wikipedia.org/wiki/Observer_pattern">Observer pattern</a> from software engineering.</p>

<p>Users can fire "events" in OpenWhisk by invoking a named <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md#creating-triggers">trigger</a> with parameters. <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md#using-rules">Rules</a> are used to "subscribe" actions to all events for a given trigger name. Actions are invoked with event parameters when a trigger is fired. Multiple rules can be configured to support multiple "listeners" to the same trigger events. Event senders are decoupled from event receivers.</p>

<p>{% img /images/loose-coupling-openwhisk/t-r-a.png %}</p>

<p>Developers using OpenWhisk are most familiar with triggers when used with <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/catalog.md">feed providers</a>. This is used to subscribe actions to external event sources. The feed provider is responsible for listening to the event source and automatically firing trigger events with event details.</p>

<p><strong>But triggers can be fired manually from actions to provide custom event streams!</strong> 🙌</p>

<p>```javascript
const openwhisk = require('openwhisk')
const params = {msg: 'event parameters'}</p>

<p>// replace code like this...
const result = await ow.actions.invoke({name: "some-action", params})</p>

<p>// ...with this
const result = await ow.triggers.invoke({name: "some-trigger", params})
```</p>

<p>This allows applications to move towards an <a href="https://en.wikipedia.org/wiki/Event-driven_architecture">event-driven architecture</a> and promotes loose-coupling between functions with all the associated benefits for testing, deployment and scalability. 👌</p>

<h3>creating triggers</h3>

<p>Triggers are managed through the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md">platform API</a>. They can be created, deleted, retrieved and fired using  HTTP requests. Users normally interact with triggers through the <a href="https://github.com/apache/incubator-openwhisk-cli">CLI</a> or <a href="https://github.com/apache/incubator-openwhisk-client-js/">platform SDKs</a>.</p>

<p>Triggers can be created using the following CLI command.</p>

<p><code>
wsk trigger create &lt;TRIGGER_NAME&gt;
</code></p>

<h3>default parameters</h3>

<p>Triggers support <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/parameters.md#setting-default-parameters-on-an-action">default parameters</a> like actions. Default parameters are stored in the platform and included in all trigger events. If the event object includes parameters with the same key, default parameter values are ignored.</p>

<p><code>
wsk trigger create &lt;TRIGGER_NAME&gt; -p &lt;PARAM&gt; &lt;PARAM_VALUE&gt; -p &lt;PARAM_2&gt; &lt;PARAM_VALUE&gt; ...
</code></p>

<h3>binding triggers to actions with rules</h3>

<p>Rules bind triggers to actions. When triggers are fired, all actions connected via rules are invoked with the trigger event. Multiple rules can refer to the same trigger supporting multiple listeners to the same event.</p>

<p>Rules can also be created using the following CLI command.</p>

<p><code>
wsk rule create RULE_NAME TRIGGER_NAME ACTION_NAME
</code></p>

<p>Tools like <a href="https://github.com/serverless/serverless-openwhisk">The Serverless Framework</a> and <a href="https://github.com/apache/incubator-openwhisk-wskdeploy">wskdeploy</a> allow users to configure triggers and rules declaratively through YAML configuration files.</p>

<h3>firing triggers</h3>

<p>The JS SDK can be used to <a href="https://github.com/apache/incubator-openwhisk-client-js#fire-trigger">fire triggers programatically</a> from applications.</p>

<p><code>javascript
const openwhisk = require('openwhisk')
const name = 'sample-trigger'
const params = {msg: 'event parameters'}
const result = ow.triggers.invoke({name, params})
</code></p>

<p>CLI commands (<code>wsk trigger fire</code>) can fire triggers manually with event parameters for testing.</p>

<p><code>
wsk trigger fire sample-trigger -p msg "event parameters"
</code></p>

<h3>activation records for triggers</h3>

<p>Activation records are created for trigger events. These activation records contain event parameters, rules fired, activations ids and invocation status for each action invoked. This is useful for debugging trigger events when issues are occurring.</p>

<p><code>
$ wsk trigger fire sample-trigger -p hello world
ok: triggered /_/sample-trigger with id &lt;ACTIVATION_ID&gt;
$ wsk activation get &lt;ACTIVATION_ID&gt;
ok: got activation &lt;ACTIVATION_ID&gt;
{
 ...
}
</code></p>

<p>The <code>response.result</code> property in the activation record contains the fired trigger event (combining default and event parameter values).</p>

<p>Rules fired by the trigger are recorded in activation records as the JSON values under the <code>logs</code> parameter.</p>

<p><code>json
{
  "statusCode": 0,
  "success": true,
  "activationId": "&lt;ACTION_ACTIVATION_ID&gt;",
  "rule": "&lt;RULE_NAME&gt;",
  "action": "&lt;ACTION_NAME&gt;"
}
</code></p>

<p><em>Activation records are only generated when triggers have enabled rules with valid actions attached</em></p>

<h2>Example - WC Goal Bot</h2>

<p>This is great in theory but what about in practice?</p>

<p><a href="https://github.com/jthomas/goalbot">Goal Bot</a> was a small serverless application I built in 2018 for the World Cup. It was a <a href="https://twitter.com/WC2018_Goals">Twitter bot</a> which tweeted out all goals scored in real-time. The application used the  "actions connected via triggers events" architecture pattern. This made development and testing easier and faster.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">⚽️ GOAL ⚽️<br>👨 Harry MAGUIRE (󠁿🏴󠁧󠁢󠁥󠁮󠁧󠁿 ) @ 30&#39;. 👨<br>🏟 Sweden 🇸🇪 (0) v England 󠁿🏴󠁧󠁢󠁥󠁮󠁧󠁿 (1) 🏟<a href="https://twitter.com/hashtag/WorldCup?src=hash&amp;ref_src=twsrc%5Etfw">#WorldCup</a></p>&mdash; WC 2018 Goal Bot (@WC2018_Goals) <a href="https://twitter.com/WC2018_Goals/status/1015604110006120448?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>This function has two functions <code>goals</code> and <code>twitter</code>.</p>

<p><code>goals</code> was <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L11-L16">responsible</a> for detecting new goals scored using an external API. When invoked, it would retrieve all goals currently scored in the World Cup. Comparing the API response to a previous cached version calculated new goals scored. This function was connected to the alarm event source to run once a minute.</p>

<p><code>twitter</code> was <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L17-L22">responsible</a> for sending tweets from the @WC_Goals account. Twitter's API was used to create  goal tweets constructed from the event parameters.</p>

<p><strong>Goal events detected in the <code>goals</code> function need to be used to invoke the <code>twitter</code> function.</strong></p>

<p>Rather than the <code>goals</code> function invoke the <code>twitter</code> function directly, a trigger event (<code>goal</code>) was <a href="https://github.com/jthomas/goalbot/blob/master/lib/goal_tracker.js#L39-L41">fired</a>. The <code>twitter</code> function was bound to the <code>goal</code> trigger using a <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L21-L22">custom rule</a>.</p>

<p>{% img /images/loose-coupling-openwhisk/goalbot.png %}</p>

<p>De-coupling the two tasks in my application (checking for new goals and creating tweets) using triggers and rules had the following benefits...</p>

<ul>
<li><p>The <code>goals</code> function could be invoked in testing without tweets being sent. By disabling the rule binding the <code>twitter</code> function to the trigger, the goals function can fire events without causing side-effects.</p></li>
<li><p>Compared to having a "mono-function" combining both tasks, splitting tasks into functions means the <code>twitter</code> function can be tested with manual events, rather than having to manipulate the database and stub API responses to generate the correct test data.</p></li>
<li><p>It would also be easy to extend this architecture with additional notification services, like slack bots. New notification services could be attached to the same trigger source with an additional rule. This would not require any changes to the <code>goals</code> or <code>twitter</code> functions.</p></li>
</ul>


<h2>Triggers versus Queues</h2>

<p>Another common solution to de-coupling functions in serverless architectures is using <a href="https://theburningmonk.com/2018/04/what-is-the-best-event-source-for-doing-pub-sub-with-aws-lambda/">message queues</a>.</p>

<p>Functions push events in external queues, rather than invoking triggers directly. Event sources are responsible for firing the registered functions with new messages. Apache OpenWhisk <a href="https://github.com/apache/incubator-openwhisk-package-kafka">supports Kafka</a> as an event source which could be used with this approach.</p>

<p><em>How does firing triggers directly compare to pushing events into an external queue (or other event source)?</em></p>

<p>Both queues and triggers can be used to achieve the same goal ("<em>connect functions via events</em>") but have different semantics. It is important to understand the benefits of both to choose the most appropriate architecture for your application.</p>

<h3>benefits of using triggers against queues</h3>

<p>Triggers are built into the Apache OpenWhisk platform. There is no configuration needed to use them. External event sources like queues need to be provisioned and managed as additional cloud services.</p>

<p>Trigger invocations are free in IBM Cloud Functions. IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">charges only</a> for execution time and memory used in functions. Queues will incur additional usage costs based on the service's pricing plan.</p>

<h3>disadvantages of using triggers against queues</h3>

<p>Triggers are not queues. Triggers are not queues. Triggers are not queues. 💯</p>

<p>If a trigger is fired and no actions are connected, the event is lost. Trigger events are not persisted until listeners are attached. <strong>If you need event persistence, message priorities, disaster recovery and other advanced features provided by message queues, use a message queue!</strong></p>

<p>Triggers are subject to <a href="https://console.bluemix.net/docs/openwhisk/openwhisk_reference.html#openwhisk_syslimits">rate limiting</a> in Apache OpenWhisk. In IBM Cloud Functions, this defaults to 1000 concurrent invocations and 5000 total invocations per namespace per minute. These limits can be raised through a support ticket but there are practical limits to the maximum rates allowed. Queues have support for much higher throughput rates.</p>

<p>External event providers are also responsible for handling the retries when triggers have been rate-limited due to excess events. Invoking triggers manually relies on the invoking function to handle this. Emulating retry behaviour from an event provider is impractical due to costs and limits on function duration.</p>

<h2>Other hints and tips</h2>

<p><strong><em>Want to invoke an action which fires triggers without setting off listeners?</em></strong></p>

<p>Rules can be dynamically disabled without having to remove them. This can be used during integration testing or debugging issues in production.</p>

<p><code>
wsk rule disable RULE_NAME
wsk rule enable RULE_NAME
</code></p>

<p><strong><em>Want to verify triggers are fired with correct events without mocking client libraries?</em></strong></p>

<p>Trigger events are not logged unless there is at least one enabled rule. Create a new rule which binds the <code>/whisk.system/utils/echo</code> action to the trigger. This built-in function just returns input parameters as the function response. This means the activation records with trigger events will now be available.</p>

<h2>conclusion</h2>

<p>Building event-driven serverless applications from loosely-coupled functions has numerous benefits including development speed, improved testability, deployment velocity, lower costs and more.</p>

<p>Decomposing "monolithic" apps into independent serverless functions often needs event handling functions to trigger off multiple backend operations, implemented in separate serverless functions. Developers unfamiliar with serverless often resort to direct function invocations.</p>

<p>Whilst this works, it introduces tight coupling between those functions, which is normally avoided in software engineering. This approach has even been highlighted as a "serverless" anti-pattern.</p>

<p>Apache OpenWhisk has an awesome feature to help with this problems, triggers and rules!</p>

<p>Triggers provide a lightweight event firing mechanism in the platform. Rules bind actions to triggers to automate invoking actions when events are fired. Applications can fire trigger events to invoke other operations, rather than using direct invocations. This keeps the event sender and receivers de-coupled from each other. 👏</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Highly Available Serverless Apps With Cloudant's Cross-Region Replication]]></title>
    <link href="http://jamesthom.as/blog/2019/01/10/highly-available-serverless-apps-with-cloudant-cross-region-replication/"/>
    <updated>2019-01-10T11:23:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/01/10/highly-available-serverless-apps-with-cloudant-cross-region-replication</id>
    <content type="html"><![CDATA[<p>Building <a href="https://www.techrepublic.com/blog/the-enterprise-cloud/what-high-availability-for-cloud-services-means-in-the-real-world/">highly available</a> serverless applications relies on eliminating "<a href="https://en.wikipedia.org/wiki/Single_point_of_failure"><em>single points of failure</em></a>" from application architectures.</p>

<p><a href="https://cloud.ibm.com/docs/tutorials/multi-region-serverless.html#deploy-serverless-apps-across-multiple-regions">Existing tutorials</a> showed how to deploy the same serverless application on IBM Cloud in different regions. Using the <a href="https://cloud.ibm.com/docs/infrastructure/cis/glb.html#global-load-balancer-glb-concepts">Global Load Balancer</a> from <a href="https://cloud.ibm.com/catalog/services/internet-svcs">IBM Cloud Internet Services</a>, traffic is distributed across multiple applications from the same hostname. The <a href="https://cloud.ibm.com/docs/infrastructure/cis/glb.html#global-load-balancer-glb-concepts">Global Load Balancer</a> automatically detects outages in the regional applications and redirects traffics as necessary.</p>

<p><strong>But what if all instances rely on the same database service and that has issues?</strong> 😱🔥</p>

<p>In addition to running multiple instances of the application, independent databases in different regions are also necessary for a highly available serverless application. Maintaining consistent application state across regions needs all database changes to be automatically synchronised between instances. 🤔</p>

<p><strong>In this blog post, we're going to look at using <a href="https://cloud.ibm.com/catalog/services/cloudant">IBM Cloudant's</a> <a href="https://console.bluemix.net/docs/services/Cloudant/guides/active-active.html#configuring-ibm-cloudant-for-cross-region-disaster-recovery">replication service</a> to set up a "<a href="https://en.wikipedia.org/wiki/Multi-master_replication">multi-master</a>" replication between regional database instances.</strong></p>

<p>Once this is enabled, database changes will automatically be synchronised in real-time between all database instances. Serverless applications can use their regional database instance and be confident application state will be consistent globally (for some definition of <a href="https://en.wikipedia.org/wiki/Eventual_consistency">consistent</a>...). 💯</p>

<h2>example serverless application - todo backend</h2>

<p>This <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">serverless application</a> implements a <a href="https://www.todobackend.com/">TODO backend</a> using <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> and <a href="https://cloud.ibm.com/catalog/services/cloudant">IBM Cloudant</a>.</p>

<p>{% img /images/ha-serverless-apps-todo/todo-frontpage.png %}</p>

<p>It provides an REST API for interacting with a TODO service. This can be used with the <a href="https://www.todobackend.com/client/index.html">front-end client</a> to add, complete and remove todos from a list.</p>

<p><strong>Let's make this <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">example serverless application</a> "highly available". 👍</strong></p>

<p>The application will be deployed to two different IBM Cloud regions (London and Dallas). Separate database instances will be provisioned in each region. Applications will use their regional database instance but share global state via replication.</p>

<p>{% img /images/ha-serverless-apps-todo/architecture.png %}</p>

<h2>deploy serverless app to multiple regions</h2>

<p>This Github <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">repo</a> has an <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis/blob/master/deploy.sh">automatic deployment script</a> to deploy the serverless application (using <code>wskdeploy</code>) and application services (using <code>terraform</code>).</p>

<p><strong><em>Install the prerequisites listed <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis#code-and-tools">here</a> before proceeding with these instructions.</em></strong></p>

<h3>download example application</h3>

<ul>
<li>Clone the Git repository to a local directory.</li>
</ul>


<p><code>
git clone https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis
</code></p>

<ul>
<li>Enter the source code directory.</li>
</ul>


<p><code>
cd ibm-cloud-functions-refarch-serverless-apis
</code></p>

<h3>create IAM key for serverless app</h3>

<p><em>Have you already signed up for an <a href="https://cloud.ibm.com/registration">IBM Cloud account</a> and <a href="https://cloud.ibm.com/docs/cli/reference/ibmcloud/download_cli.html#install_use">installed the CLI</a>? If not, please do that before proceeding.</em></p>

<ul>
<li>Create an IAM key which will be used to deploy the serverless application.</li>
</ul>


<p><code>
ibmcloud iam api-key-create serverless_api --file serverless_api.apikey
</code></p>

<h3>configure deployment variables</h3>

<ul>
<li>Create the <code>local.env</code> file in the current directory will the following contents.</li>
</ul>


<p><code>
IBMCLOUD_API_KEY=&lt;IAM_API_KEY&gt;
IBMCLOUD_ORG=&lt;YOUR_ORG&gt;
IBMCLOUD_SPACE=&lt;REGION_SPACE&gt;
IBMCLOUD_REGION=
PROVISION_INFRASTRUCTURE=true
API_USE_APPID=false
</code></p>

<ul>
<li>Replace the <code>&lt;IAM_API_KEY&gt;</code> value with the <code>apikey</code> value from the <code>serverless_api.apikey</code> file.</li>
<li>Replace the <code>&lt;IBMCLOUD_ORG&gt;</code> value with an <a href="https://cloud.ibm.com/docs/account/orgs_spaces.html#orgsspacesusers">IBM Cloud organisation</a>.</li>
<li>Replace the <code>&lt;IBMCLOUD_SPACE&gt;</code> value with an <a href="https://cloud.ibm.com/docs/account/orgs_spaces.html#orgsspacesusers">IBM Cloud space</a>.</li>
</ul>


<p>The <code>PROVISION_INFRASTRUCTURE</code> parameter makes the deployment script automatically provision all application resources using Terraform.</p>

<p>Secured API endpoints are not required for this demonstration. Setting the <code>API_USE_APPID</code> parameter to <code>false</code> disables authentication on the endpoints and provisioning the AppID service.</p>

<h3>deploy to london</h3>

<ul>
<li>Set the <code>IBMCLOUD_REGION</code> to <code>eu-gb</code> in the <code>local.env</code> file.</li>
<li>Run the following command to deploy the application and provision all application resources.</li>
</ul>


<p><code>
./deploy.sh --install
</code></p>

<p>If the deployment have succeed, the following message should be printed to the console.</p>

<p><code>
2019-01-08 10:51:51 All done.
ok: APIs
Action                                      Verb  API Name  URL
/&lt;ORG&gt;_&lt;SPACE&gt;/todo_package/todo/get_todo   get   todos     https://&lt;UK_APIGW_URL&gt;/todo
...
</code></p>

<ul>
<li>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the <a href="https://cloud.ibm.com/openwhisk/apimanagement">APIGW URL</a> shown in the console to interact with the remote TODO service in the London region.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/testing-app.gif %}</p>

<h3>deploy to dallas</h3>

<ul>
<li><p><strong>Rename the <code>terraform.tfstate</code> file in the <code>infra</code> folder to <code>terraform.tfstate.london</code></strong></p></li>
<li><p>Set the <code>IBMCLOUD_REGION</code> to <code>us-south</code> in the <code>local.env</code> file.</p></li>
<li>Run the following command to deploy the application and provision all application resources.</li>
</ul>


<p><code>
./deploy.sh --install
</code></p>

<p>If the deployment have succeed, the following message should be printed to the console.</p>

<p><code>
2019-01-08 10:51:51 All done.
ok: APIs
Action                                      Verb  API Name  URL
/&lt;ORG&gt;_&lt;SPACE&gt;/todo_package/todo/get_todo   get   todos     https://&lt;US_APIGW_URL&gt;/todo
...
</code></p>

<ul>
<li>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the <a href="https://cloud.ibm.com/openwhisk/apimanagement">APIGW URL</a> shown in the console to interact with the remote TODO service in the Dallas region.</li>
</ul>


<h2>configure cloudant cross-region replication</h2>

<p>There are now multiple copies of the same serverless application in different regions. Each region has an independent instance of Cloudant provisioned.</p>

<p><a href="https://console.bluemix.net/docs/services/Cloudant/api/replication.html">Cloudant replication</a> is a one-way synchronisation from a source to a destination database. To set up a <a href="https://console.bluemix.net/docs/services/Cloudant/guides/active-active.html#configuring-ibm-cloudant-for-cross-region-disaster-recovery">bi-directional data synchronisation</a>, two different replications will need to be configured.</p>

<h3>create api keys for replication access</h3>

<p>Before configuring replication between the regional databases, API keys need to be created to allow remote access on both hosts. API keys need to be created per regional instance.</p>

<ul>
<li>From the <a href="https://cloud.ibm.com/resources">IBM Cloud Resource List</a>, find the cloudant instances provisioned in London and Dallas.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/resource-list.png %}</p>

<ul>
<li>Open the Cloudant Dashboard for each service instance.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/opening-cloudant-dashboard.gif %}</p>

<p>Follow these instructions on both hosts to generate API keys for replication with the correct permissions.</p>

<ul>
<li>Click the "Databases" icon to show all the databases on this instance.</li>
<li>Click the 🔒 icon in the "todos" database row in the table to open the permissions page.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/databases-list.png %}</p>

<p><em>Can't find the "todos" database in the Cloudant dashboard? Make sure you interact with the TODO backend from the <a href="https://www.todobackend.com/client/index.html">front-end application</a>. This will automatically create the database if it doesn't exist.</em></p>

<ul>
<li>Click "Generate API Key" on the permissions page.</li>
<li>Make a note of the key identifier and password.</li>
<li>Set the <code>_reader_</code>, <code>_writer</code> and <code>_replicator</code> permissions for the newly created key.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/db-api-key.png %}</p>

<h3>set up cross-region replication</h3>

<p>Replication jobs need to be configured on both database hosts. These can be created from the Cloudant dashboard. <strong>Repeat these instructions on both hosts.</strong></p>

<ul>
<li>Open the Cloudant Dashboard for each service instance.</li>
<li>Click the "Replication" icon from the panel menu.</li>
<li>Click the "New Replication" button.</li>
<li>Set the following "Source" values in the "Job configuration" panel.

<ul>
<li>Type: <em>"Local Database"</em></li>
<li>Name: <em>"todos"</em></li>
<li>Authentication: <em>"Cloudant username or API Key"</em></li>
<li>Fill in the API key and password for this local database host in the input fields.</li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-source.png %}</p>

<ul>
<li>Set the following "Target" values in the "Job configuration" panel.

<ul>
<li>Type: <em>"Existing Remote Database"</em></li>
<li>Name: <em>"https://<REMOTE_CLOUDANT_HOST>/todos"</em></li>
<li>Authentication: <em>"Cloudant username or API Key"</em></li>
<li>Fill in the API key and password for the remote database host in the input fields.</li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-target.png %}</p>

<p><em>Wondering what the REMOTE_CLOUDANT_HOST is? Use hostname from the Cloudant dashboard, e.g. XXXX-bluemix.cloudant.com</em></p>

<ul>
<li>Set the following "Options" values in the "Job configuration" panel.

<ul>
<li>Replication type: <em>"Continuous"</em></li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-options.png %}</p>

<ul>
<li>Click "Start Replication"</li>
<li>Verify the replication table shows the new replication task state as "<em>Running</em>". 👍</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/replication-jobs-table.png %}</p>

<h2>test it out!</h2>

<p>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the APIGW URLs for each region simultaneously. Interactions with the todo list in one region should automatically propagate to the other region.</p>

<p>{% img /images/ha-serverless-apps-todo/todo-app.gif %}</p>

<p>The "Active Tasks" panel on the Cloudant Dashboard shows the documents replicated between instances and pending changes. If there are errors synchronising changes to the replication target, the host uses exponential backoff to re-try the replication tasks.</p>

<p><a href="https://console.bluemix.net/docs/services/Cloudant/guides/conflicts.html#finding-conflicts">Conflicts</a> between document changes are handled using CouchDB's <a href="http://guide.couchdb.org/draft/conflicts.html">conflict mechanism</a>. Applications are responsible for detecting and resolving document conflicts in the front-end.</p>

<h2>conclusion</h2>

<p>Running the same serverless application in multiple regions, using the GLB to proxy traffic, allows applications to manage regional outages. But what if all the application instances rely on the same database service? The "single point of failure" has shifted from the application runtime to the database host. 👎</p>

<p>Provisioning independent databases in each application regions is one solution. Applications use their regional database instance and are protected from issues in other regions. This strategy relies on database changes being synchronised between instances to keep the application state consistent. 👍</p>

<p>IBM Cloudant has a built-in replication service to synchronised changes between source and host databases. Setting up bi-directional replication tasks between all instances enables a  "multi-master" replication strategy. This allows applications to access any database instance and have the same state available globally. 🕺🕺🕺</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Using Custom Domains With IBM Cloud Functions]]></title>
    <link href="http://jamesthom.as/blog/2018/12/03/custom-domains-with-ibm-cloud-functions/"/>
    <updated>2018-12-03T15:11:00+00:00</updated>
    <id>http://jamesthom.as/blog/2018/12/03/custom-domains-with-ibm-cloud-functions</id>
    <content type="html"><![CDATA[<p>In this tutorial, I'm going to show you how to use a <a href="https://console.bluemix.net/docs//api-management/manage_apis.html#custom_domains">custom domain for serverless functions</a> exposed as APIs on <a href="https://cloud.ibm.com/">IBM Cloud</a>. APIs endpoints use a random sub-domain on IBM Cloud by default. Importing your own domains means endpoints can be accessible through custom URLs.</p>

<p><em>Registering a custom domain with IBM Cloud needs you to complete the following steps...</em></p>

<ul>
<li>Generate SSL/TLS certificates for your domain</li>
<li>Register domain certificates with <a href="https://console.bluemix.net/catalog/services/certificate-manager">IBM Certificate Manager</a></li>
<li>Bind a custom domain to <a href="https://cloud.ibm.com/openwhisk/apimanagement">Cloud Functions APIs</a> using the <a href="https://cloud.ibm.com/apis/">IBM Cloud APIs</a> console.</li>
</ul>


<p><strong>This tutorial assumes you already have actions on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> exposed as HTTP APIs using the built-in <a href="https://console.bluemix.net/openwhisk/apimanagement">API service</a>.</strong> If you haven't done that yet, please see the documentation <a href="https://console.bluemix.net/docs/openwhisk/">here</a> before you proceed.</p>

<p>The instructions below set up a sub-domain (<code>api.&lt;YOUR_DOMAIN&gt;</code>) to access serverless functions.</p>

<h2>Generating SSL/TLS Certificates with Let's Encrypt</h2>

<p>IBM Cloud APIs only supports HTTPS traffic with custom domains. Users needs to upload valid SSL/TLS certificates for those domains to IBM Cloud before being able to use them.</p>

<p><a href="https://letsencrypt.org/">Let's Encrypt</a> is a Certificate Authority which provides free SSL/TLS certificates for domains. Let's Encrypt is trusted by all root identify providers. This means certificates generated by this provider will be trusted by all major operating systems, web browsers, and devices.</p>

<p>Using this service, valid certificates can be generated to support custom domains on IBM Cloud.</p>

<h3>domain validation</h3>

<p>Let's Encrypt needs to verify you <a href="https://letsencrypt.org/how-it-works/">control the domain</a> before generating certificates.</p>

<p>During the verification process, the user makes an authentication token available through the domain. The service supports <a href="https://certbot.eff.org/docs/challenges.html">numerous methods</a> for exposing the authentication token, including HTTP endpoints, DNS TXT records or TLS SNI.</p>

<p>There is an application (<a href="https://certbot.eff.org/">certbot</a>) which automates generating authentication tokens and certificates.</p>

<p>I'm going to use the <a href="https://en.wikipedia.org/wiki/TXT_record">DNS TXT record</a> as the challenge mechanism. Using this approach, certbot will provide a random authentication token I need to create as the TXT record value under the <code>_acme-challenge.&lt;YOUR_DOMAIN&gt;</code> sub-domain before validation.</p>

<h3>using certbot with dns txt validation</h3>

<ul>
<li>Install <a href="https://certbot.eff.org/">certbot</a> into your <a href="https://certbot.eff.org/docs/install.html">environment</a>, e.g. using <a href="https://brew.sh/">Homebrew</a>.</li>
</ul>


<p><code>
brew install certbot
</code></p>

<ul>
<li>Run certbot in <a href="https://certbot.eff.org/docs/using.html#manual">manual mode</a> with the DNS <a href="https://certbot.eff.org/docs/challenges.html">challenge</a> method.</li>
</ul>


<p><code>
certbot certonly --manual --preferred-challenges=dns -d *.&lt;YOUR_DOMAIN&gt;
</code></p>

<p>I'm generating a wildcard certificate for any sub-domains under <code>&lt;YOUR_DOMAIN&gt;</code>. This allows me to use the same certificate with different sub-domains on IBM Cloud, rather than generating a certificate per sub-domain.</p>

<p>During the validation process, <code>certbot</code> should display the following message with the challenge token.</p>

<p>```
Please deploy a DNS TXT record under the name
_acme-challenge.<YOUR_DOMAIN> with the following value:</p>

<p><CHALLENGE_TOKEN></p>

<p>Before continuing, verify the record is deployed.</p>

<hr />

<p>Press Enter to Continue
```</p>

<h3>setting challenge token</h3>

<ul>
<li><p>Take the challenge token from <code>certbot</code> and create a new TXT record with this value for the <code>_acme-challenge.&lt;YOUR_DOMAIN&gt;</code> sub-domain.</p></li>
<li><p>Use the <code>dig</code> command to verify the TXT record is available.</p></li>
</ul>


<p><code>
dig -t txt _acme-challenge.&lt;YOUR_DOMAIN&gt;
</code></p>

<p>The challenge token should be available in the DNS response shown by <code>dig</code>. 👍</p>

<p><code>
;; ANSWER SECTION:
_acme-challenge.&lt;YOUR_DOMAIN&gt;. 3599 IN  TXT "&lt;CHALLENGE_TOKEN&gt;"
</code></p>

<ul>
<li>Press <kbd>Enter</kbd> in the terminal session running <code>certbot</code> when the challenge token is available.</li>
</ul>


<h3>retrieving domain certificates</h3>

<p><code>certbot</code> will now retrieve the TXT record for the sub-domain and verify it matches the challenge token. If the domain has been validated, <code>certbot</code> will show the directory containing the newly created certificates.</p>

<p><code>
IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at:
   /etc/letsencrypt/live/&lt;YOUR_DOMAIN&gt;/fullchain.pem
   Your key file has been saved at:
   /etc/letsencrypt/live/&lt;YOUR_DOMAIN&gt;/privkey.pem
   Your cert will expire on 2019-03-03.
...
</code></p>

<p><code>certbot</code>  creates the following files.</p>

<ul>
<li><code>cert.pem</code> - public domain certificate</li>
<li><code>privkey.pem</code> - private key for domain certificate</li>
<li><code>chain.pem</code> - intermediate domain certificates</li>
<li><code>fullchain.pem</code> - public and intermediate domain certificates in a single file.</li>
</ul>


<p><em>Registering the domain with IBM Cloud will require the public, private and intermediate certificate files.</em></p>

<h2>Registering Custom Domain with IBM Cloud</h2>

<p>Certificates for custom domains in IBM Cloud are managed by the <a href="https://console.bluemix.net/catalog/services/certificate-manager">Certificate Manager</a> service.</p>

<ul>
<li>Create a <a href="https://cloud.ibm.com/catalog/services/certificate-manager">new instance</a> of the service from the <a href="https://cloud.ibm.com/catalog/">IBM Cloud Catalog</a>.</li>
<li>From the service homepage, click the "<em>Import Certificate</em>" button.</li>
<li>Fill in the following fields in the import form. Use the generated certificate files in the upload fields.

<ul>
<li>Name</li>
<li>Certificate File (<code>cert.pem</code>)</li>
<li>Private key file (<code>privkey.pem</code>)</li>
<li>Intermediate certificate file (<code>chain.pem</code>)</li>
</ul>
</li>
</ul>


<p>After importing the certificate, check the certificate properties match the expected values</p>

<p>{% img /images/custom-domains/import-certs.gif %}</p>

<h2>Binding Domain to IBM Cloud Functions APIs</h2>

<p><a href="https://console.bluemix.net/docs/api-management/manage_apis.html#custom_domains_bind">Custom domains</a> for APIs on IBM Cloud are managed through the IBM Cloud APIs <a href="https://console.bluemix.net/apis/">console</a>.</p>

<ul>
<li>Open the "<a href="https://console.bluemix.net/apis/domains">Custom Domains</a>" section on the <a href="https://console.bluemix.net/apis/">IBM Cloud APIs</a> console.</li>
<li>Check the "Region" selector matches the region chosen for your actions and APIs.</li>
<li>Click the <code>···</code> icon on the row where "Organisation" and "Space" values match your APIs.</li>
<li>Click "<em>Change Settings</em>" from the pop-up menu.</li>
</ul>


<p>{% img /images/custom-domains/open-apis-settings.gif %}</p>

<h3>domain validation</h3>

<p>IBM Cloud now <a href="https://console.bluemix.net/docs//api-management/manage_apis.html#custom_domains">needs to verify</a> you control the custom domain being used.</p>

<p><em>Another DNS TXT record needs to be created <strong>before</strong> attempting to bind the domain.</em></p>

<ul>
<li>From the "<em>Custom Domain Settings</em>" menu, make a note of the "<em>Default domain / alias</em>" value. This should be in the format: <code>&lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud</code>.</li>
<li>Create a new TXT record for the custom sub-domain (<code>api.&lt;YOUR_DOMAIN&gt;</code>) with the default domain alias as the record value (<code>&lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud</code>).</li>
<li>Use the <code>dig</code> command to check the sub-domain TXT record exists and contains the correct value.</li>
</ul>


<p><code>
dig -t txt api.&lt;YOUR_DOMAIN&gt;
</code></p>

<p>The default domain alias value should be available in the DNS response shown by <code>dig</code>. 👍</p>

<p><code>
;; ANSWER SECTION:
api.&lt;YOUR_DOMAIN&gt;. 3599 IN  TXT "&lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud"
</code></p>

<p>Having created the TXT record, fill in the <em>Custom Domain Settings</em> form.</p>

<h3>custom domain settings</h3>

<ul>
<li>Select the "<em>Assign custom domain</em>" checkbox in the "<em>Custom domain settings</em>" form.</li>
<li>Fill in the following form fields.

<ul>
<li><em>Domain Name:</em> use the custom sub-domain to bind  (<code>api.&lt;YOUR-DOMAIN&gt;</code>).</li>
<li><em>Certificate Manager service</em>: select the certificate manger instance.</li>
<li><em>Certificate:</em> select the domain certificate from the drop-down menu.</li>
</ul>
</li>
<li>Click the "<em>Save</em>" button.</li>
</ul>


<p>Once the domain has been validated, the form will redirect to the custom domains overview. The "Custom Domain" field will now show the sub-domain bound to the correct default domain alias.</p>

<p>{% img /images/custom-domains/bind-custom-domain.gif %}</p>

<h3>add CNAME record</h3>

<ul>
<li>Remove the existing TXT record for the custom sub-domain  (<code>api.&lt;YOUR-DOMAIN&gt;</code>).</li>
<li>Add a new CNAME record mapping the custom sub-domain (<code>api.&lt;YOUR-DOMAIN&gt;</code>) to the "<em>Default domain  / alias</em>" on IBM Cloud (<code>&lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud</code>).</li>
<li>Use the <code>dig</code> command to check the CNAME record is correct.</li>
</ul>


<p><code>
dig -t CNAME api.&lt;YOUR_DOMAIN&gt;
</code></p>

<p>The default domain alias value should be available in the DNS response shown by <code>dig</code>. 👍</p>

<p><code>
;; ANSWER SECTION:
api.&lt;YOUR_DOMAIN&gt;.  3599    IN  CNAME   &lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud.
</code></p>

<h2>Testing It Out</h2>

<p>Functions should now be accessible through both the default domain alias and the new custom domain. 👏</p>

<ul>
<li>Invoke the default domain alias API URL for the function.</li>
</ul>


<p><code>
curl https://&lt;APP_ID&gt;.&lt;REGION&gt;.apiconnect.appdomain.cloud/&lt;BASE_PATH&gt;/&lt;SUB_PATH&gt;
</code></p>

<p><em>Both the <code>BASE_PATH</code> and <code>SUB_PATH</code> values come from the API definitions configured by the user.</em></p>

<ul>
<li>Invoke the custom domain API URL for the function.</li>
</ul>


<p><code>
curl https://api.&lt;YOUR_DOMAIN&gt;/&lt;BASE_PATH&gt;/&lt;SUB_PATH&gt;
</code></p>

<p><em>Make sure you use HTTPS protocol in the URL. IBM Cloud does not support HTTP traffic with custom domains.</em></p>

<p>Both responses for these URLs should the same! Hurrah. 😎</p>
]]></content>
  </entry>
  
</feed>
