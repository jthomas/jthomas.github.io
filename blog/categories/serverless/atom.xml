<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: serverless | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/serverless/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2019-02-28T11:21:27+00:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[OpenWhisk Web Action Errors With Sequences]]></title>
    <link href="http://jamesthom.as/blog/2019/02/27/openwhisk-web-action-errors-with-sequences/"/>
    <updated>2019-02-27T10:00:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/27/openwhisk-web-action-errors-with-sequences</id>
    <content type="html"><![CDATA[<p>This week, I came across an interesting problem when building HTTP APIs on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>.</p>

<p>{% blockquote  %}
How can Apache OpenWhisk Web Actions, implemented using action sequences, handle application errors that need the sequence to stop processing and a custom HTTP response to be returned?
{% endblockquote %}</p>

<p>This came from wanting to add custom <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication">HTTP authentication</a> to existing Web Actions. I had decided to enhance existing Web Actions with authentication using action sequences. This would combine a new action for authentication validation with the existing API route handlers.</p>

<p>{% img /images/sequences-and-web-actions/outline.png  %}</p>

<p>When the HTTP authentication is valid, the authentication action becomes a "<a href="https://en.wikipedia.org/wiki/NOP_(code)">no-op</a>", which passes along the HTTP request to the route handler action to process as normal.</p>

<p><strong>But what happens when authentication fails?</strong></p>

<p>The authentication action needs to stop request processing and return a <a href="https://httpstatuses.com/401">HTTP 401</a> response immediately.</p>

<p>{% img /images/sequences-and-web-actions/options.png  %}</p>

<p><em>Does Apache OpenWhisk even support this?</em></p>

<p>Fortunately, it does (phew) and I eventually worked out how to do this (based on a combination of re-reading <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/">documentation</a>, the platform <a href="https://github.com/apache/incubator-openwhisk/blob/master/core/controller/src/main/scala/org/apache/openwhisk/core/controller/WebActions.scala">source code</a> and just trying stuff out!).</p>

<p><em>Before explaining how to return custom HTTP responses using web action errors in sequences, let's review web actions, actions sequences and why developers often use them together...</em></p>

<h2>Web Actions</h2>

<p><a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">Web Actions</a> are OpenWhisk <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md">actions</a> that can be invoked using external HTTP requests.</p>

<p>Incoming HTTP requests are provided as event parameters. HTTP responses are controlled using attributes (<code>statusCode</code>, <code>body</code>, <code>headers</code>) in the action result.</p>

<p>Web Actions can be invoked directly, using the platform API, or connected to <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/apigateway.md">API Gateway endpoints</a>.</p>

<h3>example</h3>

<p>Here is an example Web Action that returns a static HTML page.</p>

<p>```javascript
function main() {
  return {</p>

<pre><code>headers: {      
  'Content-Type': 'text/html'
},
statusCode: 200,
body: '&lt;html&gt;&lt;body&gt;&lt;h3&gt;hello&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt;'
</code></pre>

<p>  }
}
```</p>

<h3>exposing web actions</h3>

<p>Web actions can be exported from any existing action by setting an <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/annotations.md#annotations-specific-to-web-actions">annotation</a>.</p>

<p>This is handled automatically by CLI using the <code>—web</code> configuration flag when creating or updating actions.</p>

<p><code>
wsk action create ACTION_NAME ACTION_CODE --web true
</code></p>

<h2>Action Sequences</h2>

<p>Multiple actions can be composed together into a "meta-action" using <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md#creating-action-sequences">sequences</a>.</p>

<p>Sequence configuration defines a series of existing actions to be called sequentially upon invocation.  Actions connected in sequences can use different runtimes and even be sequences themselves.</p>

<p><code>
wsk action create mySequence --sequence action_a,action_b,action_c
</code></p>

<p>Input events are passed to the first action in the sequence. Action results from each action in the sequence are passed to the next action in the sequence. The response from the last action in the sequence is returned as the action result.</p>

<h3>example</h3>

<p>Here is a sequence (<code>mySequence</code>) composed of three actions (<code>action_a</code>, <code>action_b</code>, <code>action_c</code>).</p>

<p><code>
wsk action create mySequence --sequence action_a,action_b,action_c
</code></p>

<p>Invoking <code>mySequence</code> will invoke <code>action_a</code> with the input parameters. <code>action_b</code> will be invoked with the result from <code>action_a</code>.  <code>action_c</code> will be invoked with the result from <code>action_b</code>. The result returned by <code>action_c</code> will be returned as the sequence result.</p>

<h2>Web Actions from Action Sequences</h2>

<p>Using Action Sequences as Web Actions is a useful pattern for externalising common HTTP request and response processing tasks into separate serverless functions.</p>

<p>These common actions can be included in multiple Web Actions, rather than manually duplicating the same boilerplate code in each HTTP route action. This is similar to the "<a href="https://dzone.com/articles/understanding-middleware-pattern-in-expressjs">middleware</a>" pattern used by lots of common web application frameworks.</p>

<p>Web Actions using this approach are easier to test, maintain and allows API handlers to implement core business logic rather than lots of duplicate boilerplate code.</p>

<h3>authentication example</h3>

<p>In my application, new authenticated web actions were composed of two actions (<code>check_auth</code> and the API route handler, e.g. <code>route_handler</code>).</p>

<p>Here is an outline of the <code>check_auth</code> function in Node.js.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>// stop sequence processing and return HTTP 401?
</code></pre>

<p>  }</p>

<p>  // ...else pass along request to next sequence action
  return params
}
```</p>

<p>The <code>check_auth</code> function will inspect the HTTP request and validate the authorisation token. If the token is valid, the function returns the input parameters untouched, which leads the platform the invoke the <code>route_handler</code> to generate the HTTP response from the API route.</p>

<p><strong>But what happens if the authentication is invalid?</strong></p>

<p>The  <code>check_auth</code> action needs to return a HTTP 401 response immediately, rather than proceeding to the  <code>route_handler</code> action.</p>

<p>{% img /images/sequences-and-web-actions/options.png  %}</p>

<h3>handling errors - synchronous results</h3>

<p>Sequence actions can stop sequence processing by returning an error. Action errors are indicated by action results which include an "error" property or return rejected promises (for asynchronous results). Upon detecting an error, the platform will return the error result as the sequence action response.</p>

<p><em>If <code>check_auth</code> returns an error upon authentication failures, sequence processing can be halted, but how to control the HTTP response?</em></p>

<p><a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md#error-handling">Error responses</a> can also control the HTTP response, using the same properties (<code>statusCode</code>, <code>headers</code> and <code>body</code>) as a successful invocation result, with one difference: <strong>those properties must be the children of the <code>error</code> property rather than top-level properties.</strong></p>

<p>This example shows the error result needed to generate an immediate HTTP 401 response.</p>

<p>```json
{
   "error": {</p>

<pre><code>  "statusCode": 401,
  "body": "Authentication credentials are invalid."
}
</code></pre>

<p>}
```</p>

<p>In Node.js, this can be returned using a synchronous result as shown here.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>const response = { statusCode: 401, body: "Authentication credentials are invalid." }
return { error: response }
</code></pre>

<p>  }</p>

<p>  return params
}
```</p>

<h3>handling errors - using promises</h3>

<p>If a rejected Promise is used to return an error from an asynchronous operation, the promise result needs to contain the HTTP response properties as <strong>top-level properties</strong>, rather than under an <code>error</code> parent. This is because the Node.js runtime automatically <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L118">serialises the promise value</a> to an <code>error</code> property on the activation result.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>const response = { statusCode: 401, body: "Authentication credentials are invalid." }
return Promise.reject(response)
</code></pre>

<p>  }</p>

<p>  return params
}
```</p>

<h2>conclusion</h2>

<p>Creating web actions from sequences is a novel way to implement the "HTTP middleware" pattern on serverless platforms. Surrounding route handlers with pre-HTTP request modifier actions for common tasks, allows route handlers to remove boilerplate code and focus on the core business logic.</p>

<p>In my application, I wanted to use this pattern was being used for custom HTTP authentication validation.</p>

<p>When the HTTP request contains the correct credentials, the request is passed along unmodified. When the credentials are invalid, the action needs to stop sequence processing and return a HTTP 401 response.</p>

<p>Working out how to do this wasn't immediately obvious from the documentation. HTTP response parameters need to included under the <code>error</code> property for synchronous results. I have now opened a PR to improve the project documentation about this.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pluggable Event Providers for Apache OpenWhisk]]></title>
    <link href="http://jamesthom.as/blog/2019/02/20/pluggable-event-providers-for-apache-openwhisk/"/>
    <updated>2019-02-20T11:53:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/20/pluggable-event-providers-for-apache-openwhisk</id>
    <content type="html"><![CDATA[<p>Recently I presented my work building "<em><a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">pluggable event providers</a></em>" for <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> to the open-source community on the <a href="https://www.youtube.com/openwhisk">bi-weekly video meeting</a>.</p>

<p>This was based on my experience building a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/feeds.md">new event provider</a> for Apache OpenWhisk, which led me to prototype an <strong>easier way to add event sources to platform</strong> whilst <strong>cutting down on the boilerplate code</strong> required.</p>

<p>Slides from the talk are <a href="https://speakerdeck.com/jthomas/apache-openwhisk-pluggable-event-providers">here</a> and there's also a video recording <a href="https://www.youtube.com/watch?v=krm7X5YpGy0">available</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/krm7X5YpGy0?start=89" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<p>This blog post is overview of what I talked about on the call, explaining the background for the project and what was built. Based on positive feedback from the community, I have now open-sourced <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">both</a> <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">components</a> of the experiment and will be merging it back upstream into Apache OpenWhisk in future.</p>

<h2>pluggable event providers - why?</h2>

<p>At the end of last year, I was asked to prototype an <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html">S3-compatible</a> Object Store event source for Apache OpenWhisk. Reviewing the <a href="https://github.com/search?q=incubator-openwhisk-package">existing event providers</a> helped me understand how they work and what was needed to build a new event source.</p>

<p>This led me to an interesting question...</p>

<blockquote><p>Why do we have relatively few community contributions for event sources?</p></blockquote>

<p>Most of the existing event sources in the project were contributed by IBM. There hasn't been a new event source from an external community member. This is in stark contrast to <a href="https://github.com/search?q=incubator-openwhisk-runtime">additional platform runtimes</a>. Support for PHP, Ruby, DotNet, Go and many more languages all came from community contributions.</p>

<p><em>Digging into the source code for the existing feed providers, I came to the following conclusions....</em></p>

<ul>
<li><strong>Trigger feed providers are not simple to implement.</strong></li>
<li><strong>Documentation how existing providers work is lacking.</strong></li>
</ul>


<p>Feed providers can feel a bit like magic to users. You call the <code>wsk</code> CLI with a <code>feed</code> parameter and that's it, the platform handles everything else. But what actually happens to bind triggers to external event sources?</p>

<p><em>Let's start by explaining how trigger feeds are implemented in Apache OpenWhisk, before moving onto my idea to make contributing new feed providers easier.</em></p>

<h2>how trigger feeds work</h2>

<p>Users normally interact with trigger feeds using the <code>wsk</code> CLI. Whilst creating a trigger, the <code>feed</code> parameter can be included to connect that trigger to an external event source. Feed provider options as provided as further CLI parameters.</p>

<p><code>
wsk trigger create periodic \
  --feed /whisk.system/alarms/alarm \
  --param cron "*/2 * * * *" \
  --param trigger_payload “{…}” \
  --param startDate "2019-01-01T00:00:00.000Z" \
  --param stopDate "2019-01-31T23:59:00.000Z"
</code></p>

<p><em>But what are those trigger feed identifiers used with the <code>feed</code> parameter?</em></p>

<p><strong>It turns out they are just normal actions which have been shared in a public package!</strong></p>

<p>The CLI creates the trigger (using the platform API) and then invokes the referenced feed action. Invocation parameters include the following values used to manage the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/feeds.md#implementing-feed-actions">trigger feed lifecycle</a>.</p>

<ul>
<li><code>lifecycleEvent</code> - Feed operation (<code>CREATE</code>, <code>READ</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>PAUSE</code>, or <code>UNPAUSE</code>).</li>
<li><code>triggerName</code> - Trigger identifier.</li>
<li><code>authKey</code> - API key provided to invoke trigger.</li>
</ul>


<p>Custom feed parameters from the user are also included in the event parameters.</p>

<p><strong>This is the entire interaction of the platform with the feed provider.</strong></p>

<p>Providers are responsible for the full management lifecycle of trigger feed event sources. They have to maintain the list of registered triggers and auth keys, manage connections to user-provided event sources, fire triggers upon external events, handle retries and back-offs in cases of rate-limiting and much more.</p>

<p>Feed providers used with a trigger are stored as custom annotations. This allows the CLI to call the same feed action to stop the event binding when the trigger is deleted.</p>

<h3>trigger management</h3>

<p>Reading the source code for the <a href="https://github.com/search?q=incubator-openwhisk-package">existing feed providers</a>, nearly all of the code is responsible for handling the lifecycle of trigger management events, rather than integrating with the external event source.</p>

<p>Despite this, all of the existing providers are in separate repositories and don't share code explicitly, although the same source files have been replicated in different repos.</p>

<p>The <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB feed provider</a> is a good example of how feed providers can be implemented.</p>

<h3>couchdb feed provider</h3>

<p>The <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB trigger feed provider</a> uses a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changes.js">public action</a> to handle the lifecycle events from the <code>wsk</code> CLI.</p>

<p>{% img /images/pluggable-providers/feeds-overview.png  %}</p>

<p>This <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changes.js">action</a> just proxies the incoming requests to a separate <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changesWebAction.js">web actio</a>n. The <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">web action</a> implements the logic to handle the trigger lifecycle event. The web action uses a CouchDB database used to store registered triggers. Based upon the lifecycle event details, the web action updates the database document for that trigger.</p>

<p>{% img /images/pluggable-providers/feeds-provider.png  %}</p>

<p>The feed provider also runs a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/tree/master/provider">seperate Docker container</a>, which handles listening to CouchDB change feeds from user-provided credentials. It uses the changes feed from the trigger management database, modified from the web action, to listen for triggers being added, removed, disabled or re-enabled.</p>

<p>{% img /images/pluggable-providers/feeds-fire-trigger.png  %}</p>

<p>When database change events <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/provider/lib/utils.js#L78">occur</a>, the container <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/provider/lib/utils.js#L66-L76">fires triggers</a> on the platform with the event details.</p>

<h2>building a new event provider?</h2>

<p>Having understood how feed providers work (and how the existing providers were designed), I started to think about the new event source for an S3-compatible object store.</p>

<p>Realising ~90% of the code between providers was the same, I wondered if there was a different approach to creating new event providers, rather than cloning an existing provider and changing the small amount of code used to interact with the event sources.</p>

<p><strong>What about building a generic event provider which a pluggable event source?</strong></p>

<p>This generic event provider would handle all the trigger management logic, which isn't specific to individual event sources. The event source plugin would manage connecting to external event sources and then firing triggers as event occurred. Event source plugins would implement a standard interface and be registered dynamically during startup.</p>

<p>{% img /images/pluggable-providers/generic-provider.png  %}</p>

<h3>advantages</h3>

<p>Using this approach would make it much easier to contribute and maintain new event sources.</p>

<ul>
<li><p>Users would be able to create new event sources with a few lines of custom integration code, rather than replicating all the generic trigger lifecycle management code.</p></li>
<li><p>Maintaining a single repo for the generic event provider is easier than having the same code copied and pasted in multiple independent repositories.</p></li>
</ul>


<p>I started hacking away at the existing CouchDB event provider to replace the event source integration with a generic plugin interface. Having completed this, I then wrote a new S3-compatible event source using the plugin model. After a couple of weeks I had something working....</p>

<h2>generic event provider</h2>

<p>The <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">generic event provider</a> is based on the exiting CouchDB feed provider source code. The project contains the stateful container code and feed package actions (public &amp; web). It uses the same platform services (CouchDB and Redis) as the existing provider to maintain trigger details.</p>

<p>The event provider plugin is integrated through the <code>EVENT_PROVIDER</code> environment variable. The name should refer to a Node.js module from NPM with the following <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider#plugin-interface">interface</a>.</p>

<p>```javascript
// initialise plugin instance (must be a JS constructor)
module.exports = function (trigger_manager, logger) {</p>

<pre><code>// register new trigger feed
const add = async (trigger_id, trigger_params) =&gt; {}
// remove existing trigger feed
const remove = async trigger_id =&gt; {}
</code></pre>

<p>   return { add, remove }
}</p>

<p>// valiate feed parameters
module.exports.validate = async trigger_params => {}
```</p>

<p>When a new trigger is added to the trigger feeds' database, the details will be passed to the <code>add</code> method. Trigger parameters will be used to set up listening to the external event source. When external events occur, the <code>trigger_manager</code> can be use to automatically fire triggers.</p>

<p>When users delete triggers with feeds, the trigger will be removed from the database. This will lead to the <code>remove</code> method being called. Plugins should stop listening to messages for this event source.</p>

<h3>firing trigger events</h3>

<p>As event arrive from the external source, the plugin can use the <code>trigger_manager</code> instance, passed in through the constructor, to fire triggers with the identifier.</p>

<p>The <code>trigger_manager</code> parameter exposes two async functions:</p>

<ul>
<li><code>fireTrigger(id, params)</code> - fire trigger given by id passed into <code>add</code> method with event parameters.</li>
<li><code>disableTrigger(id, status_code, message)</code> - disable trigger feed due to external event source issues.</li>
</ul>


<p>Both functions handle the retry logic and error handling for those operations. These should be used by the event provider plugin to fire  triggers when events arrive from external sources and then disable triggers due to external event source issues.</p>

<h3>validating event source parameters</h3>

<p>This static function on the plugin constructor is used to validate incoming trigger feed parameters for correctness, e.g. checking  authentication credentials for an event source. It is passed the trigger  parameters from the user.</p>

<h2>S3 event feed provider</h2>

<p>Using this new generic event provider, I was able to create an event source for an <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">S3-compatible object store</a>. Most importantly, this new event source was implemented using just <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed/tree/master/lib">~300 lines</a> of JavaScript! This is much smaller than the 7500 lines of code in the generic event provider.</p>

<p>The feed provider polls buckets on an interval using the <code>ListObjects</code> <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html">API call</a>. Results are cached in Redis to allow comparison between intervals. Comparing the differences in bucket file name and etags, allows file change events to be detected.</p>

<p>Users can call the feed provider with a bucket name, endpoint, API key and polling interval.</p>

<p><code>
wsk trigger create test-s3-trigger --feed /&lt;PROVIDER_NS&gt;/s3-trigger-feed/changes --param bucket &lt;BUCKET_NAME&gt; --param interval &lt;MINS&gt; --param s3_endpoint &lt;S3_ENDPOINT&gt; --param s3_apikey &lt;COS_KEY&gt;
</code></p>

<p>File events are fired as the bucket files change with the following trigger events.</p>

<p>```
{
  "file": {</p>

<pre><code>"ETag": "\"fb47672a6f7c34339ca9f3ed55c6e3a9\"",
"Key": "file-86.txt",
"LastModified": "2018-12-19T08:33:27.388Z",
"Owner": {
  "DisplayName": "80a2054e-8d16-4a47-a46d-4edf5b516ef6",
  "ID": "80a2054e-8d16-4a47-a46d-4edf5b516ef6"
},
"Size": 25,
"StorageClass": "STANDARD"
</code></pre>

<p>  },
  "status": "deleted"
}
```</p>

<p><em>Pssst - if you are using <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> - I actually have this deployed and running so you can try it out. Use the <code>/james.thomas@uk.ibm.com_dev/s3-trigger-feed/changes</code> feed action name. This package is only available in the London region.</em></p>

<h2>next steps</h2>

<p>Feedback on the call was overwhelming positive on my experiment. Based upon this, I've now open-sourced both the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">generic event provider</a> and <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">s3 event source plugin</a> to allow the community to evaluate the project further.</p>

<p>I'd like to build a few more example event providers to validate the approach further before moving towards contributing this code back upstream.</p>

<p>If you want to try this generic event provider out with your own install of OpenWhisk, please see the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider/blob/master/README.md#running-the-provider--plugin">documentation</a> in the README for how to get started.</p>

<p>If you want to build new event sources, please see the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider/blob/master/README.md#plugin-interface">instructions</a> in the generic feed provider repository and take a look at the S3 plugin for an example to follow.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CouchDB Filters with OpenWhisk Triggers]]></title>
    <link href="http://jamesthom.as/blog/2019/02/12/couchdb-filters-with-openwhisk-triggers/"/>
    <updated>2019-02-12T14:22:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/12/couchdb-filters-with-openwhisk-triggers</id>
    <content type="html"><![CDATA[<p>Imagine you have an <a href="http://openwhisk.incubator.apache.org/">OpenWhisk</a> <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md">action</a> to send emails to users to verify their email addresses. User profiles, containing email addresses and verification statuses, are maintained in a <a href="https://couchdb.apache.org/">CouchDB</a> database.</p>

<p>```json
{</p>

<pre><code>...
"email": {
    "address": "user@host.com",
    "status": "unverified"
}
</code></pre>

<p>}
```</p>

<p>Setting up a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB trigger feed</a> allows the email action <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md">to be invoked</a> when the user profile changes. When user profiles have unverified email addresses, the action can send verification emails.</p>

<p>Whilst this works fine - it will result in a lot of unnecessary invocations. All modifications to user profiles, not just the email field, will result in the action being invoked. This will incur a cost despite the action having nothing to do.</p>

<blockquote><p>How can we restrict document change events to just those we care about?</p></blockquote>

<p>CouchDB <a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html#filter-functions">filter functions</a> to the rescue 🦸‍♂️🦸‍.</p>

<h2>CouchDB Filter Functions</h2>

<p><a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html#filter-functions">Filter functions</a> are Javascript functions executed against (potential) <a href="http://guide.couchdb.org/draft/notifications.html">change feed events</a>. The function is invoked with each document update. The return value is evaluated as a boolean variable. If true, the document is published on the changes feed. Otherwise, the event is filtered from the changes feed.</p>

<h3>example</h3>

<p>Filter functions are created through <a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html">design documents</a>. Function source strings are stored as properties under the <code>filters</code> document attribute. Key names are used as filter identifiers.</p>

<p>Filter functions should have the following interface.</p>

<p>```javascript
function(doc, req){</p>

<pre><code>// document passes test
if (doc.property == 'value'){
    return true;
}

// ... else ignore document upate
return false;
</code></pre>

<p>}
```</p>

<p> <code>doc</code> is the modified document object and <code>req</code> contains (optional) request parameters.</p>

<p><em>Let's now explain how to create a filter function to restrict profile update events to just those with unverified email addresses...</em></p>

<h2>Filtering Profile Updates</h2>

<h3>user profile documents</h3>

<p>In this example, email addresses are stored in user profile documents under the <code>email</code> property. <code>address</code> contains the user's email address and <code>status</code> records the verification status (<code>unverified</code> or <code>verified</code>).</p>

<p>When a new user is added, or an existing user changes their email address, the <code>status</code> attribute is set to <code>unverified</code>. This indicates a verification message needs to be sent to the email address.</p>

<p>```json
{</p>

<pre><code>...
"email": {
    "address": "user@host.com",
    "status": "unverified"
}
</code></pre>

<p>}
```</p>

<h3>unverified email filter</h3>

<p>Here is the CouchDB filter function that will ignore document updates with verified email addresses.</p>

<p>```
function(doc){</p>

<pre><code>if (doc.email.status == 'unverified'){
    return true;
}

return false
</code></pre>

<p>}
```</p>

<h3>design document with filters</h3>

<p>Save the following JSON document in CouchDB. This creates a new design document (<code>profile</code>) containing a filter function (<code>unverified-emails</code>).</p>

<p>```json
{
  "<em>id": "</em>design/profile",<br/>
  "filters": {</p>

<pre><code>"unverified-emails": "function (doc) {\n  if (doc.email.status == 'unverified') {\n    return true\n  }\n  return false\n}"
</code></pre>

<p>  },
  "language": "javascript"
}
```</p>

<h3>trigger feed with filter</h3>

<p>Once the design document is created, the filter name can be used as a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant#create-the-trigger-using-the-filter-function">trigger feed parameter</a>.</p>

<p><code>
wsk trigger create verify_emails --feed /_/myCloudant/changes \
--param dbname user_profiles \
--param filter "profile/unverified-emails"
</code></p>

<p>The trigger only fires when a profile change contains an unverified email address. No more unnecessary invocations, which saves us money! 😎</p>

<h3>caveats</h3>

<p><em>"Why are users getting multiple verification emails?"</em> 😡</p>

<p>If a user changes their profile information, whilst leaving their email address the same but before clicking the verification email, an additional email will be sent.</p>

<p>This is because the <code>status</code> field is still in the <code>unverified</code> state when the next document update occurs. Filter functions are stateless and can't decide if this email address has already been seen.</p>

<p>Instead of leaving the <code>status</code> field as <code>unverified</code>, the email action should change the state to another value, e.g. <code>pending</code>, to indicate the verification email has been sent.</p>

<p>Any further document updates, whilst waiting for the verification response, won't pass the filter and users won't receive multiple emails. 👍</p>

<h2>Conclusion</h2>

<p>CouchDB filters are an easy way to subscribe to a subset of events from the changes feed. Combining CouchDB trigger feeds with filters allows actions to ignore irrelevant document updates. Multiple trigger feeds can be set up from a single database using filter functions.</p>

<p>As well as saving unnecessary invocations (and therefore money), this can simplify data models. A single database can be used to store all documents, rather than having to split different types into multiple databases, whilst still supporting changes feeds per document type.</p>

<p>This is an awesome feature of CouchDB!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Loosely-coupled Serverless Functions With Apache Openwhisk]]></title>
    <link href="http://jamesthom.as/blog/2019/01/18/loosely-coupled-serverless-functions-with-openwhisk/"/>
    <updated>2019-01-18T15:10:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/01/18/loosely-coupled-serverless-functions-with-openwhisk</id>
    <content type="html"><![CDATA[<p>Just like software engineering, <a href="https://medium.com/@PaulDJohnston/serverless-best-practices-b3c97d551535">best practices for serverless applications</a> advise keeping functions small and focused on a single task, aka "<a href="https://en.wikipedia.org/wiki/Unix_philosophy#Do_One_Thing_and_Do_It_Well">do one thing and do it well</a>". Small single-purpose functions are easier to develop, test and debug. 👍</p>

<p><strong>But what happens when you need execute multiple asynchronous tasks (implemented as separate functions) from an incoming event, like an API request?</strong> 🤔</p>

<h2>Functions Calling Functions?</h2>

<p>Functions can invoke other functions directly, using asynchronous calls through the client SDK. This works at the cost of introducing <a href="https://en.wikipedia.org/wiki/Coupling_%28computer_programming%29">tighter coupling</a> between functions, which is generally avoided in software engineering! Disadvantages of this approach include...</p>

<ul>
<li><em>Functions which call other functions can be more difficult to test. Test cases needs to mock out the client SDK to remove side-effects during unit or integration tests.</em></li>
<li><em>It can lead to repetitive code if you want to fire multiple tasks with the same event. Each invocation needs to manually handle error conditions and re-tries on network or other issues, which complicates the business logic.</em></li>
<li><em>Modifying the functions being invoked cannot be changed dynamically. The function doing the invoking has to be re-deployed with updated code.</em></li>
</ul>


<p><a href="https://twitter.com/PaulDJohnston">Some people</a> have even labelled "<em>functions calling functions</em>" an <a href="https://medium.com/@PaulDJohnston/serverless-best-practices-b3c97d551535">anti-pattern</a> in serverless development! 😱</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Most common Serverless mistake?<br><br>Functions calling other functions<br><br>Why do people make this mistake?<br><br>Because people assume they should build functions like microservices and then use them in a similar way.<br><br>Causes no end of problems</p>&mdash; Serverless / Green Data Advocate (@PaulDJohnston) <a href="https://twitter.com/PaulDJohnston/status/1085106548270088193?ref_src=twsrc%5Etfw">January 15, 2019</a></blockquote>


<p><strong>Hmmm... so what should we do?</strong></p>

<p>Apache OpenWhisk has an awesome feature to help with this problem, triggers and rules! 👏</p>

<h2>OpenWhisk Triggers &amp; Rules</h2>

<p>Triggers and Rules in OpenWhisk are similar to the <a href="https://en.wikipedia.org/wiki/Observer_pattern">Observer pattern</a> from software engineering.</p>

<p>Users can fire "events" in OpenWhisk by invoking a named <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md#creating-triggers">trigger</a> with parameters. <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md#using-rules">Rules</a> are used to "subscribe" actions to all events for a given trigger name. Actions are invoked with event parameters when a trigger is fired. Multiple rules can be configured to support multiple "listeners" to the same trigger events. Event senders are decoupled from event receivers.</p>

<p>{% img /images/loose-coupling-openwhisk/t-r-a.png %}</p>

<p>Developers using OpenWhisk are most familiar with triggers when used with <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/catalog.md">feed providers</a>. This is used to subscribe actions to external event sources. The feed provider is responsible for listening to the event source and automatically firing trigger events with event details.</p>

<p><strong>But triggers can be fired manually from actions to provide custom event streams!</strong> 🙌</p>

<p>```javascript
const openwhisk = require('openwhisk')
const params = {msg: 'event parameters'}</p>

<p>// replace code like this...
const result = await ow.actions.invoke({name: "some-action", params})</p>

<p>// ...with this
const result = await ow.triggers.invoke({name: "some-trigger", params})
```</p>

<p>This allows applications to move towards an <a href="https://en.wikipedia.org/wiki/Event-driven_architecture">event-driven architecture</a> and promotes loose-coupling between functions with all the associated benefits for testing, deployment and scalability. 👌</p>

<h3>creating triggers</h3>

<p>Triggers are managed through the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md">platform API</a>. They can be created, deleted, retrieved and fired using  HTTP requests. Users normally interact with triggers through the <a href="https://github.com/apache/incubator-openwhisk-cli">CLI</a> or <a href="https://github.com/apache/incubator-openwhisk-client-js/">platform SDKs</a>.</p>

<p>Triggers can be created using the following CLI command.</p>

<p><code>
wsk trigger create &lt;TRIGGER_NAME&gt;
</code></p>

<h3>default parameters</h3>

<p>Triggers support <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/parameters.md#setting-default-parameters-on-an-action">default parameters</a> like actions. Default parameters are stored in the platform and included in all trigger events. If the event object includes parameters with the same key, default parameter values are ignored.</p>

<p><code>
wsk trigger create &lt;TRIGGER_NAME&gt; -p &lt;PARAM&gt; &lt;PARAM_VALUE&gt; -p &lt;PARAM_2&gt; &lt;PARAM_VALUE&gt; ...
</code></p>

<h3>binding triggers to actions with rules</h3>

<p>Rules bind triggers to actions. When triggers are fired, all actions connected via rules are invoked with the trigger event. Multiple rules can refer to the same trigger supporting multiple listeners to the same event.</p>

<p>Rules can also be created using the following CLI command.</p>

<p><code>
wsk rule create RULE_NAME TRIGGER_NAME ACTION_NAME
</code></p>

<p>Tools like <a href="https://github.com/serverless/serverless-openwhisk">The Serverless Framework</a> and <a href="https://github.com/apache/incubator-openwhisk-wskdeploy">wskdeploy</a> allow users to configure triggers and rules declaratively through YAML configuration files.</p>

<h3>firing triggers</h3>

<p>The JS SDK can be used to <a href="https://github.com/apache/incubator-openwhisk-client-js#fire-trigger">fire triggers programatically</a> from applications.</p>

<p><code>javascript
const openwhisk = require('openwhisk')
const name = 'sample-trigger'
const params = {msg: 'event parameters'}
const result = ow.triggers.invoke({name, params})
</code></p>

<p>CLI commands (<code>wsk trigger fire</code>) can fire triggers manually with event parameters for testing.</p>

<p><code>
wsk trigger fire sample-trigger -p msg "event parameters"
</code></p>

<h3>activation records for triggers</h3>

<p>Activation records are created for trigger events. These activation records contain event parameters, rules fired, activations ids and invocation status for each action invoked. This is useful for debugging trigger events when issues are occurring.</p>

<p><code>
$ wsk trigger fire sample-trigger -p hello world
ok: triggered /_/sample-trigger with id &lt;ACTIVATION_ID&gt;
$ wsk activation get &lt;ACTIVATION_ID&gt;
ok: got activation &lt;ACTIVATION_ID&gt;
{
 ...
}
</code></p>

<p>The <code>response.result</code> property in the activation record contains the fired trigger event (combining default and event parameter values).</p>

<p>Rules fired by the trigger are recorded in activation records as the JSON values under the <code>logs</code> parameter.</p>

<p><code>json
{
  "statusCode": 0,
  "success": true,
  "activationId": "&lt;ACTION_ACTIVATION_ID&gt;",
  "rule": "&lt;RULE_NAME&gt;",
  "action": "&lt;ACTION_NAME&gt;"
}
</code></p>

<p><em>Activation records are only generated when triggers have enabled rules with valid actions attached</em></p>

<h2>Example - WC Goal Bot</h2>

<p>This is great in theory but what about in practice?</p>

<p><a href="https://github.com/jthomas/goalbot">Goal Bot</a> was a small serverless application I built in 2018 for the World Cup. It was a <a href="https://twitter.com/WC2018_Goals">Twitter bot</a> which tweeted out all goals scored in real-time. The application used the  "actions connected via triggers events" architecture pattern. This made development and testing easier and faster.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">⚽️ GOAL ⚽️<br>👨 Harry MAGUIRE (󠁿🏴󠁧󠁢󠁥󠁮󠁧󠁿 ) @ 30&#39;. 👨<br>🏟 Sweden 🇸🇪 (0) v England 󠁿🏴󠁧󠁢󠁥󠁮󠁧󠁿 (1) 🏟<a href="https://twitter.com/hashtag/WorldCup?src=hash&amp;ref_src=twsrc%5Etfw">#WorldCup</a></p>&mdash; WC 2018 Goal Bot (@WC2018_Goals) <a href="https://twitter.com/WC2018_Goals/status/1015604110006120448?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>This function has two functions <code>goals</code> and <code>twitter</code>.</p>

<p><code>goals</code> was <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L11-L16">responsible</a> for detecting new goals scored using an external API. When invoked, it would retrieve all goals currently scored in the World Cup. Comparing the API response to a previous cached version calculated new goals scored. This function was connected to the alarm event source to run once a minute.</p>

<p><code>twitter</code> was <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L17-L22">responsible</a> for sending tweets from the @WC_Goals account. Twitter's API was used to create  goal tweets constructed from the event parameters.</p>

<p><strong>Goal events detected in the <code>goals</code> function need to be used to invoke the <code>twitter</code> function.</strong></p>

<p>Rather than the <code>goals</code> function invoke the <code>twitter</code> function directly, a trigger event (<code>goal</code>) was <a href="https://github.com/jthomas/goalbot/blob/master/lib/goal_tracker.js#L39-L41">fired</a>. The <code>twitter</code> function was bound to the <code>goal</code> trigger using a <a href="https://github.com/jthomas/goalbot/blob/master/serverless.yml#L21-L22">custom rule</a>.</p>

<p>{% img /images/loose-coupling-openwhisk/goalbot.png %}</p>

<p>De-coupling the two tasks in my application (checking for new goals and creating tweets) using triggers and rules had the following benefits...</p>

<ul>
<li><p>The <code>goals</code> function could be invoked in testing without tweets being sent. By disabling the rule binding the <code>twitter</code> function to the trigger, the goals function can fire events without causing side-effects.</p></li>
<li><p>Compared to having a "mono-function" combining both tasks, splitting tasks into functions means the <code>twitter</code> function can be tested with manual events, rather than having to manipulate the database and stub API responses to generate the correct test data.</p></li>
<li><p>It would also be easy to extend this architecture with additional notification services, like slack bots. New notification services could be attached to the same trigger source with an additional rule. This would not require any changes to the <code>goals</code> or <code>twitter</code> functions.</p></li>
</ul>


<h2>Triggers versus Queues</h2>

<p>Another common solution to de-coupling functions in serverless architectures is using <a href="https://theburningmonk.com/2018/04/what-is-the-best-event-source-for-doing-pub-sub-with-aws-lambda/">message queues</a>.</p>

<p>Functions push events in external queues, rather than invoking triggers directly. Event sources are responsible for firing the registered functions with new messages. Apache OpenWhisk <a href="https://github.com/apache/incubator-openwhisk-package-kafka">supports Kafka</a> as an event source which could be used with this approach.</p>

<p><em>How does firing triggers directly compare to pushing events into an external queue (or other event source)?</em></p>

<p>Both queues and triggers can be used to achieve the same goal ("<em>connect functions via events</em>") but have different semantics. It is important to understand the benefits of both to choose the most appropriate architecture for your application.</p>

<h3>benefits of using triggers against queues</h3>

<p>Triggers are built into the Apache OpenWhisk platform. There is no configuration needed to use them. External event sources like queues need to be provisioned and managed as additional cloud services.</p>

<p>Trigger invocations are free in IBM Cloud Functions. IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">charges only</a> for execution time and memory used in functions. Queues will incur additional usage costs based on the service's pricing plan.</p>

<h3>disadvantages of using triggers against queues</h3>

<p>Triggers are not queues. Triggers are not queues. Triggers are not queues. 💯</p>

<p>If a trigger is fired and no actions are connected, the event is lost. Trigger events are not persisted until listeners are attached. <strong>If you need event persistence, message priorities, disaster recovery and other advanced features provided by message queues, use a message queue!</strong></p>

<p>Triggers are subject to <a href="https://console.bluemix.net/docs/openwhisk/openwhisk_reference.html#openwhisk_syslimits">rate limiting</a> in Apache OpenWhisk. In IBM Cloud Functions, this defaults to 1000 concurrent invocations and 5000 total invocations per namespace per minute. These limits can be raised through a support ticket but there are practical limits to the maximum rates allowed. Queues have support for much higher throughput rates.</p>

<p>External event providers are also responsible for handling the retries when triggers have been rate-limited due to excess events. Invoking triggers manually relies on the invoking function to handle this. Emulating retry behaviour from an event provider is impractical due to costs and limits on function duration.</p>

<h2>Other hints and tips</h2>

<p><strong><em>Want to invoke an action which fires triggers without setting off listeners?</em></strong></p>

<p>Rules can be dynamically disabled without having to remove them. This can be used during integration testing or debugging issues in production.</p>

<p><code>
wsk rule disable RULE_NAME
wsk rule enable RULE_NAME
</code></p>

<p><strong><em>Want to verify triggers are fired with correct events without mocking client libraries?</em></strong></p>

<p>Trigger events are not logged unless there is at least one enabled rule. Create a new rule which binds the <code>/whisk.system/utils/echo</code> action to the trigger. This built-in function just returns input parameters as the function response. This means the activation records with trigger events will now be available.</p>

<h2>conclusion</h2>

<p>Building event-driven serverless applications from loosely-coupled functions has numerous benefits including development speed, improved testability, deployment velocity, lower costs and more.</p>

<p>Decomposing "monolithic" apps into independent serverless functions often needs event handling functions to trigger off multiple backend operations, implemented in separate serverless functions. Developers unfamiliar with serverless often resort to direct function invocations.</p>

<p>Whilst this works, it introduces tight coupling between those functions, which is normally avoided in software engineering. This approach has even been highlighted as a "serverless" anti-pattern.</p>

<p>Apache OpenWhisk has an awesome feature to help with this problems, triggers and rules!</p>

<p>Triggers provide a lightweight event firing mechanism in the platform. Rules bind actions to triggers to automate invoking actions when events are fired. Applications can fire trigger events to invoke other operations, rather than using direct invocations. This keeps the event sender and receivers de-coupled from each other. 👏</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Highly Available Serverless Apps With Cloudant's Cross-Region Replication]]></title>
    <link href="http://jamesthom.as/blog/2019/01/10/highly-available-serverless-apps-with-cloudant-cross-region-replication/"/>
    <updated>2019-01-10T11:23:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/01/10/highly-available-serverless-apps-with-cloudant-cross-region-replication</id>
    <content type="html"><![CDATA[<p>Building <a href="https://www.techrepublic.com/blog/the-enterprise-cloud/what-high-availability-for-cloud-services-means-in-the-real-world/">highly available</a> serverless applications relies on eliminating "<a href="https://en.wikipedia.org/wiki/Single_point_of_failure"><em>single points of failure</em></a>" from application architectures.</p>

<p><a href="https://cloud.ibm.com/docs/tutorials/multi-region-serverless.html#deploy-serverless-apps-across-multiple-regions">Existing tutorials</a> showed how to deploy the same serverless application on IBM Cloud in different regions. Using the <a href="https://cloud.ibm.com/docs/infrastructure/cis/glb.html#global-load-balancer-glb-concepts">Global Load Balancer</a> from <a href="https://cloud.ibm.com/catalog/services/internet-svcs">IBM Cloud Internet Services</a>, traffic is distributed across multiple applications from the same hostname. The <a href="https://cloud.ibm.com/docs/infrastructure/cis/glb.html#global-load-balancer-glb-concepts">Global Load Balancer</a> automatically detects outages in the regional applications and redirects traffics as necessary.</p>

<p><strong>But what if all instances rely on the same database service and that has issues?</strong> 😱🔥</p>

<p>In addition to running multiple instances of the application, independent databases in different regions are also necessary for a highly available serverless application. Maintaining consistent application state across regions needs all database changes to be automatically synchronised between instances. 🤔</p>

<p><strong>In this blog post, we're going to look at using <a href="https://cloud.ibm.com/catalog/services/cloudant">IBM Cloudant's</a> <a href="https://console.bluemix.net/docs/services/Cloudant/guides/active-active.html#configuring-ibm-cloudant-for-cross-region-disaster-recovery">replication service</a> to set up a "<a href="https://en.wikipedia.org/wiki/Multi-master_replication">multi-master</a>" replication between regional database instances.</strong></p>

<p>Once this is enabled, database changes will automatically be synchronised in real-time between all database instances. Serverless applications can use their regional database instance and be confident application state will be consistent globally (for some definition of <a href="https://en.wikipedia.org/wiki/Eventual_consistency">consistent</a>...). 💯</p>

<h2>example serverless application - todo backend</h2>

<p>This <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">serverless application</a> implements a <a href="https://www.todobackend.com/">TODO backend</a> using <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> and <a href="https://cloud.ibm.com/catalog/services/cloudant">IBM Cloudant</a>.</p>

<p>{% img /images/ha-serverless-apps-todo/todo-frontpage.png %}</p>

<p>It provides an REST API for interacting with a TODO service. This can be used with the <a href="https://www.todobackend.com/client/index.html">front-end client</a> to add, complete and remove todos from a list.</p>

<p><strong>Let's make this <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">example serverless application</a> "highly available". 👍</strong></p>

<p>The application will be deployed to two different IBM Cloud regions (London and Dallas). Separate database instances will be provisioned in each region. Applications will use their regional database instance but share global state via replication.</p>

<p>{% img /images/ha-serverless-apps-todo/architecture.png %}</p>

<h2>deploy serverless app to multiple regions</h2>

<p>This Github <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis">repo</a> has an <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis/blob/master/deploy.sh">automatic deployment script</a> to deploy the serverless application (using <code>wskdeploy</code>) and application services (using <code>terraform</code>).</p>

<p><strong><em>Install the prerequisites listed <a href="https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis#code-and-tools">here</a> before proceeding with these instructions.</em></strong></p>

<h3>download example application</h3>

<ul>
<li>Clone the Git repository to a local directory.</li>
</ul>


<p><code>
git clone https://github.com/IBM/ibm-cloud-functions-refarch-serverless-apis
</code></p>

<ul>
<li>Enter the source code directory.</li>
</ul>


<p><code>
cd ibm-cloud-functions-refarch-serverless-apis
</code></p>

<h3>create IAM key for serverless app</h3>

<p><em>Have you already signed up for an <a href="https://cloud.ibm.com/registration">IBM Cloud account</a> and <a href="https://cloud.ibm.com/docs/cli/reference/ibmcloud/download_cli.html#install_use">installed the CLI</a>? If not, please do that before proceeding.</em></p>

<ul>
<li>Create an IAM key which will be used to deploy the serverless application.</li>
</ul>


<p><code>
ibmcloud iam api-key-create serverless_api --file serverless_api.apikey
</code></p>

<h3>configure deployment variables</h3>

<ul>
<li>Create the <code>local.env</code> file in the current directory will the following contents.</li>
</ul>


<p><code>
IBMCLOUD_API_KEY=&lt;IAM_API_KEY&gt;
IBMCLOUD_ORG=&lt;YOUR_ORG&gt;
IBMCLOUD_SPACE=&lt;REGION_SPACE&gt;
IBMCLOUD_REGION=
PROVISION_INFRASTRUCTURE=true
API_USE_APPID=false
</code></p>

<ul>
<li>Replace the <code>&lt;IAM_API_KEY&gt;</code> value with the <code>apikey</code> value from the <code>serverless_api.apikey</code> file.</li>
<li>Replace the <code>&lt;IBMCLOUD_ORG&gt;</code> value with an <a href="https://cloud.ibm.com/docs/account/orgs_spaces.html#orgsspacesusers">IBM Cloud organisation</a>.</li>
<li>Replace the <code>&lt;IBMCLOUD_SPACE&gt;</code> value with an <a href="https://cloud.ibm.com/docs/account/orgs_spaces.html#orgsspacesusers">IBM Cloud space</a>.</li>
</ul>


<p>The <code>PROVISION_INFRASTRUCTURE</code> parameter makes the deployment script automatically provision all application resources using Terraform.</p>

<p>Secured API endpoints are not required for this demonstration. Setting the <code>API_USE_APPID</code> parameter to <code>false</code> disables authentication on the endpoints and provisioning the AppID service.</p>

<h3>deploy to london</h3>

<ul>
<li>Set the <code>IBMCLOUD_REGION</code> to <code>eu-gb</code> in the <code>local.env</code> file.</li>
<li>Run the following command to deploy the application and provision all application resources.</li>
</ul>


<p><code>
./deploy.sh --install
</code></p>

<p>If the deployment have succeed, the following message should be printed to the console.</p>

<p><code>
2019-01-08 10:51:51 All done.
ok: APIs
Action                                      Verb  API Name  URL
/&lt;ORG&gt;_&lt;SPACE&gt;/todo_package/todo/get_todo   get   todos     https://&lt;UK_APIGW_URL&gt;/todo
...
</code></p>

<ul>
<li>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the <a href="https://cloud.ibm.com/openwhisk/apimanagement">APIGW URL</a> shown in the console to interact with the remote TODO service in the London region.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/testing-app.gif %}</p>

<h3>deploy to dallas</h3>

<ul>
<li><p><strong>Rename the <code>terraform.tfstate</code> file in the <code>infra</code> folder to <code>terraform.tfstate.london</code></strong></p></li>
<li><p>Set the <code>IBMCLOUD_REGION</code> to <code>us-south</code> in the <code>local.env</code> file.</p></li>
<li>Run the following command to deploy the application and provision all application resources.</li>
</ul>


<p><code>
./deploy.sh --install
</code></p>

<p>If the deployment have succeed, the following message should be printed to the console.</p>

<p><code>
2019-01-08 10:51:51 All done.
ok: APIs
Action                                      Verb  API Name  URL
/&lt;ORG&gt;_&lt;SPACE&gt;/todo_package/todo/get_todo   get   todos     https://&lt;US_APIGW_URL&gt;/todo
...
</code></p>

<ul>
<li>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the <a href="https://cloud.ibm.com/openwhisk/apimanagement">APIGW URL</a> shown in the console to interact with the remote TODO service in the Dallas region.</li>
</ul>


<h2>configure cloudant cross-region replication</h2>

<p>There are now multiple copies of the same serverless application in different regions. Each region has an independent instance of Cloudant provisioned.</p>

<p><a href="https://console.bluemix.net/docs/services/Cloudant/api/replication.html">Cloudant replication</a> is a one-way synchronisation from a source to a destination database. To set up a <a href="https://console.bluemix.net/docs/services/Cloudant/guides/active-active.html#configuring-ibm-cloudant-for-cross-region-disaster-recovery">bi-directional data synchronisation</a>, two different replications will need to be configured.</p>

<h3>create api keys for replication access</h3>

<p>Before configuring replication between the regional databases, API keys need to be created to allow remote access on both hosts. API keys need to be created per regional instance.</p>

<ul>
<li>From the <a href="https://cloud.ibm.com/resources">IBM Cloud Resource List</a>, find the cloudant instances provisioned in London and Dallas.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/resource-list.png %}</p>

<ul>
<li>Open the Cloudant Dashboard for each service instance.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/opening-cloudant-dashboard.gif %}</p>

<p>Follow these instructions on both hosts to generate API keys for replication with the correct permissions.</p>

<ul>
<li>Click the "Databases" icon to show all the databases on this instance.</li>
<li>Click the 🔒 icon in the "todos" database row in the table to open the permissions page.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/databases-list.png %}</p>

<p><em>Can't find the "todos" database in the Cloudant dashboard? Make sure you interact with the TODO backend from the <a href="https://www.todobackend.com/client/index.html">front-end application</a>. This will automatically create the database if it doesn't exist.</em></p>

<ul>
<li>Click "Generate API Key" on the permissions page.</li>
<li>Make a note of the key identifier and password.</li>
<li>Set the <code>_reader_</code>, <code>_writer</code> and <code>_replicator</code> permissions for the newly created key.</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/db-api-key.png %}</p>

<h3>set up cross-region replication</h3>

<p>Replication jobs need to be configured on both database hosts. These can be created from the Cloudant dashboard. <strong>Repeat these instructions on both hosts.</strong></p>

<ul>
<li>Open the Cloudant Dashboard for each service instance.</li>
<li>Click the "Replication" icon from the panel menu.</li>
<li>Click the "New Replication" button.</li>
<li>Set the following "Source" values in the "Job configuration" panel.

<ul>
<li>Type: <em>"Local Database"</em></li>
<li>Name: <em>"todos"</em></li>
<li>Authentication: <em>"Cloudant username or API Key"</em></li>
<li>Fill in the API key and password for this local database host in the input fields.</li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-source.png %}</p>

<ul>
<li>Set the following "Target" values in the "Job configuration" panel.

<ul>
<li>Type: <em>"Existing Remote Database"</em></li>
<li>Name: <em>"https://<REMOTE_CLOUDANT_HOST>/todos"</em></li>
<li>Authentication: <em>"Cloudant username or API Key"</em></li>
<li>Fill in the API key and password for the remote database host in the input fields.</li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-target.png %}</p>

<p><em>Wondering what the REMOTE_CLOUDANT_HOST is? Use hostname from the Cloudant dashboard, e.g. XXXX-bluemix.cloudant.com</em></p>

<ul>
<li>Set the following "Options" values in the "Job configuration" panel.

<ul>
<li>Replication type: <em>"Continuous"</em></li>
</ul>
</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/task-options.png %}</p>

<ul>
<li>Click "Start Replication"</li>
<li>Verify the replication table shows the new replication task state as "<em>Running</em>". 👍</li>
</ul>


<p>{% img /images/ha-serverless-apps-todo/replication-jobs-table.png %}</p>

<h2>test it out!</h2>

<p>Use the <a href="https://www.todobackend.com/client/index.html">TODO front-end application</a> with the APIGW URLs for each region simultaneously. Interactions with the todo list in one region should automatically propagate to the other region.</p>

<p>{% img /images/ha-serverless-apps-todo/todo-app.gif %}</p>

<p>The "Active Tasks" panel on the Cloudant Dashboard shows the documents replicated between instances and pending changes. If there are errors synchronising changes to the replication target, the host uses exponential backoff to re-try the replication tasks.</p>

<p><a href="https://console.bluemix.net/docs/services/Cloudant/guides/conflicts.html#finding-conflicts">Conflicts</a> between document changes are handled using CouchDB's <a href="http://guide.couchdb.org/draft/conflicts.html">conflict mechanism</a>. Applications are responsible for detecting and resolving document conflicts in the front-end.</p>

<h2>conclusion</h2>

<p>Running the same serverless application in multiple regions, using the GLB to proxy traffic, allows applications to manage regional outages. But what if all the application instances rely on the same database service? The "single point of failure" has shifted from the application runtime to the database host. 👎</p>

<p>Provisioning independent databases in each application regions is one solution. Applications use their regional database instance and are protected from issues in other regions. This strategy relies on database changes being synchronised between instances to keep the application state consistent. 👍</p>

<p>IBM Cloudant has a built-in replication service to synchronised changes between source and host databases. Setting up bi-directional replication tasks between all instances enables a  "multi-master" replication strategy. This allows applications to access any database instance and have the same state available globally. 🕺🕺🕺</p>
]]></content>
  </entry>
  
</feed>
