<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: serverless | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/serverless/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2019-08-08T10:33:32+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Serverless Functions with WebAssembly Modules]]></title>
    <link href="http://jamesthom.as/blog/2019/08/06/serverless-and-webassembly-modules/"/>
    <updated>2019-08-06T10:25:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/08/06/serverless-and-webassembly-modules</id>
    <content type="html"><![CDATA[<p>Watching a <a href="https://london.serverlessdays.io/speakers/lin/">recent talk</a> by <a href="https://twitter.com/linclark">Lin Clark</a> and <a href="https://twitter.com/tschneidereit">Till Schneidereit</a> about <a href="https://webassembly.org/">WebAssembly</a> (Wasm) inspired me to start experimenting with using WebAssembly <a href="https://webassembly.org/docs/modules/">modules</a> from <a href="https://en.wikipedia.org/wiki/Serverless_computing">serverless functions</a>.</p>

<p>This blog post demonstrates how to invoke functions written in C from Node.js serverless functions. Source code in C is compiled to Wasm modules and bundled in the deployment package. Node.js code implements the serverless platform handler and calls native functions upon invocations.</p>

<p>The examples should work (with some modifications) on any serverless platform that supports deploying Node.js functions from a zip file. I'll be using <a href="https://cloud.ibm.com/functions/">IBM Cloud Functions</a> (<a href="https://openwhisk.apache.org/">Apache OpenWhisk</a>).</p>

<h2>WebAssembly</h2>

<blockquote><p>WebAssembly (abbreviated <em>Wasm</em>) is a binary instruction format for a stack-based virtual machine. Wasm is designed as a portable target for compilation of high-level languages like C/C++/Rust.</p>

<p><a href="https://webassembly.org/">https://webassembly.org/</a></p></blockquote>

<p>Wasm started as a project to run low-level languages in the browser. This was envisioned as a way to execute computationally intensive tasks in the client, e.g. image manipulation, machine learning, graphics engines. This would improve performance for those tasks compared to using JavaScript.</p>

<p>WebAssembly compiles languages like C, C++ and Rust to a portable instruction format, rather than platform-specific machine code. Compiled Wasm files are interpreted by a Wasm VM in the browser or other runtimes. <a href="https://developer.mozilla.org/en-US/docs/WebAssembly/Using_the_JavaScript_API">APIs have been defined</a> to support importing and executing Wasm modules from JavaScript runtimes. These APIs have been implemented in multiple browsers and recent Node.js versions (v8.0.0+).</p>

<p><strong>This means Node.js serverless functions, using a runtime version above 8.0.0, can use WebAssembly!</strong></p>

<h3>Wasm Modules + Serverless</h3>

<p><em>"Why would we want to use WebAssembly Modules from Node.js Serverless Functions?"</em> ü§î</p>

<h4>Performance</h4>

<p>Time is literally money with serverless platforms. The faster the code executes, the less it will cost. Using C, C++ or Rust code, compiled to Wasm modules, for <a href="https://medium.com/@torch2424/webassembly-is-fast-a-real-world-benchmark-of-webassembly-vs-es6-d85a23f8e193">computationally intensive tasks</a> can be much faster than the same algorithms implemented in JavaScript.</p>

<h4>Easier use of native libraries</h4>

<p>Node.js already <a href="https://github.com/nodejs/node-gyp">has a way</a> to use native libraries (in C or C++) from the runtime. This works by compiling the native code during the NPM installation process. Libraries bundled in deployment packages need to be compiled for the serverless platform runtime, not the development environment.</p>

<p>Developers often resort to using <a href="https://github.com/apache/openwhisk/blob/master/docs/actions-nodejs.md#handling-npm-libraries-with-native-dependencies">specialised containers</a> or <a href="https://aws.amazon.com/blogs/compute/nodejs-packages-in-lambda/">VMs</a>, that try to match the runtime environments, for library compilation. This process is error-prone, difficult to debug and a source of problems for developers new to serverless.</p>

<p>Wasm is deliberately platform independent. This means Wasm code compiled locally will work on any Wasm runtime. No more worrying about platform architectures and complex toolchains for native libraries!</p>

<h4>Additional runtime support</h4>

<p><a href="https://github.com/appcypher/awesome-wasm-langs">Dozens of languages</a> now support compiling to WebAssembly.</p>

<p>Want to write serverless functions in Rust, C, or Lua? No problem! By wrapping Wasm modules with a small Node.js handler function, developers can write their serverless applications in any language with "compile to Wasm" support.</p>

<p>Developers don't have to be restricted to the runtimes provided by the platform.</p>

<h3>JS APIs in Node.js</h3>

<p>Here is the code needed to load a Wasm module from Node.js. Wasm modules are distributed in <code>.wasm</code> files. Loaded modules are instantiated into instances, by providing a configurable runtime environment. Functions exported from Wasm modules can then be invoked on these instances from Node.js.</p>

<p><code>javascript
const wasm_module = 'library.wasm'
const bytes = fs.readFileSync(wasm_module)
const wasmModule = new WebAssembly.Module(bytes);
const wasmMemory = new WebAssembly.Memory({initial: 512});
const wasmInstance = new WebAssembly.Instance(wasmModule, { env: { memory: wasmMemory } }})
</code></p>

<h4>Calling Functions</h4>

<p>Exported Wasm functions are available on the <code>exports</code> property of the <code>wasmInstance</code>. These properties can be invoked as normal functions.</p>

<p><code>javascript
const result = wasmInstance.exports.add(2, 2)
</code></p>

<h4>Passing &amp; Returning Values</h4>

<p>Exported Wasm functions can only receive and return <a href="https://webassembly.github.io/spec/core/syntax/types.html">native Wasm types</a>. This (<a href="https://github.com/WebAssembly/reference-types">currently</a>) means only integers.</p>

<p>Values that can be represented as a series of numbers, e.g. strings or arrays, can be <a href="https://stackoverflow.com/questions/41875728/pass-a-javascript-array-as-argument-to-a-webassembly-function">written directly</a> to the Wasm instance memory heap from Node.js. Heap memory references can be passed as the function parameter values, allowing the Wasm code to read these values. More complex types (e.g. JS objects) are not supported.</p>

<p>This process can also be <a href="https://stackoverflow.com/questions/41353389/how-can-i-return-a-javascript-string-from-a-webassembly-function">used in reverse</a>, with Wasm functions returning heap references to pass back strings or arrays with the function result.</p>

<p>For more details on how memory works in Web Assembly, please see this <a href="https://hacks.mozilla.org/2017/07/memory-in-webassembly-and-why-its-safer-than-you-think/">page</a>.</p>

<h2>Examples</h2>

<p>Having covered the basics, let's look at some examples...</p>

<p>I'll start with calling a <a href="https://gist.github.com/jthomas/5de757fd36b3c6904e5c5f12c8264b41">simple C function from a Node.js serverless function</a>. This will demonstrate the complete steps needed to compile and use a small C program as a Wasm module. Then I'll look at a more real-world use-case, <a href="https://github.com/jthomas/openwhisk-image-resize-wasm">dynamic image resizing</a>. This will use a C library compiled to Wasm to improve performance.</p>

<p>Examples will be deployed to <a href="https://cloud.ibm.com/functions">IBM Cloud Functions</a> (<a href="https://openwhisk.apache.org/">Apache OpenWhisk</a>). They should work on other serverless platforms (supporting the Node.js runtime) with small modifications to the handler function's interface.</p>

<h3>Simple Function Calls</h3>

<h4>Create Source Files</h4>

<ul>
<li>Create a file <code>add.c</code> with the following contents:</li>
</ul>


<p><code>c
int add(int a, int b) {
  return a + b;
}
</code></p>

<ul>
<li>Create a file (<code>index.js</code>) with the following contents:</li>
</ul>


<p>```javascript
'use strict';
const fs = require('fs');
const util = require('util')</p>

<p>const WASM_MODULE = 'add.wasm'
let wasm_instance</p>

<p>async function load_wasm(wasm_module) {
  if (!wasm_instance) {</p>

<pre><code>const bytes = fs.readFileSync(wasm_module);
const memory = new WebAssembly.Memory({initial: 1});
const env = {
  __memory_base: 0, memory
}

const { instance, module } = await WebAssembly.instantiate(bytes, { env });
wasm_instance = instance
</code></pre>

<p>  }</p>

<p>  return wasm_instance.exports._add
}</p>

<p>exports.main = async function ({ a = 1, b = 1 }) {
  const add = await load_wasm(WASM_MODULE)
  const sum = add(a, b)
  return { sum }
}
```</p>

<ul>
<li>Create a file (<code>package.json</code>) with the following contents:</li>
</ul>


<p><code>json
{
  "name": "wasm",
  "version": "1.0.0",
  "main": "index.js"
}
</code></p>

<h4>Compile Wasm Module</h4>

<p>This C source file needs compiling to a WebAssembly module. There are different projects to handle this. I will be using <a href="https://emscripten.org/">Emscripten</a>, which uses LLVM to compile C and C++ to WebAssembly.</p>

<ul>
<li><p><a href="https://emscripten.org/docs/getting_started/downloads.html">Install</a> the <a href="https://emscripten.org/">Emscripten</a> toolchain.</p></li>
<li><p>Run the following command to generate the Wasm module.</p></li>
</ul>


<p><code>sh
emcc -s WASM=1 -s SIDE_MODULE=1 -s EXPORTED_FUNCTIONS="['_add']" -O1 add.c -o add.wasm
</code></p>

<p><em>The <code>SIDE_MODULE</code> option tells the compiler the Wasm module will be loaded manually using the JS APIs. This stops Emscripten generating a corresponding JS file to do this automatically. Functions exposed on the Wasm module are controlled by the <code>EXPORTED_FUNCTIONS</code> configuration parameter.</em></p>

<h4>Deploy Serverless Function</h4>

<ul>
<li>Create deployment package with source files.</li>
</ul>


<p><code>
zip action.zip index.js add.wasm package.json
</code></p>

<ul>
<li>Create serverless function from deployment package.</li>
</ul>


<p><code>
ibmcloud wsk action create wasm action.zip --kind nodejs:10
</code></p>

<ul>
<li>Invoke serverless function to test Wasm module.</li>
</ul>


<p>```
$ ibmcloud wsk action invoke wasm -r -p a 2 -p b 2
{</p>

<pre><code>"sum": 4
</code></pre>

<p>}
```</p>

<p>It works! üéâüéâüéâ</p>

<p>Whilst this is a trivial example, it demonstrates the workflow needed to compile C source files to Wasm modules and invoke exported functions from Node.js serverless functions. Let's move onto a more realistic example...</p>

<h3>Dynamic Image Resizing</h3>

<p>This <a href="https://github.com/jthomas/openwhisk-image-resize-wasm">repository</a> contains a serverless function to resize images using a C library called via WebAssembly. It is a fork of the <a href="https://github.com/cloudflare/cloudflare-workers-wasm-demo">original code</a> created by Cloudflare for their Workers platform. See the original repository for details on what the repository contains and how the files work.</p>

<h4>Checkout Repository</h4>

<ul>
<li>Retrieve the source files by checking out this <a href="https://github.com/jthomas/openwhisk-image-resize-wasm">repository</a>.</li>
</ul>


<p><code>
git clone https://github.com/jthomas/openwhisk-image-resize-wasm
</code></p>

<p>This repository contains the pre-compiled Wasm module (<code>resize.wasm</code>) needed to resize images using the <a href="https://github.com/nothings/stb">stb library</a>. The module exposes two functions: <code>init</code> and <code>resize</code>.</p>

<p>The <code>init</code> function <a href="https://github.com/jthomas/openwhisk-image-resize-wasm/blob/master/main.c#L29-L38">returns a heap reference</a> to write the image bytes for processing <a href="https://github.com/jthomas/openwhisk-image-resize-wasm/blob/master/worker.js#L59">into</a>. The <code>resize</code> <a href="https://github.com/jthomas/openwhisk-image-resize-wasm/blob/master/main.c#L49">function</a> is called with two values, the image byte array length and new width value. It uses these values to read the image bytes from the heap and calls the library functions to resize the image to the desired width. Resized image bytes are written back to the heap and the new byte array length is returned.</p>

<h4>Deploy Serverless Function</h4>

<ul>
<li>Create deployment package from source files.</li>
</ul>


<p><code>
zip action.zip resizer.wasm package.json worker.js
</code></p>

<ul>
<li>Create serverless function from deployment package.</li>
</ul>


<p><code>
ibmcloud wsk action update resizer action.zip --kind nodejs:10 --web true
</code></p>

<ul>
<li>Retrieve HTTP URL for Web Action.</li>
</ul>


<p><code>
ibmcloud wsk action get resizer --url
</code></p>

<p><em>This should return a URL like:</em> <code>https://&lt;region&gt;.cloud.ibm.com/api/v1/web/&lt;ns&gt;/default/resizer</code></p>

<ul>
<li>Open the Web Action URL with the <code>.http</code> extension.</li>
</ul>


<p><code>
https://&lt;region&gt;.cloud.ibm.com/api/v1/web/&lt;ns&gt;/default/resizer.http
</code></p>

<p>This should return the following image resized to 250 pixels (from 900 pixels).</p>

<p><img src="https://bit.ly/2ZlP838" alt="Pug with Ice-cream" /></p>

<p>URL query parameters (<code>url</code> and <code>width</code>) can be used to modify the image source or output width for the next image, e.g.</p>

<p><code>
https://&lt;region&gt;.cloud.ibm.com/api/v1/web/&lt;ns&gt;/default/resizer.http?url=&lt;IMG_URL&gt;&amp;width=500
</code></p>

<h2>Conclusion</h2>

<p>WebAssembly may have started as a way to run native code in the browser, but soon expanded to server-side runtime environments like Node.js. WebAssembly modules are supported on any serverless platform with a Node.js v8.0.0+ runtime.</p>

<p>Wasm provides a fast, safe and secure way to ship portable modules from compiled languages. Developers don't have to worry about whether the module is compiled for the correct platform architecture or linked against unavailable dynamic libraries. This is especially useful for serverless functions in Node.js, where compiling native libraries for production runtimes can be challenging.</p>

<p>Wasm modules can be used to improve performance for computationally intensive calculations, which lowers invocation times and, therefore, costs less. It also provides an easy way to utilise additional runtimes on serverless platforms without any changes by the platform provider.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Serverless APIs for MAX models]]></title>
    <link href="http://jamesthom.as/blog/2019/07/02/serverless-max-models/"/>
    <updated>2019-07-02T10:25:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/07/02/serverless-max-models</id>
    <content type="html"><![CDATA[<p>IBM's <a href="https://developer.ibm.com/exchanges/models/">Model Asset eXchange</a> provides a <a href="https://developer.ibm.com/exchanges/models/all/">curated list</a> of free Machine Learning models for developers. Models currently published include detecting <a href="https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/">emotions</a> or <a href="https://developer.ibm.com/exchanges/models/all/max-facial-age-estimator/">ages</a> in faces from images, <a href="https://developer.ibm.com/exchanges/models/all/max-weather-forecaster/">forecasting the weather</a>, converting <a href="https://developer.ibm.com/exchanges/models/all/max-speech-to-text-converter/">speech to text</a> and more. Models are pre-trained and ready for use in the cloud.</p>

<p>Models are published as series of <a href="https://hub.docker.com/search?q=codait&amp;type=image">public Docker images</a>. Images automatically expose a HTTP API for model predictions. Documentation in the model repositories explains how to run images locally (using Docker) or deploy to the cloud (using Kubernetes). This got me thinking‚Ä¶</p>

<p><strong>Could MAX models be used from serverless functions?</strong> ü§î</p>

<p>Running machine learning models on serverless platforms can take advantage of the horizontal scalability to process large numbers of computationally intensive classification tasks in parallel. Coupled with the serverless pricing structure ("<em>no charge for idle</em>"), this can be an extremely cheap and effective way to perform model classifications in the cloud.</p>

<p><strong>CHALLENGE ACCEPTED!</strong> ü¶∏‚Äç‚ôÇÔ∏èü¶∏‚Äç‚ôÄÔ∏è</p>

<p>After a couple days of experimentation, I had worked out an easy way to <a href="https://github.com/jthomas/serverless-max-models">automatically expose MAX models as Serverless APIs</a> on <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a>.  üéâüéâüéâ</p>

<p><em>I've given instructions below on how to create those APIs from the models using a simple script. If you just want to use the models, follow those instructions. If you are interested in understanding how this works, keep reading as I explain afterwards what I did...</em></p>

<h2>Running MAX models on IBM Cloud Functions</h2>

<p><a href="https://github.com/jthomas/serverless-max-models">This repository</a> contains a <a href="https://github.com/jthomas/serverless-max-models/blob/master/build.sh">bash script</a> which builds custom Docker runtimes with MAX models for usage on <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a>. Pushing these images to Docker Hub allows IBM Cloud Functions to use them as <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">custom runtimes</a>. <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">Web Actions</a> created from these custom runtime images expose the same Prediction API described in the model documentation. They can be used with no further changes or custom code needed.</p>

<h3>prerequisites</h3>

<p>Please follow the links below to set up the following tools before proceeding.</p>

<ul>
<li><a href="https://www.docker.com/">Docker</a></li>
<li><a href="https://hub.docker.com/">Docker Hub account</a></li>
<li><a href="https://cloud.ibm.com/registration">IBM Cloud account</a></li>
<li><a href="https://cloud.ibm.com/openwhisk/learn/cli">IBM Cloud Functions CLI installed</a></li>
</ul>


<p><strong>Check out the "<a href="https://github.com/jthomas/serverless-max-models">Serverless MAX Models</a> repository. Run all the following commands from that folder.</strong></p>

<p><code>
git clone https://github.com/jthomas/serverless-max-models
cd serverless-max-models
</code></p>

<h3>build custom runtime images</h3>

<ul>
<li>Set the following environment variables (<code>MODELS</code>) with <a href="https://hub.docker.com/search?q=codait&amp;type=image">MAX model names</a> and run build script.

<ul>
<li><code>MODELS</code>: MAX model names, e.g. <code>max-facial-emotion-classifier</code></li>
<li><code>USERNAME</code>: Docker Hub username.</li>
</ul>
</li>
</ul>


<p><code>
MODELS="..." USERNAME="..." ./build.sh
</code></p>

<p>This will create Docker images locally with the MAX model names and push to Docker Hub for usage in IBM Cloud Functions. <strong>IBM Cloud Functions only supports public Docker images as custom runtimes.</strong></p>

<h3>create actions using custom runtimes</h3>

<ul>
<li>Create a Web Action using the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">custom Docker runtime</a>.</li>
</ul>


<p><code>
ibmcloud wsk action create &lt;MODEL_IMAGE&gt; --docker &lt;DOCKERHUB_NAME&gt;/&lt;MODEL_IMAGE&gt; --web true -m 512
</code></p>

<ul>
<li>Retrieve the Web Action URL (<code>https://&lt;REGION&gt;.functions.cloud.ibm.com/api/v1/web/&lt;NS&gt;/default/&lt;ACTION&gt;</code>)</li>
</ul>


<p><code>
ibmcloud wsk action get &lt;MODEL_IMAGE&gt; --url
</code></p>

<h3>invoke web action url with prediction api parameters</h3>

<p>Use the same API request parameters as defined in the Prediction API specification with the Web Action URL. This will invoke model predictions and return the result as the HTTP response, e.g.</p>

<p><code>
curl -F "image=@assets/happy-baby.jpeg" -XPOST &lt;WEB_ACTION_URL&gt;
</code></p>

<p><em>NOTE: The first invocation after creating an action may incur long cold-start delays due to the platform pulling the remote image into the local registry. Once the image is available in the platform, both further cold and warm invocations will be much faster.</em></p>

<h2>Example</h2>

<p>Here is an example of creating a serverless API using the <code>max-facial-emotion-classifier</code> <a href="https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/">MAX model</a>. Further examples of models which have been tested are available <a href="https://github.com/jthomas/serverless-max-models/blob/master/README.md#models">here</a>. If you encounter problems, please <a href="https://github.com/jthomas/serverless-max-models/issues">open an issue</a> on Github.</p>

<h3>max-facial-emotion-classifier</h3>

<ul>
<li><a href="https://developer.ibm.com/exchanges/models/all/max-facial-emotion-classifier/">Facial Emotion Classifier (<code>max-facial-emotion-classifier</code>)</a></li>
</ul>


<p>Start by creating the action using the custom runtime and then retrieve the Web Action URL.</p>

<p><code>
$ ibmcloud wsk action create max-facial-emotion-classifier --docker &lt;DOCKERHUB_NAME&gt;/max-facial-emotion-classifier --web true -m 512
ok: created action max-facial-emotion-classifier
$ ibmcloud wsk action get max-facial-emotion-classifier --url
ok: got action max-facial-emotion-classifier
https://&lt;REGION&gt;.functions.cloud.ibm.com/api/v1/web/&lt;NS&gt;/default/max-facial-emotion-classifier
</code></p>

<p>According to the <a href="http://max-facial-emotion-classifier.max.us-south.containers.appdomain.cloud/">API definition</a> for this model, the prediction API expects a form submission with an image file to classify. Using a <a href="https://github.com/IBM/MAX-Facial-Emotion-Classifier/blob/master/assets/happy-baby.jpeg">sample image</a> from the model repo, the model can be tested using curl.</p>

<p><code>
$ curl -F "image=@happy-baby.jpeg" -XPOST https://&lt;REGION&gt;.functions.cloud.ibm.com/api/v1/web/&lt;NS&gt;/default/max-facial-emotion-classifier
</code></p>

<p>```json
{
  "status": "ok",
  "predictions": [</p>

<pre><code>{
  "detection_box": [
    0.15102639296187684,
    0.3828125,
    0.5293255131964809,
    0.5830078125
  ],
  "emotion_predictions": [
    {
      "label_id": "1",
      "label": "happiness",
      "probability": 0.9860254526138306
    },
    ...
  ]
}
</code></pre>

<p>  ]
}
```</p>

<h4>performance</h4>

<p><em>Example Invocation Duration (Cold):</em> ~4.8 seconds</p>

<p><em>Example Invocation Duration (Warm):</em> ~ 800 ms</p>

<h2>How does this work?</h2>

<h3>background</h3>

<p>Running machine learning classifications using pre-trained models from serverless functions has historically been challenging due to the following reason‚Ä¶</p>

<blockquote><p>Developers do not control runtime environments in (most) serverless cloud platforms. Libraries and dependencies needed by the functions must be provided in the deployment package. Most platforms limit deployment package sizes (~50MB compressed &amp; ~250MB uncompressed).</p></blockquote>

<p>Machine Learning libraries and models can be much larger than those deployment size limits. This stops them being included in deployment packages. Loading files dynamically during invocations may be possible but incurs extremely long cold-start delays and additional costs.</p>

<p>Fortunately, <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> is based on the open-source serverless project, <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>. This platform supports bespoke function runtimes using <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">custom Docker images</a>. Machine learning libraries and models can therefore be provided in custom runtimes. This removes the need to include them in deployment packages or be loaded at runtime.</p>

<p><em>Interested in reading other blog posts about using machine learning libraries and toolkits with IBM Cloud Functions? See <a href="http://jamesthom.as/blog/2017/08/04/large-applications-on-openwhisk/">these posts</a> for <a href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/">more details</a>.</em></p>

<h3>MAX model images</h3>

<p>IBM's <a href="https://developer.ibm.com/exchanges/models/all/">Model Asset eXchange</a> publishes Docker images for each model, alongside the pre-trained model files. Images expose a <a href="https://github.com/IBM/MAX-Text-Sentiment-Classifier#3-use-the-model">HTTP API for predictions</a> using the model on port 5000, built using Python and Flask. <a href="http://max-text-sentiment-classifier.max.us-south.containers.appdomain.cloud/">Swagger files</a> for the APIs describe the available operations, input parameters and response bodies.</p>

<p>These images use a custom application framework (<a href="https://pypi.org/project/maxfw/">maxfw</a>), based on Flask, to standardise exposing MAX models as HTTP APIs. This framework handles input parameter validation, response marshalling, CORS support, etc. This allows model runtimes to just implement the prediction API handlers, rather than the entire HTTP application.</p>

<p>Since the framework already handles exposing the model as a HTTP API, I started looking for a way to simulate an external HTTP request coming into the framework. If this was possible, I could trigger this fake request from a Python Web Action to perform the model classification from input parameters. The Web Action would then covert the HTTP response returned into the valid Web Action response parameters.</p>

<h3>flask test client</h3>

<p>Reading through the Flask <a href="http://flask.pocoo.org/docs/1.0/testing/">documentation</a>, I came across the perfect solution! üëèüëèüëè</p>

<blockquote><p>Flask provides a way to test your application by exposing the Werkzeug test Client and handling the context locals for you. You can then use that with your favourite testing solution.</p></blockquote>

<p>This allows application routes to be executed with the <a href="https://werkzeug.palletsprojects.com/en/0.15.x/test/#werkzeug.test.Client">test client</a>, without actually running the HTTP server.</p>

<p><code>python
max_app = MAXApp(API_TITLE, API_DESC, API_VERSION)
max_app.add_api(ModelPredictAPI, '/predict')
test_client = max_app.app.test_client()
r = test_client.post('/model/predict', data=content, headers=headers)
</code></p>

<p>Using this code within a serverless Python function allows function invocations to trigger the prediction API.  The serverless function only has to convert input parameters to the fake HTTP request and then serialise the response back to JSON.</p>

<h3>python docker action</h3>

<p>The custom MAX model runtime image needs to implement the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">HTTP API expected</a> by Apache OpenWhisk. This API is used to instantiate the runtime environment and then pass in invocation parameters on each request. Since the runtime image contains all files and code need to process requests, the <code>/init</code> handler becomes a <a href="https://english.stackexchange.com/questions/25993/what-does-no-op-mean">no-op</a>. The <code>/run</code> handler converts <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md#http-context">Web Action HTTP parameters</a> into the fake HTTP request.</p>

<p>Here is the Python script used to proxy incoming Web Actions requests to the framework model service.</p>

<p>```python
from maxfw.core import MAXApp
from api import ModelPredictAPI
from config import API_TITLE, API_DESC, API_VERSION
import json
import base64
from flask import Flask, request, Response</p>

<p>max_app = MAXApp(API_TITLE, API_DESC, API_VERSION)
max_app.add_api(ModelPredictAPI, '/predict')</p>

<h1>Use flask test client to simulate HTTP requests for the prediction APIs</h1>

<h1>HTTP request data will come from action invocation parameters, neat huh? :)</h1>

<p>test_client = max_app.app.test_client()
app = Flask(<strong>name</strong>)</p>

<h1>This implements the Docker runtime API used by Apache OpenWhisk</h1>

<h1>https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md</h1>

<h1>/init is a no-op as everything is provided in the image.</h1>

<p>@app.route("/init", methods=['POST'])
def init():</p>

<pre><code>return ''
</code></pre>

<h1>Action invocation requests will be received as the <code>value</code> parameter in request body.</h1>

<h1>Web Actions provide HTTP request parameters as <code>__ow_headers</code> &amp; <code>__ow_body</code> parameters.</h1>

<p>@app.route("/run", methods=['POST'])
def run():</p>

<pre><code>body = request.json
form_body = body['value']['__ow_body']
headers = body['value']['__ow_headers']

# binary image content provided as base64 strings
content = base64.b64decode(form_body)

# send fake HTTP request to prediction API with invocation data
r = test_client.post('/model/predict', data=content, headers=headers)
r_headers = dict((x, y) for x, y in r.headers)

# binary data must be encoded as base64 strings to return in JSON response
is_image = r_headers['Content-Type'].startswith('image')
r_data = base64.b64encode(r.data) if is_image else r.data
body = r_data.decode("utf-8")

response = {'headers': r_headers, 'status': r.status_code, 'body': body }
print (r.status)
return Response(json.dumps(response), status=200, mimetype='application/json')
</code></pre>

<p>app.run(host='0.0.0.0', port=8080)
```</p>

<h3>building into an image</h3>

<p>Since the MAX models already exist as public Docker images, those images can be used as base images when building custom runtimes. Those base images handle adding model files and all dependencies needed to execute them into the image.</p>

<p>This is the <code>Dockerfile</code> used by the build script to create the custom model image. The <code>model</code> parameter refers to the build argument containing the model name.</p>

<p>```bash
ARG model
FROM codait/${model}:latest</p>

<p>ADD openwhisk.py .</p>

<p>EXPOSE 8080</p>

<p>CMD python openwhisk.py
```</p>

<p>This is then used from the following build script to create a custom runtime image for the model.</p>

<p>```bash</p>

<h1>!/bin/bash</h1>

<p>set -e -u</p>

<p>for model in $MODELS; do
  echo "Building $model runtime image"
  docker build -t $model --build-arg model=$model .
  echo "Pushing $model to Docker Hub"
  docker tag $model $USERNAME/$model
  docker push $USERNAME/$model
done
```</p>

<p>Once the image is published to Docker Hub, it can be referenced when creating new Web Actions (using the <code>‚Äîdocker</code> parameter). üòé</p>

<p><code>
ibmcloud wsk action create &lt;MODEL_IMAGE&gt; --docker &lt;DOCKERHUB_NAME&gt;/&lt;MODEL_IMAGE&gt; --web true -m 512
</code></p>

<h2>Conclusion</h2>

<p>IBM's Model Asset eXchange is a curated collection of Machine Learning models, ready to deploy to the cloud for a variety of tasks. All models are available as a series of public Docker images. Models images automatically expose HTTP APIs for classifications.</p>

<p>Documentation in the model repositories explains how to run them locally and deploy using Kubernetes, but what about using on serverless cloud platforms? Serverless platforms are becoming a popular option for deploying Machine Learning models, due to horizontal scalability and cost advantages.</p>

<p>Looking through the source code for the model images, I discovered a mechanism to hook into the custom model framework used to export the model files as HTTP APIs. This allowed me write a simple wrapper script to proxy serverless function invocations to the model prediction APIs. API responses would be serialised back into the Web Action response format.</p>

<p>Building this script into a new Docker image, using the existing model image as the base image, created a new runtime which could be used on the platform. Web Actions created from this runtime image would automatically expose the same HTTP APIs as the existing image!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing Long-Running Apache OpenWhisk Actions Results]]></title>
    <link href="http://jamesthom.as/blog/2019/05/14/accessing-long-running-openwhisk-actions-results/"/>
    <updated>2019-05-14T11:35:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/05/14/accessing-long-running-openwhisk-actions-results</id>
    <content type="html"><![CDATA[<p><a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> actions are invoked by sending HTTP POST requests to the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md">platform API</a>. Invocation requests have two <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md#actions">different modes</a>: <strong>blocking</strong> and <strong>non-blocking</strong>.</p>

<p><strong>Blocking invocations</strong> mean the platform won't send the HTTP response until the action finishes. This allows it to include the action result in the response.  Blocking invocations are used when you want to invoke an action and wait for the result.</p>

<p>```
$ wsk action invoke my_action --blocking
ok: invoked /_/my_action with id db70ef682fae4f8fb0ef682fae2f8fd5
{</p>

<pre><code>"activationId": "db70ef682fae4f8fb0ef682fae2f8fd5",
...
"response": {
    "result": { ... },
    "status": "success",
    "success": true
},
...
</code></pre>

<p>}
```</p>

<p><strong>Non-blocking invocations</strong> return as soon as the platform processes the invocation request. This is before the action has finished executing. HTTP responses from non-blocking invocations only include activation identifiers, as the action result is not available.</p>

<p><code>
$ wsk action invoke my_action
ok: invoked /_/my_action with id d2728aaa75394411b28aaa7539341195
</code></p>

<p><strong>HTTP responses from a blocking invocation will only wait for a limited amount of time before returning.</strong> This defaults to 65 seconds in the <a href="https://github.com/apache/incubator-openwhisk/blob/master/core/controller/src/main/resources/application.conf#L21">platform configuration file</a>. If an action invocation has not finished before this timeout limit, a HTTP 5xx status response is returned.</p>

<p>Hmmm‚Ä¶ ü§î</p>

<p><strong><em>"So, how can you invoke an action and wait for the result when actions take longer than this limit?"</em></strong></p>

<p>This question comes up regularly from developers building applications using the platform. I've decided to turn my answer into a blog post to help others struggling with this issue (after answering this question again this week üòé).</p>

<h3>solution</h3>

<ul>
<li><em>Invoke the action using a <a href="https://github.com/apache/incubator-openwhisk-client-js#invoke-action">non-blocking invocation</a>.</em></li>
<li><em>Use the returned activation identifier to poll the <a href="https://github.com/apache/incubator-openwhisk-client-js#retrieve-resource">activation result API</a>.</em></li>
<li><em>The HTTP response for the activation result will return a HTTP 404 response until the action finishes.</em></li>
</ul>


<p>When polling for activation results from non-blocking invocations, you should enforce a limit on the maximum polling time allowed. This is because HTTP 404s can be returned due to other scenarios (e.g. invalid activation identifiers). Enforcing a time limit ensures that, in the event of issues in the application code or the platform, the polling loop with eventually stop!</p>

<p><em>Setting the maximum polling time to the action timeout limit (plus a small offset) is a good approach.</em></p>

<p>An action cannot run for longer than its timeout limit. If the activation record is not available after this duration has elapsed (plus a small offset to handle internal platform delays), something has gone wrong. Continuing to poll after this point runs the risk of turning the polling operation into an infinite loop...</p>

<h3>example code</h3>

<p>This example provides an implementation of this approach for Node.js using the <a href="https://github.com/apache/incubator-openwhisk-client-js">JavaScript Client SDK</a>.</p>

<p>```javascript
"use strict";</p>

<p>const openwhisk = require('openwhisk')</p>

<p>const options = { apihost: <API_HOST>, api_key: <API_KEY> }
const ow = openwhisk(options)</p>

<p>// action duration limit (+ small offset)
const timeout_ms = 85000
// delay between polling requests
const polling_delay = 1000
// action to invoke
const action = 'delay'</p>

<p>const now = () => (new Date().getTime())
const max_polling_time = now() + timeout_ms</p>

<p>const delay = async ms => new Promise(resolve => setTimeout(resolve, ms))</p>

<p>const activation = await ow.actions.invoke({name: action})
console.log(<code>new activation id: ${activation.activationId}</code>)</p>

<p>let result = null</p>

<p>do {
  try {</p>

<pre><code>result = await ow.activations.get({ name: activation.activationId })
console.log(`activation result (${activation.activationId}) now available!`)
</code></pre>

<p>  } catch (err) {</p>

<pre><code>if (err.statusCode !== 404) {
  throw err
}
console.log(`activation result (${activation.activationId}) not available yet`)
</code></pre>

<p>  }</p>

<p>  await delay(polling_delay)
} while (!result &amp;&amp; now() &lt; max_polling_time)</p>

<p>console.log(<code>activation result (${activation.activationId})</code>, result)
```</p>

<h3>testing it out</h3>

<p>Here is the source code for an action which will not return until 70 seconds have passed. Blocking invocations firing this action will result in a HTTP timeout before the response is returned.</p>

<p>```javascript
const delay = async ms => new Promise(resolve => setTimeout(resolve, ms))</p>

<p>function main() {
  return delay(70*1000)
}
```</p>

<p>Using the script above, the action result will be retrieved from a non-blocking invocation.</p>

<ul>
<li><em>Create an action from the source file in the example above.</em></li>
</ul>


<p><code>
wsk action create delay delay.js --timeout 80000 --kind nodejs:10
</code></p>

<ul>
<li><em>Run the Node.js script to invoke this action and poll for the activation result.</em></li>
</ul>


<p><code>
node script.js
</code></p>

<p>If the script runs correctly, log messages will display the polling status and then the activation result.</p>

<p><code>
$ node script.js
new activation id: d4efc4641b544320afc4641b54132066
activation result (d4efc4641b544320afc4641b54132066) not available yet
activation result (d4efc4641b544320afc4641b54132066) not available yet
activation result (d4efc4641b544320afc4641b54132066) not available yet
...
activation result (d4efc4641b544320afc4641b54132066) now available!
activation result (d4efc4641b544320afc4641b54132066) { ... }
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Saving Money and Time With Node.js Worker Threads in Serverless Functions]]></title>
    <link href="http://jamesthom.as/blog/2019/05/08/node-dot-js-worker-threads-with-serverless-functions/"/>
    <updated>2019-05-08T12:17:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/05/08/node-dot-js-worker-threads-with-serverless-functions</id>
    <content type="html"><![CDATA[<p>Node.js v12 was <a href="https://foundation.nodejs.org/announcements/2019/04/24/node-js-foundation-and-js-foundation-merge-to-form-openjs-foundation-2">released last month</a>. This new version includes support for <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a>, that are enabled by default. Node.js <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a> make it simple to execute JavaScript code in parallel using threads. üëèüëèüëè</p>

<p>This is useful for Node.js applications with CPU-intensive workloads. Using Worker Threads, JavaScript code can be executed code concurrently using multiple CPU cores. This reduces execution time compared to a non-Worker Threads version.</p>

<p>If serverless platforms provide Node.js v12 on multi-core environments, functions can use this feature to reduce execution time and, therefore, lower costs. Depending on the workload, functions can utilise all available CPU cores to parallelise work, rather than executing more functions concurrently. üí∞üí∞üí∞</p>

<p><strong>In this blog post, I'll explain how to use Worker Threads from a serverless function. I'll be using <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> (<a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) as the example platform but this approach is applicable for any serverless platform with Node.js v12 support and a multi-core CPU runtime environment.</strong></p>

<h2>Node.js v12 in IBM Cloud Functions (Apache OpenWhisk)</h2>

<p><em>This section of the blog post is specifically about using the new <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js v12 runtime</a> on IBM Cloud Functions (powered by <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>). If you are using a different serverless platform, feel free to skip ahead to the next section‚Ä¶</em></p>

<p>I've recently <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/pull/126">been working</a> on adding the Node.js v12 runtime to Apache OpenWhisk.</p>

<p>Apache OpenWhisk uses <a href="https://hub.docker.com/u/openwhisk">Docker containers</a> as runtime environments for serverless functions. All runtime images are maintained in separate repositories for each supported language, e.g. <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js</a>, <a href="https://github.com/apache/incubator-openwhisk-runtime-java">Java</a>, <a href="https://github.com/apache/incubator-openwhisk-runtime-python">Python</a>, etc. Runtime images are automatically built and pushed to <a href="https://hub.docker.com/r/openwhisk/">Docker Hub</a> when the repository is updated.</p>

<h3>node.js v12 runtime image</h3>

<p>Here is <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/pull/126">the PR</a> used to add the new Node.js v12 runtime image to Apache OpenWhisk. This led to the following <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v12">runtime image</a> being exported to Docker Hub: <code>openwhisk/action-nodejs-v12</code>.</p>

<p>Having this image available as a native runtime in Apache OpenWhisk requires <a href="https://github.com/apache/incubator-openwhisk/pull/4472">upstream changes</a> to the project's runtime manifest. After this happens, developers will be able to use the <code>--kind</code> CLI flag to select this runtime version.</p>

<p><code>
ibmcloud wsk action create action_name action.js --kind nodejs:12
</code></p>

<p><a href="http://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> is powered by <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>. It will eventually pick up the upstream project changes to include this new runtime version. Until that happens, Docker support allows usage of this new runtime before it is built-in the platform.</p>

<p><code>
ibmcloud wsk action create action_name action.js --docker openwhisk/action-nodejs-v12
</code></p>

<h3>example</h3>

<p>This Apache OpenWhisk action returns the version of Node.js used in the runtime environment.</p>

<p>```javascript
function main () {
  return {</p>

<pre><code>version: process.version
</code></pre>

<p>  }
}
```</p>

<p>Running this code on IBM Cloud Functions, using the Node.js v12 runtime image, allows us to confirm the new Node.js version is available.</p>

<p>```
$ ibmcloud wsk action create nodejs-v12 action.js --docker openwhisk/action-nodejs-v12
ok: created action nodejs-v12
$ ibmcloud wsk action invoke nodejs-v12 --result
{</p>

<pre><code>"version": "v12.1.0"
</code></pre>

<p>}
```</p>

<h2>Worker Threads in Serverless Functions</h2>

<p><a href="https://medium.com/@Trott/using-worker-threads-in-node-js-80494136dbb6">This is a great introdution blog post</a> to Workers Threads. It uses an example of generating prime numbers as the CPU intensive task to benchmark. Comparing the performance of the single-threaded version to multiple-threads - the performance is improved as a factor of the threads used (up to the number of CPU cores available).</p>

<p>This code can be ported to run in a serverless function. Running with different input values and thread counts will allow benchmarking of the performance improvement.</p>

<h3>non-workers version</h3>

<p>Here is the <a href="https://gist.github.com/jthomas/71c76d62ddfd146c4bf763f5b2f0eec1">sample code</a> for a serverless function to generate prime numbers. It does not use Worker Threads. It will run on the main <a href="https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/">event loop</a> for the Node.js process. This means it will only utilise a single thread (and therefore single CPU core).</p>

<p>```javascript
'use strict';</p>

<p>const min = 2</p>

<p>function main(params) {
  const { start, end } = params
  console.log(params)
  const primes = []
  let isPrime = true;
  for (let i = start; i &lt; end; i++) {</p>

<pre><code>for (let j = min; j &lt; Math.sqrt(end); j++) {
  if (i !== j &amp;&amp; i%j === 0) {
    isPrime = false;
    break;
  }
}
if (isPrime) {
  primes.push(i);
}
isPrime = true;
</code></pre>

<p>  }</p>

<p>  return { primes }
}
```</p>

<h3>porting the code to use worker threads</h3>

<p>Here is the prime number calculation code which uses Worker Threads. Dividing the total input range by the number of Worker Threads generates individual thread input values. Worker Threads are spawned and passed chunked input ranges. Threads calculate primes and then send the result back to the parent thread.</p>

<script src="https://gist.github.com/Trott/7bb7ee55c247047d030b4c427434ef51.js"></script>


<p>Reviewing the code to start converting it to a serverless function, I realised there were two issues running this code in serverless environment: <strong>worker thread initialisation</strong> and <strong>optimal worker thread counts</strong>.</p>

<h4>How to initialise Worker Threads?</h4>

<p>This is how the existing source code <a href="https://nodejs.org/dist/latest-v12.x/docs/api/worker_threads.html#worker_threads_new_worker_filename_options">initialises the Worker Threads</a>.</p>

<p><code>javascript
 threads.add(new Worker(__filename, { workerData: { start: myStart, range }}));
</code></p>

<p> <em><code>__filename</code> is a special global variable in Node.js which contains the currently executing script file path.</em></p>

<p>This means the Worker Thread will be initialised with a copy of the currently executing script. Node.js provides a special variable to indicate whether the script is executing in the parent or child thread. This can be used to branch script logic.</p>

<p><strong>So, what's the issue with this?</strong></p>

<p>In the Apache OpenWhisk Node.js runtime, action source files are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61-L79">dynamically imported</a> into the runtime environment. The script used to start the Node.js runtime process is for the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js">platform handler</a>, not the action source files. This means the <code>__filename</code> variable does not point to the action source file.</p>

<p>This issue is fixed by separating the serverless function handler and worker thread code into separate files. Worker Threads can be started with a reference to the worker thread script source file, rather than the currently executing script name.</p>

<p><code>javascript
 threads.add(new Worker("./worker.js", { workerData: { start: myStart, range }}));
</code></p>

<h4>How Many Worker Threads?</h4>

<p>The next issue to resolve is how many Worker Threads to use. In order to maximise parallel processing capacity, there should be a Worker Thread for each CPU core. This is the maximum number of threads that can run concurrently.</p>

<p>Node.js provides CPU information for the runtime environment using the <code>os.cpus()</code> <a href="https://nodejs.org/api/os.html#os_os_cpus">function</a>. The result is an array of objects (one per logical CPU core), with model information, processing speed and elapsed processing times. The length of this array will determine number of Worker Threads used. This ensures the number of Worker Threads will always match the CPU cores available.</p>

<p><code>javascript
const threadCount = os.cpus().length
</code></p>

<h3>workers threads version</h3>

<p>Here is the serverless version of the prime number generation algorithm which uses Worker Threads.</p>

<p>The code is split over two files - <code>primes-with-workers.js</code> and <code>worker.js</code>.</p>

<h4>primes-with-workers.js</h4>

<p><a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43">This file</a> contains the serverless function handler used by the platform. Input ranges (based on the <code>min</code> and <code>max</code> action parameters) are divided into chunks, based upon the number of Worker Threads. The handler function creates a Worker Thread for each chunk and waits for the message with the result. Once all the results have been retrieved, it returns all those primes numbers as the invocation result.</p>

<p>```javascript
'use strict';</p>

<p>const { Worker } = require('worker_threads');
const os = require('os')
const threadCount = os.cpus().length</p>

<p>const compute_primes = async (start, range) => {
  return new Promise((resolve, reject) => {</p>

<pre><code>let primes = []
console.log(`adding worker (${start} =&gt; ${start + range})`)
const worker = new Worker('./worker.js', { workerData: { start, range }})

worker.on('error', reject)
worker.on('exit', () =&gt; resolve(primes))
worker.on('message', msg =&gt; {
  primes = primes.concat(msg)
})
</code></pre>

<p>  })
}</p>

<p>async function main(params) {
  const { min, max } = params
  const range = Math.ceil((max - min) / threadCount)
  let start = min &lt; 2 ? 2 : min
  const workers = []</p>

<p>  console.log(<code>Calculating primes with ${threadCount} threads...</code>);</p>

<p>  for (let i = 0; i &lt; threadCount - 1; i++) {</p>

<pre><code>const myStart = start
workers.push(compute_primes(myStart, range))
start += range
</code></pre>

<p>  }</p>

<p>  workers.push(compute_primes(start, max - start))</p>

<p>  const primes = await Promise.all(workers)
  return { primes: primes.flat() }
}</p>

<p>exports.main = main
```</p>

<h4>workers.js</h4>

<p><a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43#file-workers-js">This is the script</a> used in the Worker Thread. The <code>workerData</code> value is used to receive number ranges to search for prime numbers. Primes numbers are sent back to the parent thread using the <code>postMessage</code> function. Since this script is only used in the Worker Thread, it does need to use the <code>isMainThread</code> value to check if it is a child or parent process.</p>

<p>```javascript
'use strict';
const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');</p>

<p>const min = 2</p>

<p>function generatePrimes(start, range) {
  const primes = []
  let isPrime = true;
  let end = start + range;
  for (let i = start; i &lt; end; i++) {</p>

<pre><code>for (let j = min; j &lt; Math.sqrt(end); j++) {
  if (i !== j &amp;&amp; i%j === 0) {
    isPrime = false;
    break;
  }
}
if (isPrime) {
  primes.push(i);
}
isPrime = true;
</code></pre>

<p>  }</p>

<p>  return primes
}</p>

<p>const primes = generatePrimes(workerData.start, workerData.range);
parentPort.postMessage(primes)
```</p>

<h4>package.json</h4>

<p>Source files deployed from a zip file also need to include a <code>package.json</code> file in the archive. The <code>main</code> property is used to determine the script to import as the exported package module.</p>

<p><code>json
{
  "name": "worker_threads",
  "version": "1.0.0",
  "main": "primes-with-workers.js",
}
</code></p>

<h2>Performance Comparison</h2>

<p>Running both functions with the same input parameters allows execution time comparison. The Worker Threads version should improve performance by a factor proportional to available CPU cores. Reducing execution time also means reduced costs in a serverless platform.</p>

<h3>non-workers performance</h3>

<p>Creating a new serverless function (<code>primes</code>) from the non-worker threads source code, using the Node.js v12 runtime, I can test with small values to check correctness.</p>

<p>```sh
$ ibmcloud wsk action create primes primes.js --docker openwhisk/action-nodejs-v12
ok: created action primes
$ ibmcloud wsk action invoke primes --result -p start 2 -p end 10
{</p>

<pre><code>"primes": [ 2, 3, 5, 7 ]
</code></pre>

<p>}
```</p>

<p>Playing with sample input values, 10,000,000 seems like a useful benchmark value. This takes long enough with the single-threaded version to benefit from parallelism.</p>

<p>```sh
$ time ibmcloud wsk action invoke primes --result -p start 2 -p end 10000000 > /dev/null</p>

<p>real    0m35.151s
user    0m0.840s
sys 0m0.315s
```</p>

<p><strong>Using the simple single-threaded algorithm it takes the serverless function around ~35 seconds to calculate primes up to ten million.</strong></p>

<h3>workers threads performance</h3>

<p>Creating a new serverless function, from the worker threads-based source code using the Node.js v12 runtime, allows me to verify it works as expected for small input values.</p>

<p>```
$ ibmcloud wsk action create primes-workers action.zip --docker openwhisk/action-nodejs-v12
ok: created action primes-workers
$ ibmcloud wsk action invoke primes-workers --result -p min 2 -p max 10
{</p>

<pre><code>"primes": [ 2, 3, 5, 7 ]
</code></pre>

<p>}
```</p>

<p>Hurrah, it works.</p>

<p>Invoking the function with an <code>max</code> parameter of 10,000,000 allows us to benchmark against the non-workers version of the code.</p>

<p>```sh
$ time ibmcloud wsk action invoke primes-workers --result -p min 2 -p max 10000000 --result > /dev/null</p>

<p>real    0m8.863s
user    0m0.804s
sys 0m0.302s
```</p>

<p><strong>The workers versions only takes ~25% of the time of the single-threaded version!</strong></p>

<p>This is because IBM Cloud Functions' runtime environments provide access to four CPU cores. Unlike other platforms, CPU cores are not tied to memory allocations. Utilising all available CPU cores concurrently allows the algorithm to run 4x times as fast. Since serverless platforms charge based on execution time, reducing execution time also means reducing costs.</p>

<p><strong>The worker threads version also costs 75% less than the single-threaded version!</strong></p>

<h2>Conclusion</h2>

<p><a href="https://foundation.nodejs.org/announcements/2019/04/24/node-js-foundation-and-js-foundation-merge-to-form-openjs-foundation-2">Node.js v12</a> was released in April 2019. This version included support for <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a>, that were enabled by default (rather than needing an optional runtime flag). Using multiple CPU cores in Node.js applications has never been easier!</p>

<p>Node.js applications with CPU-intensive workloads can utilise this feature to reduce execution time. Since serverless platforms charge based upon execution time, this is especially useful for Node.js serverless functions. Utilising multiple CPU cores leads, not only to improved performance, but also lower bills.</p>

<p>PRs have been <a href="https://github.com/apache/incubator-openwhisk/pull/4472">opened</a> to enable Node.js v12 as a built-in runtime to the Apache OpenWhisk project. This Docker <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v12">image</a> for the new runtime version is already available on Docker Hub. This means it can be used with any Apache OpenWhisk instance straight away!</p>

<p>Playing with Worker Threads on IBM Cloud Functions allowed me to demonstrate how to speed up performance for CPU-intensive workloads by utilising multiple cores concurrently. Using <a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43">an example of prime number generation</a>, calculating all primes up to ten million took ~35 seconds with a single thread and ~8 seconds with four threads. This represents a reduction in execution time and cost of 75%!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Apache OpenWhisk Web Action HTTP Proxy]]></title>
    <link href="http://jamesthom.as/blog/2019/04/29/apache-openwhisk-web-action-http-proxy/"/>
    <updated>2019-04-29T10:29:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/04/29/apache-openwhisk-web-action-http-proxy</id>
    <content type="html"><![CDATA[<p><em>What if you could take an existing web application and run it on a serverless platform with no changes?</em> ü§î</p>

<p>Lots of existing (simple) stateless web applications are perfect candidates for serverless, but use web  frameworks that don't know how to integrate with those platforms. People have started to develop a <a href="https://github.com/IBM/expressjs-openwhisk">number</a> <a href="https://github.com/claudiajs/claudia">of</a> <a href="https://github.com/logandk/serverless-wsgi">custom</a> <a href="https://github.com/Miserlou/Zappa">plugins</a> for those frameworks to try and bridge this gap.</p>

<p>These plugins can provide an easier learning curve for developers new to serverless. They can still use familiar web application frameworks whilst learning about the platforms. It also provides a path to "lift and shift" existing (simple) web applications to serverless platforms.</p>

<p>This approach relies on custom framework plugins being available, for every web app framework and serverless platform, which is not currently the case. <strong>Is there a better solution?</strong></p>

<p>Recently, I've been experimenting with <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk's</a> <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">Docker support</a> to prototype a <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy">different approach</a>. This solution allows any web application to run on the platform, without needing bespoke framework plugins, with minimal changes. <em>Sounds interesting? Read about how I did this below‚Ä¶</em> üëç</p>

<h2>Apache OpenWhisk Web Action HTTP Proxy</h2>

<p><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy">This project</a> provides a static binary which proxies HTTP traffic from <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">Web Actions</a> to existing web applications. <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md#http-context">HTTP events</a> received by the Web Action Proxy are forwarded as HTTP requests to the web application. HTTP responses from the web application are returned as Web Action <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md#handling-http-requests-with-actions">responses</a>.</p>

<p><img src="https://raw.githubusercontent.com/jthomas/openwhisk-web-action-http-proxy/master/web-action-proxy.png" alt="Apache OpenWhisk Web Action HTTP Proxy" /></p>

<p>Both the proxy and web application needed to be started inside the serverless runtime environment. The proxy uses port 8080 and the web application can use any other port. An environment variable or action parameter can be used to configure the local port to proxy.</p>

<p>Running both HTTP processes on the platform is possible due to custom runtime support in Apache OpenWhisk. This allows using <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">custom Docker images</a> as the runtime environment. Custom runtimes images can be built which include the proxy binary and (optionally) the web application source files.</p>

<p>Two different options are available for getting web application source files into the runtime environment.</p>

<ul>
<li><strong>Build source files directly into the container image alongside proxy binary.</strong></li>
<li><strong>Dynamically inject source files into container runtime during initialisation.</strong></li>
</ul>


<p>Building source files into the container is simpler and incurs lower cold-starts delays, but means source code will be publicly available on Docker Hub. Injecting source files through action zips means the public container image can exclude all private source files and secrets. The extra initialisation time for dynamic injection does increase cold-start delays.</p>

<p><em><strong>Please note: This is an alpha-stage experiment!</strong> Don't expect everything to work. This project is designed to run small simple stateless web applications on Apache OpenWhisk. Please don't attempt to "lift 'n' shift" a huge stateful enterprise app server onto the platform!</em></p>

<h3>Node.js + Express Example</h3>

<p>This is an <a href="https://github.com/jthomas/express_example">example Node.js web application</a>, built using the <a href="https://expressjs.com/">Express web application framework</a>:</p>

<p><img src="https://camo.githubusercontent.com/2aa43809d8d8a9f9ccb906c1028d81f1ba1913d9/687474703a2f2f7368617065736865642e636f6d2f696d616765732f61727469636c65732f657870726573735f6578616d706c652e6a7067" alt="https://camo.githubusercontent.com/2aa43809d8d8a9f9ccb906c1028d81f1ba1913d9/687474703a2f2f7368617065736865642e636f6d2f696d616765732f61727469636c65732f657870726573735f6578616d706c652e6a7067" /></p>

<p>The web application renders static HTML content for three routes (<code>/</code>,  <code>/about</code> and <code>/contact</code>). CSS files and fonts are also served by the backend.</p>

<p><strong>Use these steps to run this web application on Apache OpenWhisk using the Web Action Proxy...</strong></p>

<ul>
<li>Clone project repo.</li>
</ul>


<p><code>
git clone https://github.com/jthomas/express_example
</code></p>

<ul>
<li>Install project dependencies in the <code>express_example</code> directory.</li>
</ul>


<p><code>
npm install
</code></p>

<ul>
<li>Bundle web application and libraries into zip file.</li>
</ul>


<p><code>
zip -r action.zip *
</code></p>

<ul>
<li>Create the Web Action (using a custom runtime image) with the following command.</li>
</ul>


<p><code>
wsk action create --docker jamesthomas/generic_node_proxy --web true --main "npm start" -p "__ow_proxy_port" 3000 web_app action.zip
</code></p>

<ul>
<li>Retrieve the Web Action URL for the new action.</li>
</ul>


<p><code>
wsk action get web_app --url
</code></p>

<ul>
<li>Open the Web Action URL in a HTTP web browser. <em>(Note: Web Action URLs must end with a forward-slash to work correctly, e.g. <code>https://&lt;OW_HOST&gt;/api/v1/web/&lt;NAMESPACE&gt;/default/web_app/</code>).</em></li>
</ul>


<p><img src="/images/express-js-web-action-proxy.gif" alt="Web Action Proxy Express JS" /></p>

<p>If this works, the web application should load as above. Clicking links in the menu will navigate to different pages in the application.</p>

<h4>custom runtime image</h4>

<p>This example Web Action uses my own pre-built custom runtime image for Node.js web applications (<code>jamesthomas/generic_node_proxy</code>). This was created from the following Dockerfile to support dynamic runtime injection of web application source files.</p>

<p>```
FROM node:10</p>

<p>ADD proxy /app/
WORKDIR /app
EXPOSE 8080</p>

<p>CMD ./proxy
```</p>

<h3>More Examples</h3>

<p>See the <code>examples</code> directory in the project repository for sample applications with build instructions for the following runtimes.</p>

<ul>
<li><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/examples/nodejs+express">Node.js with Express.js</a></li>
<li><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/examples/python+flask">Python with Flask</a></li>
<li><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/examples/java+openliberty">Java with OpenLiberty</a></li>
</ul>


<h3>Usage &amp; Configuration</h3>

<p>Web application source files can be either be <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy#usage-dynamic-runtime-injection">dynamically injected</a> (as in the example above) or <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy#usage-sources-files-in-image">built into</a> the custom runtime image.</p>

<p><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#usage-dynamic-runtime-injection">Dynamic injection</a> uses a custom runtime image with just the proxy binary and runtime dependencies. Web application source files are provided in the action zip file and extracted into the runtime upon initialisation. The proxy will start the app server during cold-starts.</p>

<p>Alternatively, source files for the web application <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#usage-sources-files-in-image">can be included directly</a> in the runtime image. The container start command will start both processes concurrently. No additional files are provided when creating the web action.</p>

<p><a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#configuration">Configuration</a> for values such as the proxy port, can be provided using <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#environment-variables">environment variables</a> or <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#action-parameters">default action parameters</a>.</p>

<p><em>Please see the <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md">project documentation</a> for more details on both these approaches, how to use them and configuration parameters.</em></p>

<h2>Challenges</h2>

<p>This experiment is still in the alpha-stage and comes with many restrictions at the moment...</p>

<ul>
<li>HTTP request and responses sizes are limited to the maximum sizes allowed by Apache OpenWhisk for input parameters and activation results. This defaults to 1MB in the open-source project and 5MB on IBM Cloud Functions.</li>
<li>Page links must use URLs with relative paths to the Web Action URL rather than the host root, e.g. <code>href="home"</code> rather than <code>href="/home"</code>. This is due to the Web Actions being served from a sub-path of the platform (<code>/api/v1/web/&lt;NAMESPACE&gt;/default/&lt;ACTION&gt;</code>) rather than the host root.</li>
<li>Docker images will be pulled from the public registry on the first invocation. This will lead to long cold-start times for the first request after the action has been created. Large image sizes = longer delays. This only occurs on the first invocation.</li>
<li>Web app startup times affect cold start times. The proxy blocks waiting for the web application to start before responding. This delay is included in each cold start. Concurrent HTTP requests from a web  browser for static page assets will (initially) result in multiple cold starts.</li>
<li>Web Sockets and other complex HTTP features, e.g. server-side events, cannot be supported.</li>
<li>Web applications will run in ephemeral container environments that  are paused between requests and destroyed without warning. This is not a traditional web application environment, e.g. running background tasks will not work.</li>
</ul>


<p>Lots of things haven't been tested. Don't expect complex stateful web applications to work.</p>

<h2>Conclusion</h2>

<p>Being able to run existing web applications on serverless platforms opens up a huge opportunity for moving simple (and stateless) web application over to those platforms. These applications can then benefit from the scaling, cost and operational benefits serverless platforms provide.</p>

<p>Previous attempts to support traditional web applications on serverless platforms relied on custom framework plugins. This approach was limited by the availability of custom plugins for each web application framework and serverless platform.</p>

<p>Playing around with Apache OpenWhisk's custom runtime support, I had an idea‚Ä¶ <strong>could a generic HTTP proxy be used to support any framework without needing any plugins?</strong> This led to the <a href="https://github.com/jthomas/openwhisk-web-action-http-proxy/blob/master/README.md#usage-sources-files-in-image">Apache OpenWhisk Web Action HTTP Proxy</a> project.</p>

<p>By building a custom runtime, the HTTP proxy and web application can both be started within the same serverless environment. HTTP events received by the Web Action Proxy are forwarded as HTTP requests to the web application. HTTP responses from the web application are returned as Web Action responses.</p>

<p>Web application sources files can be injected into the runtime environment during initialisation or built straight into the custom runtime image. No significant changes are required in the web application and it does not need custom framework plugins.</p>

<p>Apache OpenWhisk's support for custom Docker runtimes opens up a huge range of opportunities for running more varied workloads on serverless platforms - and this is a great example of that!</p>
]]></content>
  </entry>
  
</feed>
