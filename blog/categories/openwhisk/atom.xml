<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: openwhisk | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/openwhisk/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2019-04-26T16:34:37+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Serverless CI/CD with Travis CI, Serverless Framework and IBM Cloud Functions]]></title>
    <link href="http://jamesthom.as/blog/2019/04/23/serverless-ci-slash-cd-with-travis-ci-serverless-framework-and-ibm-cloud-functions/"/>
    <updated>2019-04-23T15:08:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/04/23/serverless-ci-slash-cd-with-travis-ci-serverless-framework-and-ibm-cloud-functions</id>
    <content type="html"><![CDATA[<p>How do you set up a <a href="https://dzone.com/articles/what-is-cicd">CI/CD pipeline</a> for serverless applications?</p>

<p>This blog post will explain how to use <a href="https://travis-ci.org/">Travis CI</a>, <a href="https://github.com/serverless/serverless">The Serverless Framework</a> and the <a href="https://github.com/avajs/ava">AVA testing framework</a> to set up a fully-automated build, deploy and test pipeline for a serverless application. It will use a real example of a production <a href="https://github.com/jthomas/openwhisk-release-verification">serverless application</a>, built using <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> and running on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>. The CI/CD pipeline will execute the following tasks...</p>

<ul>
<li><strong>Run project unit tests.</strong></li>
<li><strong>Deploy application to test environment.</strong></li>
<li><strong>Run acceptance tests against test environment.</strong></li>
<li><strong>Deploy application to production environment.</strong></li>
<li><strong>Run smoke tests against production environment.</strong></li>
</ul>


<p>Before diving into the details of the CI/CD pipeline setup, let's start by showing the example serverless application being used for this project...</p>

<h2>Serverless Project - <a href="http://apache.jamesthom.as/">http://apache.jamesthom.as/</a></h2>

<p>The "<a href="https://github.com/jthomas/openwhisk-release-verification">Apache OpenWhisk Release Verification</a>" project is a serverless web application to help committers verify release candidates for the open-source project. It automates running the verification steps from the <a href="https://cwiki.apache.org/confluence/display/OPENWHISK/How+to+verify+the+release+checklist+and+vote+on+OpenWhisk+modules+under+Apache">ASF release checklist</a> using serverless functions. Automating release candidate validation makes it easier for  committers to participate in release voting.</p>

<p><img src="https://raw.githubusercontent.com/jthomas/openwhisk-release-verification/master/release-verification-tool.gif" alt="Apache OpenWhisk Release Verification Tool" /></p>

<p>The project consists of a static web assets (HTML, JS, CSS files) and HTTP APIs. Static web assets are hosted by Github Pages from the <a href="https://github.com/jthomas/openwhisk-release-verification">project repository</a>. HTTP APIs are implemented as Apache OpenWhisk <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md">actions</a> and exposed using the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/apigateway.md">API Gateway</a> service. <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> is used to host the Apache OpenWhisk application.</p>

<p>No other cloud services, like databases, are needed by the backend. Release candidate information is retrieved in real-time by parsing the <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/">HTML page</a> from the ASF website.</p>

<p><img src="http://jamesthom.as/images/ow_release_verifier/architecture.png" alt="Serverless Architecture" /></p>

<h3>Configuration</h3>

<p><a href="https://github.com/serverless/serverless">The Serverless Framework</a> (with the <a href="https://github.com/serverless/serverless-openwhisk">Apache OpenWhisk provider plugin</a>) is used to define the serverless functions used in the application. HTTP endpoints are also defined in the YAML configuration file.</p>

<p>```yaml
service: release-verfication</p>

<p>provider:
  name: openwhisk
  runtime: nodejs:10</p>

<p>functions:
  versions:</p>

<pre><code>handler: index.versions
events:
  - http: GET /api/versions
</code></pre>

<p>  version_files:</p>

<pre><code>handler: index.version_files
events:
  - http:
      method: GET
      path: /api/versions/{version}
      resp: http
</code></pre>

<p>...</p>

<p>plugins:
  - serverless-openwhisk
```</p>

<p>The framework handles all deployment and configuration tasks for the application. Setting up the application in a new environment is as simple as running the <code>serverless deploy</code> <a href="https://github.com/serverless/serverless">command</a>.</p>

<h3>Environments</h3>

<p>Apache OpenWhisk uses <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#fully-qualified-names">namespaces</a> to group individual packages, actions, triggers and rules. Different namespaces can be used to provide isolated environments for applications.</p>

<p>IBM Cloud Functions automatically creates <a href="https://cloud.ibm.com/docs/openwhisk?topic=cloud-functions-cloudfunctions_cli#region_info">user-based namespaces</a> in platform instances. These auto-generated namespaces mirror the IBM Cloud organisation and space used to access the instance. Creating <a href="https://cloud.ibm.com/docs/account?topic=account-orgsspacesusers#cf-org-concepts">new spaces within an organisation</a> will provision extra namespaces.</p>

<p>I'm using a custom organisation for the application with three different spaces: <strong>dev</strong>, <strong>test</strong> and <strong>prod</strong>.</p>

<p><strong>dev</strong> is used as a test environment to deploy functions during development. <strong>test</strong> is used by the CI/CD pipeline to deploy a temporary instance of the application during acceptance tests. <strong>prod</strong> is the production environment hosting the external application actions.</p>

<h3>Credentials</h3>

<p>The <a href="https://cloud.ibm.com/docs/cli?topic=cloud-cli-install-ibmcloud-cli">IBM Cloud CLI</a> is used to handle IBM Cloud Functions credentials. <a href="https://cloud.ibm.com/docs/iam?topic=iam-manapikey">Platform API keys</a> will be used to log in the CLI from the CI/CD system.</p>

<p>When Cloud Functions CLI commands are issued (after targeting a new region, organisation or space), API keys for that Cloud Functions instance are automatically retrieved and stored locally. The Serverless Framework knows how to use these local credentials when interacting with the platform.</p>

<h3>High Availability?</h3>

<p>The Apache OpenWhisk Release Verifier is not a critical cloud application which needs "<a href="https://en.wikipedia.org/wiki/Five_nines">five nines</a>" of availability. The application is idle most of the time. It does not need a <a href="https://en.wikipedia.org/wiki/High_availability">highly available</a> serverless architecture. This means the build pipeline does not have to...</p>

<ul>
<li><a href="https://cloud.ibm.com/docs/tutorials?topic=solution-tutorials-multi-region-serverless#multi-region-serverless">Deploy application instances in multiple cloud regions.</a></li>
<li><a href="https://www.ibm.com/blogs/bluemix/2019/04/load-balancing-api-calls-across-regions-with-ibm-cloud-internet-services-and-cloud-api-gateway/">Set up a global load balancer between regional instances.</a></li>
<li>Support "<a href="https://www.martinfowler.com/bliki/BlueGreenDeployment.html">zero downtime deploys</a>" to minimise downtime during deployments.</li>
<li>Automatic roll-back to previous versions on production issues.</li>
</ul>


<p>New deployments will simply overwrite resources in the production namespace in a single region. If the production site is broken after a deployment, the smoke tests should catch this and email me to fix it!</p>

<h2>Testing</h2>

<p>Given this tool will be used to check release candidates for the open-source project, I wanted to ensure it worked properly! Incorrect validation results could lead to invalid source archives being published.</p>

<p>I've chosen to rely heavily on unit tests to check the core business logic. These tests ensure all validation tasks work correctly, including PGP signature verification, cryptographic hash matching, LICENSE file contents and other ASF requirements for project releases.</p>

<p>Additionally, I've used end-to-end acceptance tests to validate the HTTP APIs work as expected. HTTP requests are sent to the API GW endpoints, with responses compared against expected values. All available release candidates are run through the validation process to check no errors are returned.</p>

<h3>Unit Tests</h3>

<p><a href="https://en.wikipedia.org/wiki/Unit_testing">Unit tests</a> are implemented with the <a href="https://github.com/avajs/ava">AVA testing framework</a>. Unit tests live in the <code>unit/test/</code> <a href="https://github.com/jthomas/openwhisk-release-verification/tree/master/test/unit">folder</a>.</p>

<p>The <code>npm test</code> command alias runs the <code>ava test/unit/</code> command to execute all unit tests. This command can be executed locally, during development, or from the CI/CD pipeline.</p>

<p>```bash
$ npm test</p>

<blockquote><p>release-verification@1.0.0 test ~/code/release-verification
ava test/unit/</p></blockquote>

<p> 27 tests passed
```</p>

<h3>Acceptance Tests</h3>

<p><a href="https://en.wikipedia.org/wiki/Acceptance_testing">Acceptance tests</a> check API endpoints return the expected responses for valid (and invalid) requests. Acceptance tests are executed against the API Gateway endpoints for an application instance.</p>

<p>The hostname used for HTTP requests is controlled using an environment variable (<code>HOST</code>). Since the same test suite test is used for acceptance and smoke tests, setting this environment variable is the only configuration needed to run tests against different environments.</p>

<p>API endpoints in the test and production environments are exposed using different custom sub-domains (<code>apache-api.jamesthom.as</code> and <code>apache-api-test.jamesthom.as</code>). NPM <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/package.json#L8-L9">scripts are used</a> to provide commands (<code>acceptance-test</code> &amp; <code>acceptance-prod</code>) which set the environment hostname before running the test suite.</p>

<p>```javascript
"scripts": {</p>

<pre><code>"acceptance-test": "HOST=apache-api-test.jamesthom.as ava -v --fail-fast test/acceptance/",
"acceptance-prod": "HOST=apache-api.jamesthom.as ava -v --fail-fast test/acceptance/"
</code></pre>

<p>  },
```</p>

<p>```
$ npm run acceptance-prod</p>

<blockquote><p>release-verification@1.0.0 acceptance-prod ~/code/release-verification
HOST=apache-api.jamesthom.as ava -v --fail-fast  test/acceptance/</p></blockquote>

<p>  ‚úî should return list of release candidates (3.7s)</p>

<pre><code>‚Ñπ running api testing against https://apache-api.jamesthom.as/api/versions
</code></pre>

<p>  ‚úî should return 404 for file list when release candidate is invalid (2.1s)</p>

<pre><code>‚Ñπ running api testing against https://apache-api.jamesthom.as/api/versions/unknown
</code></pre>

<p>  ...</p>

<p>  6 tests passed
```</p>

<p>Acceptance tests are also implemented with the AVA testing framework. All acceptance tests live in a single <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/test/acceptance/api.js">test file</a> (<code>unit/acceptance/api.js</code>).</p>

<h2>CI/CD Pipeline</h2>

<p>When new commits are pushed to the <code>master</code> branch on the project repository, the following steps needed to be kicked off by the build pipeline‚Ä¶</p>

<ul>
<li><em>Run project unit tests.</em></li>
<li><em>Deploy application to test environment.</em></li>
<li><em>Run acceptance tests against test environment.</em></li>
<li><em>Deploy application to production environment.</em></li>
<li><em>Run smoke tests against production environment.</em></li>
</ul>


<p>If any of the steps fail, the build pipeline should stop and send me a notification email.</p>

<h3>Travis</h3>

<p><a href="https://travis-ci.org/">Travis CI</a> is used to implement the CI/CD build pipeline. Travis CI uses a <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/.travis.yml">custom file</a> (<code>.travis.yml</code>) in the project repository to configure the build pipeline. This YAML file defines commands to execute during each phase of build pipeline. If any of the commands fail, the build will stop at that phase without proceeding.</p>

<p><em>Here is the completed <code>.travis.yml</code> file for this project: https://github.com/jthomas/openwhisk-release-verification/blob/master/.travis.yml</em></p>

<p>I'm using the following Travis CI <a href="https://docs.travis-ci.com/user/job-lifecycle#the-job-lifecycle">build phases</a> to implement the pipeline: <strong>install</strong>, <strong>before_script</strong>, <strong>script</strong>, <strong>before_deploy</strong> and <strong>deploy</strong>. Commands will run in the Node.js 10 build environment, which pre-installs the language runtime and package manager.</p>

<p><code>yaml
language: node_js
node_js:
  - "10"
</code></p>

<h4>install</h4>

<p>In the <code>install</code> <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/.travis.yml#L5-L9">phase</a>, I need to set up the build environment to deploy the application and run tests.</p>

<p>This means installing the IBM Cloud CLI, <a href="https://cloud.ibm.com/openwhisk/learn/cli">Cloud Functions CLI plugin</a>, The Serverless Framework (with Apache OpenWhisk plugin), application test framework (AvaJS) and other project dependencies.</p>

<p>The IBM Cloud CLI is installed using a shell script. Running a CLI sub-command installs the <a href="https://cloud.ibm.com/openwhisk/learn/cli">Cloud Functions plugin</a>.</p>

<p>The Serverless Framework is installed as global NPM package (using <code>npm -g install</code>). The Apache OpenWhisk provider plugin is handled as <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/package.json#L13-L23">normal project dependency</a>, along with the test framework. Both those dependencies are installed using NPM.</p>

<p><code>yaml
install:
  - curl -fsSL https://clis.cloud.ibm.com/install/linux | sh
  - ibmcloud plugin install cloud-functions
  - npm install serverless -g
  - npm install
</code></p>

<h4>before_script</h4>

<p>This <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/.travis.yml#L11-L16">phase</a> is used to run unit tests, catching errors in core business logic, before setting up credentials (used in the <code>script</code> phase) for the acceptance test environment. Unit test failures will halt the build immediately, skipping test and production deployments.</p>

<p>Custom variables provide the API key, platform endpoint, organisation and space identifiers which are used for the test environment. The CLI is authenticated using these values, before running the <code>ibmcloud fn api list</code> command. This ensures Cloud Functions credentials are available locally, as used by The Serverless Framework.</p>

<p><code>yaml
before_script:
  - npm test
  - ibmcloud login --apikey $IBMCLOUD_API_KEY -a $IBMCLOUD_API_ENDPOINT
  - ibmcloud target -o $IBMCLOUD_ORG -s $IBMCLOUD_TEST_SPACE
  - ibmcloud fn api list &gt; /dev/null
  - ibmcloud target
</code></p>

<h4>script</h4>

<p>With the build system configured, the application can be deployed to test environment, followed by running acceptance tests. If either deployment or acceptance tests fail, the build will stop, skipping the production deployment.</p>

<p>Acceptance tests use an environment variable to configure the hostname test cases are executed against. The <code>npm run acceptance-test</code> alias command sets this value to the test environment hostname (<code>apache-api-test.jamesthom.as</code>) before running the test suite.</p>

<p><code>yaml
script:
  - sls deploy
  - npm run acceptance-test
</code></p>

<h4>before_deploy</h4>

<p>Before deploying to production, Cloud Functions credentials need to be updated. The IBM Cloud CLI is used to target the production environment, before running a Cloud Functions CLI command. This updates local credentials with the production environment credentials.</p>

<p><code>yaml
before_deploy:
  - ibmcloud target -s $IBMCLOUD_PROD_SPACE
  - ibmcloud fn api list &gt; /dev/null
  - ibmcloud target
</code></p>

<h4>deploy</h4>

<p>If all the proceeding stages have successfully finished, the application can be deployed to the production. Following this final deployment, smoke tests are used to check production APIs still work as expected.</p>

<p>Smoke tests are just the same acceptance tests executed against the production environment. The <code>npm run acceptance-prod</code> alias command sets the hostname configuration value to the production environment  (<code>apache-api.jamesthom.as</code>) before running the test suite.</p>

<p><code>yaml
deploy:
  provider: script
  script: sls deploy &amp;&amp; npm run acceptance-prod
  skip_cleanup: true
</code></p>

<p><em>Using the <code>skip_cleanup</code> parameter leaves installed artifacts from previous phases in the build environment. This means we don't have to re-install the IBM Cloud CLI, The Serverless Framework or NPM dependencies needed to run the production deployment and smoke tests.</em></p>

<h3>success?</h3>

<p>If all of the <a href="https://travis-ci.org/jthomas/openwhisk-release-verification">build phases</a> are successful, the latest project code should have been deployed to the production environment. üíØüíØüíØ</p>

<p><img src="/images/build-screenshot.png" alt="Build Screenshoot" /></p>

<p>If the build failed due to unit test failures, the test suite can be ran locally to fix any errors. Deployment failures can be investigated using the console output logs from Travis CI. Acceptance test issues, against test or production environments, can be debugged by logging into those environments locally and running the test suite from my development machine.</p>

<h2>Conclusion</h2>

<p>Using Travis CI with The Serverless Framework and a JavaScript testing framework, I was able to set up a fully-automated CI/CD deployment pipeline for the Apache OpenWhisk release candidate verification tool.</p>

<p>Using a CI/CD pipeline, rather than a manual approach, for deployments has the following advantages...</p>

<ul>
<li>No more manual and error-prone deploys relying on a human üë®‚Äçüíª :)</li>
<li>Automatic unit &amp; acceptance test execution catch errors before deployments.</li>
<li>Production environment only accessed by CI/CD system, reducing accidental breakages.</li>
<li>All cloud resources must be configured in code. No "<a href="https://martinfowler.com/bliki/SnowflakeServer.html">snowflake</a>" environments allowed.</li>
</ul>


<p>Having finished code for new project features or bug fixes, all I have to do is push changes to the GitHub repository. This fires the Travis CI build pipeline which will automatically deploy the updated application to the production environment. If there are any issues, due to failed tests or deployments, I'll be notified by email.</p>

<p>This allows me to get back to adding new features to the tool (and fixing bugs) rather than wrestling with deployments, managing credentials for multiple environments and then trying to remember to run tests against the correct instances!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Automating Apache OpenWhisk Releases With Serverless]]></title>
    <link href="http://jamesthom.as/blog/2019/04/10/automating-apache-openwhisk-releases-with-serverless/"/>
    <updated>2019-04-10T15:00:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/04/10/automating-apache-openwhisk-releases-with-serverless</id>
    <content type="html"><![CDATA[<p>This blog post explains how I used <a href="https://github.com/jthomas/openwhisk-release-verification">serverless functions</a> to automate <a href="https://cwiki.apache.org/confluence/display/OPENWHISK/How+to+verify+the+release+checklist+and+vote+on+OpenWhisk+modules+under+Apache">release candidate verification</a> for the <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> project.</p>

<p><img src="https://raw.githubusercontent.com/jthomas/openwhisk-release-verification/master/release-verification-tool.gif" alt="Apache OpenWhisk Release Verification Tool" /></p>

<p><em>Automating this process has the following benefits...</em></p>

<ul>
<li><strong>Removes the chance of human errors compared to the previously manual validation process.</strong></li>
<li><strong>Allows me to validate new releases without access to my dev machine.</strong></li>
<li><strong>Usable by all committers by hosting as an <a href="http://apache.jamesthom.as/">external serverless web app</a>.</strong></li>
</ul>


<p>Automating release candidate validation makes it easier for project committers to participate in release voting. This should make it faster to get necessary release votes, allowing us to ship new versions sooner!</p>

<h2>background</h2>

<h3>apache software foundation</h3>

<p>The <a href="http://www.apache.org/">Apache Software Foundation</a> has a well-established <a href="https://www.apache.org/dev/release-publishing.html">release process</a> for delivering new product releases from projects belonging to the foundation. According to their <a href="https://www.apache.org/dev/release-publishing.html#goal">documentation</a>...</p>

<blockquote><p>An Apache release is a set of valid &amp; signed artifacts, voted on by the appropriate PMC and distributed on the ASF's official release infrastructure.</p>

<p>https://www.apache.org/dev/release-publishing.html</p></blockquote>

<p>Releasing a new software version requires the release manager to create a release candidate from the  project source files. Source archives must be cryptographically <a href="http://www.apache.org/legal/release-policy.html#release-signing">signed</a> by the release manager. All source archives for the release must be comply with <a href="http://www.apache.org/legal/release-policy.html">strict criteria</a> to be considered valid release candidates. This includes (but is not limited to) the following requirements:</p>

<ul>
<li><em>Checksums and PGP signatures for source archives are valid.</em></li>
<li><em>LICENSE, NOTICE and DISCLAIMER files included and correct.</em></li>
<li><em>All source files have license headers.</em></li>
<li><em>No compiled archives bundled in source archives.</em></li>
</ul>


<p>Release candidates can then be proposed on the project mailing list for review by members of the <a href="https://apache.org/dev/pmc.html">Project Management Committee</a> (PMC). PMC members are <a href="http://www.apache.org/legal/release-policy.html#release-approval">eligible to vote</a> on all release candidates. Before casting their votes, PMC members are required to check release candidate meets the requirements above.</p>

<p><strong>If a minimum of three positive votes is cast (with more positive than negative votes), the release passes!</strong> The release manager can then move the release candidate archives to the release <a href="https://dist.apache.org/repos/dist/release/incubator/openwhisk/">directory</a>.</p>

<h3>apache openwhisk releases</h3>

<p>As a committer and PMC member on the Apache OpenWhisk project, I'm eligible to vote on new releases.</p>

<p>Apache OpenWhisk (currently) has 52 separate <a href="https://github.com/apache?q=openwhisk">source repositories</a> under the project on GitHub. With a fast-moving open-source project, new releases candidate are constantly <a href="https://lists.apache.org/list.html?dev@openwhisk.apache.org:lte=3y:%5BVOTE%5D">being proposed</a>, which all require the necessary number of binding PMC votes to pass.</p>

<p>Manually validating release candidates can be a time-consuming process. This can make it challenging to get a quorum of binding votes from PMC members for the release to pass. I started thinking how I could improve my productivity around the validation process, enabling me to participate in more votes.</p>

<p><strong>Would it be possible to automate some (or all) of the steps in release candidate verification? Could we even use a serverless application to do this?</strong></p>

<h1>apache openwhisk release verifier</h1>

<p><strong>Spoiler Alert: YES! I ended up building a serverless application to do this for me.</strong></p>

<p>It is available at <a href="https://apache.jamesthom.as/">https://apache.jamesthom.as/</a></p>

<p><img src="/images/ow_release_verifier/overview.png" alt="Apache OpenWhisk Release Verifier" /></p>

<p>Source code for this project is available <a href="https://github.com/jthomas/openwhisk-release-verification">here</a>.</p>

<p><a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> is used to run the serverless backend for the web application. This means Apache OpenWhisk is being used to validate future releases of itself‚Ä¶ which is awesome.</p>

<h2>architecture</h2>

<p><img src="/images/ow_release_verifier/architecture.png" alt="Project Architecture" /></p>

<p>HTML, JS and CSS files are served by Github Pages from the <a href="https://github.com/jthomas/openwhisk-release-verification">project repository</a>.</p>

<p>Backend APIs are Apache OpenWhisk actions running on <a href="http://cloud.ibm.com/openwhisk">IBM Cloud Functions</a>.</p>

<p>Both the front-page and API are served from a custom sub-domains of my <a href="http://jamesthom.as/">personal domain</a>.</p>

<h3>available release candidates</h3>

<p>When the user loads the page, the drop-down list needs to contain the current list of release candidates from the ASF development <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/">distribution site</a>.</p>

<p>This information is available to the web page via the <a href="https://apache-api.jamesthom.as/api/versions">https://apache-api.jamesthom.as/api/versions</a> endpoint. The <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/">serverless function</a> powering this API parses that <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/">live HTML page</a> (extracting the current list of release candidates) each time it is invoked.</p>

<p>```sh
$ http get https://apache-api.jamesthom.as/api/versions
HTTP/1.1 200 OK
...
{</p>

<pre><code>"versions": [
    "apache-openwhisk-0.11.0-incubating-rc1",
    "apache-openwhisk-0.11.0-incubating-rc2",
    "apache-openwhisk-1.13.0-incubating-rc1",
    "apache-openwhisk-1.13.0-incubating-rc2",
    "apache-openwhisk-2.0.0-incubating-rc2",
    "apache-openwhisk-3.19.0-incubating-rc1"
]
</code></pre>

<p>}
```</p>

<h3>release candidate version info</h3>

<p>Release candidates may have <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/apache-openwhisk-2.0.0-incubating-rc2/">multiple source archives</a> being distributed in that release. Validation steps need to be executed for each of those archives within the release candidate.</p>

<p>Once a user has selected a release candidate version, source archives to validate are shown in the table. This data is available from the <a href="https://apache-api.jamesthom.as/api/versions/VERSION">https://apache-api.jamesthom.as/api/versions/VERSION</a> endpoint. This information is parsed from the <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/apache-openwhisk-2.0.0-incubating-rc2/">HTML page</a> on the ASF site.</p>

<p>```sh
$ http get https://apache-api.jamesthom.as/api/versions/apache-openwhisk-2.0.0-incubating-rc2
HTTP/1.1 200 OK
...</p>

<p>{</p>

<pre><code>"files": [
    "openwhisk-package-alarms-2.0.0-incubating-sources.tar.gz",
    "openwhisk-package-cloudant-2.0.0-incubating-sources.tar.gz",
    "openwhisk-package-kafka-2.0.0-incubating-sources.tar.gz"
]
</code></pre>

<p>}
```</p>

<h3>release verification</h3>

<p>Having selected a release candidate version, clicking the "<em>Validate</em>" button will start validation process. Triggering the <a href="https://apache-api.jamesthom.as/api/versions/VERSION/validate">https://apache-api.jamesthom.as/api/versions/VERSION/validate</a> endpoint will run the <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/index.js#L47-L64">serverless function</a> used to execute the validation steps.</p>

<p><em>This serverless function will carry out the following verification steps...</em></p>

<h4>checking download links</h4>

<p>All the source archives for a release candidate are downloaded to temporary storage in the runtime environment. The function also downloads the associated SHA512 and PGP signature files for comparison. Multiple readable streams can be created from the same file path to allow the verification steps to happen in parallel, rather than having to re-download the archive for each task.</p>

<h4>checking SHA512 hash values</h4>

<p>SHA512 sums are distributed in a text file containing hex strings with the hash value.</p>

<p><code>sh
openwhisk-package-alarms-2.0.0-incubating-sources.tar.gz:
3BF87306 D424955B B1B2813C 204CC086 6D27FA11 075F0B30 75F67782 5A0198F8 091E7D07
 B7357A54 A72B2552 E9F8D097 50090E9F A0C7DBD1 D4424B05 B59EE44E
</code></p>

<p>The serverless function needs to dynamically compute the hash for the source archive and compare the hex bytes against the text file contents. Node.js comes with a <a href="https://nodejs.org/docs/latest-v10.x/api/crypto.html">built-in crypto library</a> making it easy to create hash values from input streams.</p>

<p><em>This is the <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/lib/verify.js#L18-L35">function</a> used to compute and compare the hash values.</em></p>

<p>```javascript
const hash = async (file_stream, hash_file, name) => {
  return new Promise((resolve, reject) => {</p>

<pre><code>const sha512 = parse_hash_from_file(hash_file)

const hmac = crypto.createHash('sha512')
file_stream.pipe(hmac)

hmac.on('readable', () =&gt; {
  const stream_hash = hmac.read().toString('hex')
  const valid = stream_hash === sha512.signature
  logger.log(`file (${name}) calculated hash: ${stream_hash}`)
  logger.log(`file (${name}) hash from file:  ${sha512.signature}`)
  resolve({valid})
})

hmac.on('error', err =&gt; reject(err))
</code></pre>

<p>  })
}
```</p>

<h4>validating PGP signatures</h4>

<p><strong>Node.js' crypto library does not support validating PGP signatures.</strong></p>

<p>I've used the <a href="https://www.npmjs.com/package/openpgp">OpenPGP.js library</a> to handle this task. This is a Javascript implementation of the OpenPGP protocol (and the most popular PGP library for Node.js). Three input values are needed to <a href="https://github.com/openpgpjs/openpgpjs#create-and-verify-detached-signatures">validate PGP messages</a>.</p>

<ul>
<li><em>Message contents to check.</em></li>
<li><em>PGP signature for the message.</em></li>
<li><em>Public key for the private key used to sign the release.</em></li>
</ul>


<p>The "message" to check is the source archive. PGP signatures come from the <code>.asc</code> files located in the release candidate directory.</p>

<p>```
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1</p>

<p>iQIcBAABAgAGBQJcpO0FAAoJEHKvDMIsTPMgf0kP+wbtJ1ONZJQKjyDVx8uASMDQ
...
-----END PGP SIGNATURE-----
```</p>

<p>Public keys used to sign releases are <a href="https://dist.apache.org/repos/dist/dev/incubator/openwhisk/KEYS">stored in the root folder</a> of the release directory for that project.</p>

<p><em>This <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/lib/verify.js#L37-L58">function</a> is used to implement the signature checking process.</em></p>

<p>```javascript
const signature = async (file_stream, signature, public_keys, name) => {
  const options = {</p>

<pre><code>message: openpgp.message.fromBinary(file_stream),
signature: await openpgp.signature.readArmored(signature),
publicKeys: (await openpgp.key.readArmored(public_keys)).keys
</code></pre>

<p>  }</p>

<p>  const verified = await openpgp.verify(options)
  await openpgp.stream.readToEnd(verified.data)
  const valid = await verified.signatures[0].verified</p>

<p>  return { valid }
}
```</p>

<h4>scanning archive files</h4>

<p>Using the <a href="https://github.com/npm/node-tar">node-tar library</a>, downloaded source archives are extracted into the local runtime to allow scanning of individual files.</p>

<p>LICENSE.txt, DISCLAIMER.txt and NOTICE.txt files are checked to ensure correctness. An <a href="https://www.npmjs.com/package/isbinaryfile">external NPM library</a> is used to check all files in the archive for binary contents. The code also scans for directory names that might contain third party libraries (<code>node_modules</code> or <code>.gradle</code>).</p>

<h3>capturing validation logs</h3>

<p>It is important to provide PMC members with verifiable logs on the validation steps performed. This allows them to sanity check the steps performed (including manual validation). This verification text can also be provided in the voting emails as evidence of release candidate validity.</p>

<p>Using a <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/lib/logger.js">custom logging library</a>, all debug logs sent to the console <a href="https://github.com/jthomas/openwhisk-release-verification/blob/master/index.js#L63">are recorded in the action result</a> (and therefore returned in the API response).</p>

<h3>showing results</h3>

<p>Once all the validation tasks have been executed - the results are returned to the front-end as a JSON response. The client-side JS parses these results and updates the validation table. Validation logs are shown in a collapsible window.</p>

<p><img src="/images/ow_release_verifier/emojis.png" alt="Verification Results" /></p>

<p>Using visual emojis for pass and failure indicators for each step - the user can easily verify whether a release passes the validation checks. If any of the steps have failed, the validation logs provide an opportunity to understand why.</p>

<p><img src="/images/ow_release_verifier/logs.png" alt="Verification Logs" /></p>

<h2>other tools</h2>

<p>This is not the only tool that can automate checks needed to validate Apache Software Foundation releases.</p>

<p>Another <a href="https://twitter.com/rabbah">community member</a> has also built a bash script (<a href="https://gitbox.apache.org/repos/asf?p=incubator-openwhisk-release.git;a=blob_plain;f=tools/rcverify.sh;hb=HEAD">rcverify.sh</a>) that can verify releases on your local machine. This script will automatically download the release candidate files and run many of the same validation tasks as the remote tool locally.</p>

<p>There is also an existing tool (<a href="https://creadur.apache.org/rat/">Apache Rat</a>) from another project that provides a Java-based application for auditing license headers in source files.</p>

<h2>conclusion</h2>

<p>Getting new product releases published for an open-source project under the ASF is not a simple task for developers used to pushing a button on Github! The ASF has a series of strict guidelines on what constitutes a release and the ratification process from PMC members. PMC members need to run a series of manual verification tasks before casting binding votes on proposed release candidates.</p>

<p>This can be a time-consuming task for PMC members on a project like Apache OpenWhisk, with 52 different project repositories all being released at different intervals. In an effort to improve my own productivity around this process, I started looking for ways to automate the verification tasks. This would enable me to participate in more votes and be a "better" PMC member.</p>

<p>This led to building a serverless web application to run all the verification tasks remotely, which is now hosted at <a href="https://apache.jamesthom.as">https://apache.jamesthom.as</a>. This tool uses Apache OpenWhisk (provided by IBM Cloud Functions), which means the project is being used to verify future releases of itself! I've also <a href="https://github.com/jthomas/openwhisk-release-verification">open-sourced</a> the code to provide an example of how to use the platform for automating tasks like this.</p>

<p>With this tool and others listed above, verifying new <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> releases has never been easier!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[OpenWhisk Web Action Errors With Sequences]]></title>
    <link href="http://jamesthom.as/blog/2019/02/27/openwhisk-web-action-errors-with-sequences/"/>
    <updated>2019-02-27T10:00:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/27/openwhisk-web-action-errors-with-sequences</id>
    <content type="html"><![CDATA[<p>This week, I came across an interesting problem when building HTTP APIs on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>.</p>

<p><blockquote><p>How can Apache OpenWhisk Web Actions, implemented using action sequences, handle application errors that need the sequence to stop processing and a custom HTTP response to be returned?</p></blockquote></p>

<p>This came from wanting to add custom <a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Authentication">HTTP authentication</a> to existing Web Actions. I had decided to enhance existing Web Actions with authentication using action sequences. This would combine a new action for authentication validation with the existing API route handlers.</p>

<p><img src="/images/sequences-and-web-actions/outline.png" title="" ></p>

<p>When the HTTP authentication is valid, the authentication action becomes a "<a href="https://en.wikipedia.org/wiki/NOP_(code)">no-op</a>", which passes along the HTTP request to the route handler action to process as normal.</p>

<p><strong>But what happens when authentication fails?</strong></p>

<p>The authentication action needs to stop request processing and return a <a href="https://httpstatuses.com/401">HTTP 401</a> response immediately.</p>

<p><img src="/images/sequences-and-web-actions/options.png" title="" ></p>

<p><em>Does Apache OpenWhisk even support this?</em></p>

<p>Fortunately, it does (phew) and I eventually worked out how to do this (based on a combination of re-reading <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/">documentation</a>, the platform <a href="https://github.com/apache/incubator-openwhisk/blob/master/core/controller/src/main/scala/org/apache/openwhisk/core/controller/WebActions.scala">source code</a> and just trying stuff out!).</p>

<p><em>Before explaining how to return custom HTTP responses using web action errors in sequences, let's review web actions, actions sequences and why developers often use them together...</em></p>

<h2>Web Actions</h2>

<p><a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">Web Actions</a> are OpenWhisk <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md">actions</a> that can be invoked using external HTTP requests.</p>

<p>Incoming HTTP requests are provided as event parameters. HTTP responses are controlled using attributes (<code>statusCode</code>, <code>body</code>, <code>headers</code>) in the action result.</p>

<p>Web Actions can be invoked directly, using the platform API, or connected to <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/apigateway.md">API Gateway endpoints</a>.</p>

<h3>example</h3>

<p>Here is an example Web Action that returns a static HTML page.</p>

<p>```javascript
function main() {
  return {</p>

<pre><code>headers: {      
  'Content-Type': 'text/html'
},
statusCode: 200,
body: '&lt;html&gt;&lt;body&gt;&lt;h3&gt;hello&lt;/h3&gt;&lt;/body&gt;&lt;/html&gt;'
</code></pre>

<p>  }
}
```</p>

<h3>exposing web actions</h3>

<p>Web actions can be exported from any existing action by setting an <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/annotations.md#annotations-specific-to-web-actions">annotation</a>.</p>

<p>This is handled automatically by CLI using the <code>‚Äîweb</code> configuration flag when creating or updating actions.</p>

<p><code>
wsk action create ACTION_NAME ACTION_CODE --web true
</code></p>

<h2>Action Sequences</h2>

<p>Multiple actions can be composed together into a "meta-action" using <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md#creating-action-sequences">sequences</a>.</p>

<p>Sequence configuration defines a series of existing actions to be called sequentially upon invocation.  Actions connected in sequences can use different runtimes and even be sequences themselves.</p>

<p><code>
wsk action create mySequence --sequence action_a,action_b,action_c
</code></p>

<p>Input events are passed to the first action in the sequence. Action results from each action in the sequence are passed to the next action in the sequence. The response from the last action in the sequence is returned as the action result.</p>

<h3>example</h3>

<p>Here is a sequence (<code>mySequence</code>) composed of three actions (<code>action_a</code>, <code>action_b</code>, <code>action_c</code>).</p>

<p><code>
wsk action create mySequence --sequence action_a,action_b,action_c
</code></p>

<p>Invoking <code>mySequence</code> will invoke <code>action_a</code> with the input parameters. <code>action_b</code> will be invoked with the result from <code>action_a</code>.  <code>action_c</code> will be invoked with the result from <code>action_b</code>. The result returned by <code>action_c</code> will be returned as the sequence result.</p>

<h2>Web Actions from Action Sequences</h2>

<p>Using Action Sequences as Web Actions is a useful pattern for externalising common HTTP request and response processing tasks into separate serverless functions.</p>

<p>These common actions can be included in multiple Web Actions, rather than manually duplicating the same boilerplate code in each HTTP route action. This is similar to the "<a href="https://dzone.com/articles/understanding-middleware-pattern-in-expressjs">middleware</a>" pattern used by lots of common web application frameworks.</p>

<p>Web Actions using this approach are easier to test, maintain and allows API handlers to implement core business logic rather than lots of duplicate boilerplate code.</p>

<h3>authentication example</h3>

<p>In my application, new authenticated web actions were composed of two actions (<code>check_auth</code> and the API route handler, e.g. <code>route_handler</code>).</p>

<p>Here is an outline of the <code>check_auth</code> function in Node.js.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>// stop sequence processing and return HTTP 401?
</code></pre>

<p>  }</p>

<p>  // ...else pass along request to next sequence action
  return params
}
```</p>

<p>The <code>check_auth</code> function will inspect the HTTP request and validate the authorisation token. If the token is valid, the function returns the input parameters untouched, which leads the platform the invoke the <code>route_handler</code> to generate the HTTP response from the API route.</p>

<p><strong>But what happens if the authentication is invalid?</strong></p>

<p>The  <code>check_auth</code> action needs to return a HTTP 401 response immediately, rather than proceeding to the  <code>route_handler</code> action.</p>

<p><img src="/images/sequences-and-web-actions/options.png" title="" ></p>

<h3>handling errors - synchronous results</h3>

<p>Sequence actions can stop sequence processing by returning an error. Action errors are indicated by action results which include an "error" property or return rejected promises (for asynchronous results). Upon detecting an error, the platform will return the error result as the sequence action response.</p>

<p><em>If <code>check_auth</code> returns an error upon authentication failures, sequence processing can be halted, but how to control the HTTP response?</em></p>

<p><a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md#error-handling">Error responses</a> can also control the HTTP response, using the same properties (<code>statusCode</code>, <code>headers</code> and <code>body</code>) as a successful invocation result, with one difference: <strong>those properties must be the children of the <code>error</code> property rather than top-level properties.</strong></p>

<p>This example shows the error result needed to generate an immediate HTTP 401 response.</p>

<p>```json
{
   "error": {</p>

<pre><code>  "statusCode": 401,
  "body": "Authentication credentials are invalid."
}
</code></pre>

<p>}
```</p>

<p>In Node.js, this can be returned using a synchronous result as shown here.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>const response = { statusCode: 401, body: "Authentication credentials are invalid." }
return { error: response }
</code></pre>

<p>  }</p>

<p>  return params
}
```</p>

<h3>handling errors - using promises</h3>

<p>If a rejected Promise is used to return an error from an asynchronous operation, the promise result needs to contain the HTTP response properties as <strong>top-level properties</strong>, rather than under an <code>error</code> parent. This is because the Node.js runtime automatically <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L118">serialises the promise value</a> to an <code>error</code> property on the activation result.</p>

<p>```javascript
const check_auth = (params) => {
  const headers = params.__ow_headers
  const auth = headers['authorization']</p>

<p>  if (!is_auth_valid(auth)) {</p>

<pre><code>const response = { statusCode: 401, body: "Authentication credentials are invalid." }
return Promise.reject(response)
</code></pre>

<p>  }</p>

<p>  return params
}
```</p>

<h2>conclusion</h2>

<p>Creating web actions from sequences is a novel way to implement the "HTTP middleware" pattern on serverless platforms. Surrounding route handlers with pre-HTTP request modifier actions for common tasks, allows route handlers to remove boilerplate code and focus on the core business logic.</p>

<p>In my application, I wanted to use this pattern was being used for custom HTTP authentication validation.</p>

<p>When the HTTP request contains the correct credentials, the request is passed along unmodified. When the credentials are invalid, the action needs to stop sequence processing and return a HTTP 401 response.</p>

<p>Working out how to do this wasn't immediately obvious from the documentation. HTTP response parameters need to included under the <code>error</code> property for synchronous results. I have now opened a PR to improve the project documentation about this.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pluggable Event Providers for Apache OpenWhisk]]></title>
    <link href="http://jamesthom.as/blog/2019/02/20/pluggable-event-providers-for-apache-openwhisk/"/>
    <updated>2019-02-20T11:53:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/20/pluggable-event-providers-for-apache-openwhisk</id>
    <content type="html"><![CDATA[<p>Recently I presented my work building "<em><a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">pluggable event providers</a></em>" for <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> to the open-source community on the <a href="https://www.youtube.com/openwhisk">bi-weekly video meeting</a>.</p>

<p>This was based on my experience building a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/feeds.md">new event provider</a> for Apache OpenWhisk, which led me to prototype an <strong>easier way to add event sources to platform</strong> whilst <strong>cutting down on the boilerplate code</strong> required.</p>

<p>Slides from the talk are <a href="https://speakerdeck.com/jthomas/apache-openwhisk-pluggable-event-providers">here</a> and there's also a video recording <a href="https://www.youtube.com/watch?v=krm7X5YpGy0">available</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/krm7X5YpGy0?start=89" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<p>This blog post is overview of what I talked about on the call, explaining the background for the project and what was built. Based on positive feedback from the community, I have now open-sourced <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">both</a> <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">components</a> of the experiment and will be merging it back upstream into Apache OpenWhisk in future.</p>

<h2>pluggable event providers - why?</h2>

<p>At the end of last year, I was asked to prototype an <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html">S3-compatible</a> Object Store event source for Apache OpenWhisk. Reviewing the <a href="https://github.com/search?q=incubator-openwhisk-package">existing event providers</a> helped me understand how they work and what was needed to build a new event source.</p>

<p>This led me to an interesting question...</p>

<blockquote><p>Why do we have relatively few community contributions for event sources?</p></blockquote>

<p>Most of the existing event sources in the project were contributed by IBM. There hasn't been a new event source from an external community member. This is in stark contrast to <a href="https://github.com/search?q=incubator-openwhisk-runtime">additional platform runtimes</a>. Support for PHP, Ruby, DotNet, Go and many more languages all came from community contributions.</p>

<p><em>Digging into the source code for the existing feed providers, I came to the following conclusions....</em></p>

<ul>
<li><strong>Trigger feed providers are not simple to implement.</strong></li>
<li><strong>Documentation how existing providers work is lacking.</strong></li>
</ul>


<p>Feed providers can feel a bit like magic to users. You call the <code>wsk</code> CLI with a <code>feed</code> parameter and that's it, the platform handles everything else. But what actually happens to bind triggers to external event sources?</p>

<p><em>Let's start by explaining how trigger feeds are implemented in Apache OpenWhisk, before moving onto my idea to make contributing new feed providers easier.</em></p>

<h2>how trigger feeds work</h2>

<p>Users normally interact with trigger feeds using the <code>wsk</code> CLI. Whilst creating a trigger, the <code>feed</code> parameter can be included to connect that trigger to an external event source. Feed provider options as provided as further CLI parameters.</p>

<p><code>
wsk trigger create periodic \
  --feed /whisk.system/alarms/alarm \
  --param cron "*/2 * * * *" \
  --param trigger_payload ‚Äú{‚Ä¶}‚Äù \
  --param startDate "2019-01-01T00:00:00.000Z" \
  --param stopDate "2019-01-31T23:59:00.000Z"
</code></p>

<p><em>But what are those trigger feed identifiers used with the <code>feed</code> parameter?</em></p>

<p><strong>It turns out they are just normal actions which have been shared in a public package!</strong></p>

<p>The CLI creates the trigger (using the platform API) and then invokes the referenced feed action. Invocation parameters include the following values used to manage the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/feeds.md#implementing-feed-actions">trigger feed lifecycle</a>.</p>

<ul>
<li><code>lifecycleEvent</code> - Feed operation (<code>CREATE</code>, <code>READ</code>, <code>UPDATE</code>, <code>DELETE</code>, <code>PAUSE</code>, or <code>UNPAUSE</code>).</li>
<li><code>triggerName</code> - Trigger identifier.</li>
<li><code>authKey</code> - API key provided to invoke trigger.</li>
</ul>


<p>Custom feed parameters from the user are also included in the event parameters.</p>

<p><strong>This is the entire interaction of the platform with the feed provider.</strong></p>

<p>Providers are responsible for the full management lifecycle of trigger feed event sources. They have to maintain the list of registered triggers and auth keys, manage connections to user-provided event sources, fire triggers upon external events, handle retries and back-offs in cases of rate-limiting and much more.</p>

<p>Feed providers used with a trigger are stored as custom annotations. This allows the CLI to call the same feed action to stop the event binding when the trigger is deleted.</p>

<h3>trigger management</h3>

<p>Reading the source code for the <a href="https://github.com/search?q=incubator-openwhisk-package">existing feed providers</a>, nearly all of the code is responsible for handling the lifecycle of trigger management events, rather than integrating with the external event source.</p>

<p>Despite this, all of the existing providers are in separate repositories and don't share code explicitly, although the same source files have been replicated in different repos.</p>

<p>The <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB feed provider</a> is a good example of how feed providers can be implemented.</p>

<h3>couchdb feed provider</h3>

<p>The <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB trigger feed provider</a> uses a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changes.js">public action</a> to handle the lifecycle events from the <code>wsk</code> CLI.</p>

<p><img src="/images/pluggable-providers/feeds-overview.png" title="" ></p>

<p>This <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changes.js">action</a> just proxies the incoming requests to a separate <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/actions/event-actions/changesWebAction.js">web actio</a>n. The <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/webactions.md">web action</a> implements the logic to handle the trigger lifecycle event. The web action uses a CouchDB database used to store registered triggers. Based upon the lifecycle event details, the web action updates the database document for that trigger.</p>

<p><img src="/images/pluggable-providers/feeds-provider.png" title="" ></p>

<p>The feed provider also runs a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/tree/master/provider">seperate Docker container</a>, which handles listening to CouchDB change feeds from user-provided credentials. It uses the changes feed from the trigger management database, modified from the web action, to listen for triggers being added, removed, disabled or re-enabled.</p>

<p><img src="/images/pluggable-providers/feeds-fire-trigger.png" title="" ></p>

<p>When database change events <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/provider/lib/utils.js#L78">occur</a>, the container <a href="https://github.com/apache/incubator-openwhisk-package-cloudant/blob/master/provider/lib/utils.js#L66-L76">fires triggers</a> on the platform with the event details.</p>

<h2>building a new event provider?</h2>

<p>Having understood how feed providers work (and how the existing providers were designed), I started to think about the new event source for an S3-compatible object store.</p>

<p>Realising ~90% of the code between providers was the same, I wondered if there was a different approach to creating new event providers, rather than cloning an existing provider and changing the small amount of code used to interact with the event sources.</p>

<p><strong>What about building a generic event provider which a pluggable event source?</strong></p>

<p>This generic event provider would handle all the trigger management logic, which isn't specific to individual event sources. The event source plugin would manage connecting to external event sources and then firing triggers as event occurred. Event source plugins would implement a standard interface and be registered dynamically during startup.</p>

<p><img src="/images/pluggable-providers/generic-provider.png" title="" ></p>

<h3>advantages</h3>

<p>Using this approach would make it much easier to contribute and maintain new event sources.</p>

<ul>
<li><p>Users would be able to create new event sources with a few lines of custom integration code, rather than replicating all the generic trigger lifecycle management code.</p></li>
<li><p>Maintaining a single repo for the generic event provider is easier than having the same code copied and pasted in multiple independent repositories.</p></li>
</ul>


<p>I started hacking away at the existing CouchDB event provider to replace the event source integration with a generic plugin interface. Having completed this, I then wrote a new S3-compatible event source using the plugin model. After a couple of weeks I had something working....</p>

<h2>generic event provider</h2>

<p>The <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">generic event provider</a> is based on the exiting CouchDB feed provider source code. The project contains the stateful container code and feed package actions (public &amp; web). It uses the same platform services (CouchDB and Redis) as the existing provider to maintain trigger details.</p>

<p>The event provider plugin is integrated through the <code>EVENT_PROVIDER</code> environment variable. The name should refer to a Node.js module from NPM with the following <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider#plugin-interface">interface</a>.</p>

<p>```javascript
// initialise plugin instance (must be a JS constructor)
module.exports = function (trigger_manager, logger) {</p>

<pre><code>// register new trigger feed
const add = async (trigger_id, trigger_params) =&gt; {}
// remove existing trigger feed
const remove = async trigger_id =&gt; {}
</code></pre>

<p>   return { add, remove }
}</p>

<p>// valiate feed parameters
module.exports.validate = async trigger_params => {}
```</p>

<p>When a new trigger is added to the trigger feeds' database, the details will be passed to the <code>add</code> method. Trigger parameters will be used to set up listening to the external event source. When external events occur, the <code>trigger_manager</code> can be use to automatically fire triggers.</p>

<p>When users delete triggers with feeds, the trigger will be removed from the database. This will lead to the <code>remove</code> method being called. Plugins should stop listening to messages for this event source.</p>

<h3>firing trigger events</h3>

<p>As event arrive from the external source, the plugin can use the <code>trigger_manager</code> instance, passed in through the constructor, to fire triggers with the identifier.</p>

<p>The <code>trigger_manager</code> parameter exposes two async functions:</p>

<ul>
<li><code>fireTrigger(id, params)</code> - fire trigger given by id passed into <code>add</code> method with event parameters.</li>
<li><code>disableTrigger(id, status_code, message)</code> - disable trigger feed due to external event source issues.</li>
</ul>


<p>Both functions handle the retry logic and error handling for those operations. These should be used by the event provider plugin to fire  triggers when events arrive from external sources and then disable triggers due to external event source issues.</p>

<h3>validating event source parameters</h3>

<p>This static function on the plugin constructor is used to validate incoming trigger feed parameters for correctness, e.g. checking  authentication credentials for an event source. It is passed the trigger  parameters from the user.</p>

<h2>S3 event feed provider</h2>

<p>Using this new generic event provider, I was able to create an event source for an <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">S3-compatible object store</a>. Most importantly, this new event source was implemented using just <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed/tree/master/lib">~300 lines</a> of JavaScript! This is much smaller than the 7500 lines of code in the generic event provider.</p>

<p>The feed provider polls buckets on an interval using the <code>ListObjects</code> <a href="https://docs.aws.amazon.com/AmazonS3/latest/API/v2-RESTBucketGET.html">API call</a>. Results are cached in Redis to allow comparison between intervals. Comparing the differences in bucket file name and etags, allows file change events to be detected.</p>

<p>Users can call the feed provider with a bucket name, endpoint, API key and polling interval.</p>

<p><code>
wsk trigger create test-s3-trigger --feed /&lt;PROVIDER_NS&gt;/s3-trigger-feed/changes --param bucket &lt;BUCKET_NAME&gt; --param interval &lt;MINS&gt; --param s3_endpoint &lt;S3_ENDPOINT&gt; --param s3_apikey &lt;COS_KEY&gt;
</code></p>

<p>File events are fired as the bucket files change with the following trigger events.</p>

<p>```
{
  "file": {</p>

<pre><code>"ETag": "\"fb47672a6f7c34339ca9f3ed55c6e3a9\"",
"Key": "file-86.txt",
"LastModified": "2018-12-19T08:33:27.388Z",
"Owner": {
  "DisplayName": "80a2054e-8d16-4a47-a46d-4edf5b516ef6",
  "ID": "80a2054e-8d16-4a47-a46d-4edf5b516ef6"
},
"Size": 25,
"StorageClass": "STANDARD"
</code></pre>

<p>  },
  "status": "deleted"
}
```</p>

<p><em>Pssst - if you are using <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> - I actually have this deployed and running so you can try it out. Use the <code>/james.thomas@uk.ibm.com_dev/s3-trigger-feed/changes</code> feed action name. This package is only available in the London region.</em></p>

<h2>next steps</h2>

<p>Feedback on the call was overwhelming positive on my experiment. Based upon this, I've now open-sourced both the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider">generic event provider</a> and <a href="https://github.com/jthomas/openwhisk-s3-trigger-feed">s3 event source plugin</a> to allow the community to evaluate the project further.</p>

<p>I'd like to build a few more example event providers to validate the approach further before moving towards contributing this code back upstream.</p>

<p>If you want to try this generic event provider out with your own install of OpenWhisk, please see the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider/blob/master/README.md#running-the-provider--plugin">documentation</a> in the README for how to get started.</p>

<p>If you want to build new event sources, please see the <a href="https://github.com/jthomas/openwhisk-pluggable-event-provider/blob/master/README.md#plugin-interface">instructions</a> in the generic feed provider repository and take a look at the S3 plugin for an example to follow.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CouchDB Filters with OpenWhisk Triggers]]></title>
    <link href="http://jamesthom.as/blog/2019/02/12/couchdb-filters-with-openwhisk-triggers/"/>
    <updated>2019-02-12T14:22:00+00:00</updated>
    <id>http://jamesthom.as/blog/2019/02/12/couchdb-filters-with-openwhisk-triggers</id>
    <content type="html"><![CDATA[<p>Imagine you have an <a href="http://openwhisk.incubator.apache.org/">OpenWhisk</a> <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions.md">action</a> to send emails to users to verify their email addresses. User profiles, containing email addresses and verification statuses, are maintained in a <a href="https://couchdb.apache.org/">CouchDB</a> database.</p>

<p>```json
{</p>

<pre><code>...
"email": {
    "address": "user@host.com",
    "status": "unverified"
}
</code></pre>

<p>}
```</p>

<p>Setting up a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant">CouchDB trigger feed</a> allows the email action <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/triggers_rules.md">to be invoked</a> when the user profile changes. When user profiles have unverified email addresses, the action can send verification emails.</p>

<p>Whilst this works fine - it will result in a lot of unnecessary invocations. All modifications to user profiles, not just the email field, will result in the action being invoked. This will incur a cost despite the action having nothing to do.</p>

<blockquote><p>How can we restrict document change events to just those we care about?</p></blockquote>

<p>CouchDB <a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html#filter-functions">filter functions</a> to the rescue ü¶∏‚Äç‚ôÇÔ∏èü¶∏‚Äç.</p>

<h2>CouchDB Filter Functions</h2>

<p><a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html#filter-functions">Filter functions</a> are Javascript functions executed against (potential) <a href="http://guide.couchdb.org/draft/notifications.html">change feed events</a>. The function is invoked with each document update. The return value is evaluated as a boolean variable. If true, the document is published on the changes feed. Otherwise, the event is filtered from the changes feed.</p>

<h3>example</h3>

<p>Filter functions are created through <a href="https://docs.couchdb.org/en/stable/ddocs/ddocs.html">design documents</a>. Function source strings are stored as properties under the <code>filters</code> document attribute. Key names are used as filter identifiers.</p>

<p>Filter functions should have the following interface.</p>

<p>```javascript
function(doc, req){</p>

<pre><code>// document passes test
if (doc.property == 'value'){
    return true;
}

// ... else ignore document upate
return false;
</code></pre>

<p>}
```</p>

<p> <code>doc</code> is the modified document object and <code>req</code> contains (optional) request parameters.</p>

<p><em>Let's now explain how to create a filter function to restrict profile update events to just those with unverified email addresses...</em></p>

<h2>Filtering Profile Updates</h2>

<h3>user profile documents</h3>

<p>In this example, email addresses are stored in user profile documents under the <code>email</code> property. <code>address</code> contains the user's email address and <code>status</code> records the verification status (<code>unverified</code> or <code>verified</code>).</p>

<p>When a new user is added, or an existing user changes their email address, the <code>status</code> attribute is set to <code>unverified</code>. This indicates a verification message needs to be sent to the email address.</p>

<p>```json
{</p>

<pre><code>...
"email": {
    "address": "user@host.com",
    "status": "unverified"
}
</code></pre>

<p>}
```</p>

<h3>unverified email filter</h3>

<p>Here is the CouchDB filter function that will ignore document updates with verified email addresses.</p>

<p>```
function(doc){</p>

<pre><code>if (doc.email.status == 'unverified'){
    return true;
}

return false
</code></pre>

<p>}
```</p>

<h3>design document with filters</h3>

<p>Save the following JSON document in CouchDB. This creates a new design document (<code>profile</code>) containing a filter function (<code>unverified-emails</code>).</p>

<p>```json
{
  "<em>id": "</em>design/profile",<br/>
  "filters": {</p>

<pre><code>"unverified-emails": "function (doc) {\n  if (doc.email.status == 'unverified') {\n    return true\n  }\n  return false\n}"
</code></pre>

<p>  },
  "language": "javascript"
}
```</p>

<h3>trigger feed with filter</h3>

<p>Once the design document is created, the filter name can be used as a <a href="https://github.com/apache/incubator-openwhisk-package-cloudant#create-the-trigger-using-the-filter-function">trigger feed parameter</a>.</p>

<p><code>
wsk trigger create verify_emails --feed /_/myCloudant/changes \
--param dbname user_profiles \
--param filter "profile/unverified-emails"
</code></p>

<p>The trigger only fires when a profile change contains an unverified email address. No more unnecessary invocations, which saves us money! üòé</p>

<h3>caveats</h3>

<p><em>"Why are users getting multiple verification emails?"</em> üò°</p>

<p>If a user changes their profile information, whilst leaving their email address the same but before clicking the verification email, an additional email will be sent.</p>

<p>This is because the <code>status</code> field is still in the <code>unverified</code> state when the next document update occurs. Filter functions are stateless and can't decide if this email address has already been seen.</p>

<p>Instead of leaving the <code>status</code> field as <code>unverified</code>, the email action should change the state to another value, e.g. <code>pending</code>, to indicate the verification email has been sent.</p>

<p>Any further document updates, whilst waiting for the verification response, won't pass the filter and users won't receive multiple emails. üëç</p>

<h2>Conclusion</h2>

<p>CouchDB filters are an easy way to subscribe to a subset of events from the changes feed. Combining CouchDB trigger feeds with filters allows actions to ignore irrelevant document updates. Multiple trigger feeds can be set up from a single database using filter functions.</p>

<p>As well as saving unnecessary invocations (and therefore money), this can simplify data models. A single database can be used to store all documents, rather than having to split different types into multiple databases, whilst still supporting changes feeds per document type.</p>

<p>This is an awesome feature of CouchDB!</p>
]]></content>
  </entry>
  
</feed>
