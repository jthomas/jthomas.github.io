<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: openwhisk | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/openwhisk/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2018-12-13T08:57:31+00:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Finding photos on Twitter using face recognition with TensorFlow.js]]></title>
    <link href="http://jamesthom.as/blog/2018/10/30/finding-photos-on-twitter-using-face-recognition/"/>
    <updated>2018-10-30T09:34:00+00:00</updated>
    <id>http://jamesthom.as/blog/2018/10/30/finding-photos-on-twitter-using-face-recognition</id>
    <content type="html"><![CDATA[<p>As a developer advocate, I spend a lot of time at developer conferences (talking about serverless üòé). Upon returning from each trip, I need to compile a "trip report" on the event for my bosses. This helps demonstrate the value in attending events and that I'm not just accruing air miles and hotel points for fun... üõ´üè®</p>

<p>I always include any social media content people post about my talks in the trip report. This is usually tweets with photos of me on stage. If people are tweeting about your session, I assume they enjoyed it and wanted to share with their followers.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&quot;Servers kill your productivity&quot; <a href="https://twitter.com/thomasj?ref_src=twsrc%5Etfw">@thomasj</a> at <a href="https://twitter.com/hashtag/CodeMobileUK?src=hash&amp;ref_src=twsrc%5Etfw">#CodeMobileUK</a> <a href="https://t.co/Y4NsiBBSxT">pic.twitter.com/Y4NsiBBSxT</a></p>&mdash; Mihai C√ÆrlƒÉnaru (@MCirlanaru) <a href="https://twitter.com/MCirlanaru/status/981170555834441729?ref_src=twsrc%5Etfw">April 3, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p><strong>Finding tweets with photos about your talk from attendees is surprisingly challenging.</strong></p>

<p>Attendees often forget to include your twitter username in their tweets. This means the only way to find those photos is to manually scroll through all the results from the conference hashtag. This is problematic at conferences with thousands of attendees all tweeting during the event. <em>#devrelproblems</em>.</p>

<p>Having become bored of manually trawling through all the tweets for each conference, I had a thought...</p>

<blockquote><p><em>"Can't I write some code to do this for me?"</em></p></blockquote>

<p>This didn't seem like too ridiculous an idea. Twitter has <a href="https://developer.twitter.com/en/docs/tweets/search/overview">an API</a>, which would allow me to retrieve all tweets for a conference hashtag. Once I had all the tweet photos, couldn't I run some magic AI algorithm over the images to tell me if I was in them? ü§î</p>

<p>After a couple of weeks of hacking around (and overcoming numerous challenges) I had (to my own amazement) managed to <a href="https://github.com/jthomas/findme">build a serverless application</a> which can find unlabelled photos of a person on twitter using machine learning with <a href="https://github.com/tensorflow/tfjs">TensorFlow.js</a>.</p>

<p><img src="/images/face-recog/find-me-demo.gif" alt="FindMe Example" /></p>

<p><em>If you just want to try this application yourself, follow the instructions in the Github repo: <a href="https://github.com/jthomas/findme">https://github.com/jthomas/findme</a></em></p>

<h2>architecture</h2>

<p><img src="/images/face-recog/architecture.png" alt="FindMe Architecture Diagram" /></p>

<p>This application has four <a href="https://github.com/jthomas/findme/blob/master/serverless.yml">serverless functions</a> (two API handlers and two backend services) and a client-side application from a static web page. Users log into the <a href="https://github.com/jthomas/findme/tree/master/public">client-side application</a> using Auth0 with their Twitter account. This provides the backend application with the user's profile image and Twitter API credentials.</p>

<p>When the user invokes a search query, the client-side application invokes the API endpoint for the <code>register_search</code> <a href="https://github.com/jthomas/findme/blob/master/schedule_search.js">function</a> with the query terms and twitter credentials. This function registers a new search job in Redis and fires a new <code>search_request</code> trigger event with the query and job id. This job identifier is returned to the client to poll for real-time status updates.</p>

<p>The <code>twitter_search</code> <a href="https://github.com/jthomas/findme/blob/master/twitter_search.js">function</a> is connected to the <code>search_request</code> trigger and invoked for each event. It uses the Twitter Search API to retrieve all tweets for the search terms. If tweets retrieved from the API contain photos, those tweet ids (with photo urls) are fired as new <code>tweet_image</code> trigger events.</p>

<p>The <code>compare_images</code> <a href="https://github.com/jthomas/findme/blob/master/compare_images.js">function</a> is connected to the <code>tweet_image</code> trigger. When invoked, it downloads the user's twitter profile image along with the tweet image and runs face detection against both images, using the <code>face-api.js</code> <a href="https://github.com/justadudewhohacks/face-api.js">library</a>. If any faces in the tweet photo match the face in the user's profile image, tweet ids are written to Redis before exiting.</p>

<p>The client-side web page polls for real-time search results by polling the API endpoint for the <code>search_status</code>  <a href="https://github.com/jthomas/findme/blob/master/search_status.js">function</a> with the search job id. Tweets with matching faces are displayed on the web page using the <a href="https://developer.twitter.com/en/docs/twitter-for-websites/embedded-tweets/overview">Twitter JS library</a>.</p>

<h2>challenges</h2>

<p>Since I had found an <a href="https://github.com/justadudewhohacks/face-api.js">NPM library to handle face detection</a>, I could just use this on a serverless platform by including the library within the zip file used to create my serverless application? Sounds easy, right?!</p>

<p><strong>ahem - not so faas-t.... ‚úã</strong></p>

<p>As discussed in <a href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/">previous blog posts</a>, there are numerous challenges in using TF.js-based libraries on serverless platforms. Starting with making the packages available in the runtime and loading model files to converting images for classification, these libraries are not like using normal NPM modules.</p>

<p><em>Here are the main challenges I had to overcome to make this serverless application work...</em></p>

<h3>using tf.js libraries on a serverless platform</h3>

<p>The <a href="https://github.com/tensorflow/tfjs-node">Node.js backend drivers</a> for TensorFlow.js use a native shared C++ library  (<code>libtensorflow.so</code>) to execute models on the CPU or GPU. This native dependency is compiled for the platform during the <code>npm install</code> process. The shared library file is around 142MB, which is too large to include in the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">deployment package</a> for most serverless platforms.</p>

<p>Normal workarounds for this issue store large dependencies in an object store. These files are dynamically retrieved during cold starts and stored in the runtime filesystem, as shown in this pseudo-code. This workaround does add an additional delay to cold start invocations.</p>

<p>```javascript
let cold_start = false</p>

<p>const library = 'libtensorflow.so'</p>

<p>if (cold_start) {
  const data = from_object_store(library)
  write_to_fs(library, data)
  cold_start = true
}</p>

<p>// rest of function code‚Ä¶
```</p>

<p><strong>Fortunately, I had a better solution using Apache OpenWhisk's support for custom Docker runtimes!</strong></p>

<p>This feature allows serverless applications to use <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md#creating-native-actions">custom Docker images</a> as the runtime environment. Creating custom images with <a href="http://jamesthom.as/blog/2017/08/04/large-applications-on-openwhisk/">large libraries pre-installed</a> means they can be excluded from deployment packages. üíØ</p>

<p>Apache OpenWhisk publishes all existing <a href="https://hub.docker.com/r/openwhisk/">runtime images</a> on Docker Hub. Using existing runtime images as base images means Dockerfiles for custom runtimes are minimal. Here's the Dockerfile needed to build a custom runtime with the TensorFlow.js Node.js backend drivers pre-installed.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs-node
```</p>

<p>Once this image has been built and published on Dockerhub, you can use it when creating new functions.</p>

<p><em>I used this approach to build a <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/tags/">custom TensorFlow.js runtime</a> which is available on Docker Hub: <code>jamesthomas/action-nodejs-v8:tfjs-faceapi</code></em></p>

<p>OpenWhisk actions created using the <code>wsk</code> command-line use a configuration flag (<code>--docker</code>) to specify custom runtime images.</p>

<p><code>
wsk action create classify source.js --docker jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></p>

<p>The OpenWhisk provider plugin for The Serverless Framework also supports <a href="https://github.com/serverless/serverless-openwhisk#custom-runtime-images">custom runtime images</a> through a configuration parameter (<code>image</code>) under the function configuration.</p>

<p>```yaml
service: machine-learning</p>

<p>provider:
  name: openwhisk</p>

<p>functions:
  classify:</p>

<pre><code>handler: source.main
image: jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></pre>

<p>```</p>

<p>Having fixed the issue of library loading on serverless platforms, I could move onto the next problem, loading the pre-trained models... üíΩ</p>

<h3>loading pre-trained models</h3>

<p>Running the <a href="https://github.com/justadudewhohacks/face-api.js#usage-loading-models">example code</a> to load the pre-trained models for face recognition gave me this error:</p>

<p><code>
ReferenceError: fetch is not defined
</code></p>

<p>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I discovered how to manually load TensorFlow.js models from the filesystem using the <code>file://</code> URI prefix. Unfortunately, the <code>face-api.js</code> library doesn't support this feature. Models are <a href="https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/4a7d981dbb37e0d3dabc962e1cbfb6122e535263/src/dom/loadWeightMap.ts#L12">automatically loaded</a> using the <code>fetch</code> HTTP client. This HTTP client is available into modern browsers but not in the Node.js runtime.</p>

<p>Overcoming this issue relies on providing an instance of a compatible HTTP client in the runtime. The <code>node-fetch</code> library is a <a href="https://www.npmjs.com/package/node-fetch">implementation of the fetch client</a> API for the Node.js runtime. By manually installing this module and exporting as a global variable, the library can then use the HTTP client as expected.</p>

<p><code>javascript
// Make HTTP client available in runtime
global.fetch = require('node-fetch')
</code></p>

<p>Model configuration and weight files can then be loaded from the library's Github repository using this URL:</p>

<p>https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/</p>

<p><code>js
faceapi.loadFaceDetectionModel('&lt;GITHUB_URL&gt;')
</code></p>

<h3>face detection in images</h3>

<p>The <code>face-api.js</code> library has a <a href="https://github.com/justadudewhohacks/face-api.js#detecting-faces">utility function</a> (<code>models.allFaces</code>) to automatically detect and calculate descriptors for all faces found in an image. Descriptors are a feature vector (of 128 32-bit float values) which uniquely describes the characteristics of a persons face.</p>

<p><code>javascript
const results = await models.allFaces(input, minConfidence)
</code></p>

<p><em>The input to this function is the input tensor with the RGB values from an image. In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I explained how to convert an image from the filesystem in Node.js to the input tensor needed by the model.</em></p>

<p>Finding a user by comparing their twitter profile against photos from tweets starts by running face detection against both images. By comparing computed descriptor values, a measure of similarity can be established between faces from the images.</p>

<h3>face comparison</h3>

<p>Once the face descriptors have been calculated the library provides a utility function to compute the euclidian distance between two descriptors vectors. If the difference between two face descriptors is less than a threshold value, this is used to identify the same person in both images.</p>

<p>```javascript
const distance = faceapi.euclideanDistance(descriptor1, descriptor2)</p>

<p>if (distance &lt; 0.6)
  console.log('match')
else
  console.log('no match')
```</p>

<p>I've no idea why 0.6 is chosen as the threshold value but this seemed to work for me! Even small changes to this value dramatically reduced the precision and recall rates for my test data. I'm calling it the Goldilocks value, just use it...</p>

<h2>performance</h2>

<p>Once I had the end to end application working, I wanted to make it was fast as possible. By optimising the performance, I could improve the application responsiveness and reduce compute costs for my backend. Time is literally money with serverless platforms.</p>

<h3>baseline performance</h3>

<p>Before attempting to optimise my application, I needed to understand the baseline performance. Setting up experiments to record invocation durations gave me the following average test results.</p>

<ul>
<li><em>Warm invocations</em>: ~5 seconds</li>
<li><em>Cold invocations</em>: ~8 seconds</li>
</ul>


<p>Instrumenting the code with <code>console.time</code> statements revealed execution time was comprised of five main sections.</p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       3200 ms       </td>
<td>       2000 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 8 seconds</strong>   </td>
<td>   <strong>~ 5 seconds</strong>  </td>
</tr>
</tbody>
</table>


<p><em>Initialisation</em> was the delay during cold starts to create the runtime environment and load all the library files and application code. <em>Model Loading</em> recorded the time spent instantiating the TF.js models from the source files. <em>Image Loading</em> was the time spent converting the RGB values from images into input tensors, this happened twice, once for the twitter profile picture and again for the tweet photo. <em>Face Detection</em> is the elapsed time to execute the <code>models.allFaces</code> method and <code>faceapi.euclideanDistance</code> methods for all the detected faces. <em>Everything else</em> is well... everything else.</p>

<p>Since model loading was the largest section, this seemed like an obvious place to start optimising. üìàüìâ</p>

<h3>loading model files from disk</h3>

<p>Overcoming the initial model loading issue relied on manually exposing the expected HTTP client in the Node.js runtime. This allowed models to be dynamically loaded (over HTTP) from the external Github repository. Models files were about 36MB.</p>

<p>My first idea was to load these model files from the filesystem, which should be much faster than downloading from Github. Since I was already building a custom Docker runtime, it was a one-line change to include the model files within the runtime filesystem.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs-node</p>

<p>COPY weights weights
```</p>

<p>Having re-built the image and pushed to Docker Hub, the classification function's runtime environment now included models files in the filesystem.</p>

<p><strong>But how do we make the <code>face-api.js</code> library load models files from the filesystem when it is using a HTTP client?</strong></p>

<p>My solution was to write a <code>fetch</code> client that proxied calls to retrieve files from a HTTP endpoint to the local filesystem. üò± I'd let you decide whether this is a brilliant or terrible idea!</p>

<p>```javascript
global.fetch = async (file) => {
  return {</p>

<pre><code>json: () =&gt; JSON.parse(fs.readFileSync(file, 'utf8')),
arrayBuffer: () =&gt; fs.readFileSync(file)
</code></pre>

<p>  }
}</p>

<p>const model = await models.load('/weights')
```</p>

<p>The <code>face-api.js</code> library only used two methods (<code>json()</code> &amp; <code>arrayBuffer()</code>) from the HTTP client. Stubbing out these methods to proxy <code>fs.readFileSync</code> meant files paths were loaded from the filesystem. Amazingly, this seemed to just work, hurrah!</p>

<p><strong>Implementing this feature and re-running performance tests revealed this optimisation saved about 500 ms from the Model Loading section.</strong></p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       1500 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 4.5 seconds</strong>  </td>
</tr>
</tbody>
</table>


<p>This was less of an improvement than I'd expected. Parsing all the model files and instantiating the internal objects was more computationally intensive than I realised. This performance improvement did improve both cold and warm invocations, which was a bonus.</p>

<p><em>Despite this optimisation, model loading was still the largest section in the classification function...</em></p>

<h3>caching loaded models</h3>

<p>There's a good strategy to use when optimising serverless functions...</p>

<p><img src="/images/face-recog/cache-all-the-things.jpg" alt="CACHE ALL THE THINGS" /></p>

<p>Serverless runtimes re-use runtime containers for consecutive requests, known as warm environments. Using local state, like global variables or the runtime filesystem, to cache data between requests can be used to improve performance during those invocations.</p>

<p>Since model loading was such an expensive process, I wanted to cache initialised models. Using a global variable, I could control whether to trigger model loading or return the pre-loaded models. Warm environments would re-use pre-loaded models and remove model loading delay.</p>

<p>```javascript
const faceapi = require('face-api.js')</p>

<p>let LOADED = false</p>

<p>exports.load = async location => {
  if (!LOADED) {</p>

<pre><code>await faceapi.loadFaceDetectionModel(location)
await faceapi.loadFaceRecognitionModel(location)
await faceapi.loadFaceLandmarkModel(location)

LOADED = true
</code></pre>

<p>  }</p>

<p>  return faceapi
}
```</p>

<p><strong>This performance improvement had a significant impact of the performance for warm invocations. Model loading became "free".</strong> üëç</p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       0 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 3 seconds</strong>  </td>
</tr>
</tbody>
</table>


<h3>caching face descriptors</h3>

<p>In the initial implementation, the face comparison function was executing face detection against both the user's twitter profile image and tweet photo for comparison. Since the twitter profile image was the same in each search request, running face detection against this image would always return the same results.</p>

<p>Rather than having this work being redundantly computed in each function, caching the results of the computed face descriptor for the profile image meant it could re-used across invocations. This would reduce by 50% the work necessary in the Image &amp; Model Loading sections.</p>

<p>The <code>face-api.js</code> library returns the face descriptor as a typed array with 128 32-bit float values. Encoding this values as a hex string allows them to be stored and retrieved from Redis. This code was used to convert float values to hex strings, whilst maintaining the exact precision of those float values.</p>

<p>```javascript
const encode = typearr => {
  const encoded = Buffer.from(typearr.buffer).toString('hex')<br/>
  return encoded
}</p>

<p>const decode = encoded => {
  const decoded = Buffer.from(encoded, 'hex')
  const uints = new Uint8Array(decoded)
  const floats = new Float32Array(uints.buffer)
  return floats
}
```</p>

<p>This optimisation improves the performance of most cold invocations and all warm invocations, removing over 1200 ms of computation time to compute the results.</p>

<table>
<thead>
<tr>
<th></th>
<th>                    </th>
<th align="center"> Cold Starts (Cached) </th>
<th align="center">    Warm Starts    </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Initialisation     </td>
<td align="center">       1200 ms        </td>
<td align="center">       0 ms        |</td>
</tr>
<tr>
<td></td>
<td> Model Loading      </td>
<td align="center">       2700 ms        </td>
<td align="center">      1500 ms      |</td>
</tr>
<tr>
<td></td>
<td> Image Loading      </td>
<td align="center">        500 ms        </td>
<td align="center">      500 ms       |</td>
</tr>
<tr>
<td></td>
<td> Face Detection     </td>
<td align="center">   700 ms - 900 ms    </td>
<td align="center">  700 ms - 900 ms  |</td>
</tr>
<tr>
<td></td>
<td> Everything Else    </td>
<td align="center">       1000 ms        </td>
<td align="center">      500 ms       |</td>
</tr>
<tr>
<td></td>
<td> <strong>Total Duration</strong> </td>
<td align="center">   <strong>~ 6 seconds</strong>    </td>
<td align="center"> <strong>~ 2.5 seconds</strong> |</td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       0 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms      </td>
<td>     500 ms       </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms  </td>
<td> 700 ms - 900 ms </td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 3 seconds</strong>  </td>
</tr>
</tbody>
</table>


<h3>final results + cost</h3>

<p>Application performance was massively improved with all these optimisations. As demonstrated in the video above, the application could process tweets in real-time, returning almost instant results. Average invocation durations were now.</p>

<ul>
<li><em>Warm invocations</em>: ~2.5 seconds</li>
<li><em>Cold invocations (Cached)</em>: ~6 seconds</li>
</ul>


<p>Serverless platforms charge for compute time by the millisecond, so these improvements led to cost savings of 25% for cold invocations (apart the first classification for a user) and 50% for warm invocations.</p>

<p>Classification functions used 512MB of RAM which meant IBM Cloud Functions would provide 320,000 "warm" classifications or 133,333 "cold" classifications within the free tier each month. Ignoring the free tier, 100,000 "warm" classifications would cost $5.10 and 100,000 "cold" classifications $2.13.</p>

<h2>conclusion</h2>

<p>Using TensorFlow.js with serverless cloud platforms makes it easy to build scalable machine learning applications in the cloud. Using the horizontal scaling capabilities of serverless platforms, thousands of model classifications can be ran in parallel. This can be more performant than having dedicated hardware with a GPU, especially with compute costs for serverless applications being so cheap.</p>

<p>TensorFlow.js is ideally suited to serverless application due to the JS interface, (relatively) small library size and availability of pre-trained models. Despite having no prior experience in Machine Learning, I was able to use the library to build a face recognition pipeline, processing 100s of images in parallel, for real-time results. This amazing library opens up machine learning to a whole new audience!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Serverless Machine Learning With TensorFlow.js]]></title>
    <link href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/"/>
    <updated>2018-08-13T12:16:00+01:00</updated>
    <id>http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js</id>
    <content type="html"><![CDATA[<p>In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to use <a href="https://js.tensorflow.org/">TensorFlow.js</a> on Node.js to run <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d#file-script-js">visual recognition on images from the local filesystem</a>. TensorFlow.js is a JavaScript version of the open-source machine learning library from Google.</p>

<p>Once I had this working with a local Node.js script, my next idea was to convert it into a serverless function. Running this function on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> (<a href="https://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) would turn the script into my own visual recognition microservice.</p>

<p>{% img /images/tfjs-serverless/tf-js-example.gif Serverless TensorFlow.js Function %}</p>

<p>Sounds easy, right? It's just a JavaScript library? So, zip it up and away we go... <strong><em>ahem</em></strong> üëä</p>

<p><em>Converting the image classification script to run in a serverless environment had the following challenges...</em></p>

<ul>
<li><strong>TensorFlow.js libraries need to be available in the runtime.</strong></li>
<li><strong>Native bindings for the library must be compiled against the platform architecture.</strong></li>
<li><strong>Models files need to be loaded from the filesystem.</strong></li>
</ul>


<p>Some of these issues were more challenging than others to fix! Let's start by looking at the details of each issue, before explaining how <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">Docker support</a> in Apache OpenWhisk can be used to resolve them all.</p>

<h2>Challenges</h2>

<h3>TensorFlow.js Libraries</h3>

<p>TensorFlow.js libraries are not included in the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js runtimes</a> provided by the Apache OpenWhisk.</p>

<p>External libraries <a href="http://jamesthom.as/blog/2016/11/28/npm-modules-in-openwhisk/">can be imported</a> into the runtime by deploying applications from a zip file. Custom <code>node_modules</code> folders included in the zip file will be extracted in the runtime. Zip files are limited to a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">maximum size of 48MB</a>.</p>

<h4>Library Size</h4>

<p>Running <code>npm install</code> for the TensorFlow.js libraries used revealed the first problem... the resulting <code>node_modules</code> directory was 175MB. üò±</p>

<p>Looking at the contents of this folder, the <code>tfjs-node</code> module compiles a <a href="https://github.com/tensorflow/tfjs-node/tree/master/src">native shared library</a> (<code>libtensorflow.so</code>) that is 135M. This means no amount of JavaScript minification is going to get those external dependencies under the magic 48 MB limit. üëé</p>

<h4>Native Dependencies</h4>

<p>The <code>libtensorflow.so</code> native shared library must be compiled using the platform runtime. Running <code>npm install</code>  locally automatically compiles native dependencies against the host platform. Local environments may use different CPU architectures (Mac vs Linux) or link against shared libraries not available in the serverless runtime.</p>

<h3>MobileNet Model Files</h3>

<p>TensorFlow models files <a href="https://js.tensorflow.org/tutorials/model-save-load.html">need loading from the filesystem</a> in Node.js. Serverless runtimes do provide a temporary filesystem inside the runtime environment. Files from deployment zip files are automatically extracted into this environment before invocations. There is no external access to this filesystem outside the lifecycle of the serverless function.</p>

<p>Models files for the MobileNet model were 16MB. If these files are included in the deployment package, it leaves 32MB for the rest of the application source code. Although the model files are small enough to include in the zip file, what about the TensorFlow.js libraries? Is this the end of the blog post? Not so fast....</p>

<p><strong>Apache OpenWhisk's support for custom runtimes provides a simple solution to all these issues!</strong></p>

<h2>Custom Runtimes</h2>

<p>Apache OpenWhisk uses Docker containers as the runtime environments for serverless functions (actions). All platform runtime images are <a href="https://hub.docker.com/r/openwhisk/">published on Docker Hub</a>, allowing developers to start these environments locally.</p>

<p>Developers can also <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">specify custom runtime images</a> when creating actions. These images must be publicly available on Docker Hub. Custom runtimes have to expose the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">same HTTP API</a> used by the platform for invoking actions.</p>

<p>Using platform runtime images as <a href="https://docs.docker.com/glossary/?term=parent%20image">parent images</a> makes it simple to build custom runtimes. Users can run commands during the Docker build to install additional libraries and other dependencies. The parent image already contains source files with the HTTP API service handling platform requests.</p>

<h3>TensorFlow.js Runtime</h3>

<p>Here is the Docker build file for the Node.js action runtime with additional TensorFlow.js dependencies.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs @tensorflow-models/mobilenet @tensorflow/tfjs-node jpeg-js</p>

<p>COPY mobilenet mobilenet
```</p>

<p><code>openwhisk/action-nodejs-v8:latest</code> is the Node.js action runtime image <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">published by OpenWhisk</a>.</p>

<p>TensorFlow libraries and other dependencies are installed using <code>npm install</code> in the build process. Native dependencies for the <code>@tensorflow/tfjs-node</code> library are automatically compiled for the correct platform by installing during the build process.</p>

<p>Since I'm building a new runtime, I've also added the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model files</a> to the image. Whilst not strictly necessary, removing them from the action zip file reduces deployment times.</p>

<p><strong><em>Want to skip the next step? Use this image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/"><code>jamesthomas/action-nodejs-v8:tfjs</code></a> rather than building your own.</em></strong></p>

<h3>Building The Runtime</h3>

<p><em>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to download model files from the public storage bucket.</em></p>

<ul>
<li>Download a version of the MobileNet model and place all files in the <code>mobilenet</code> directory.</li>
<li>Copy the Docker build file from above to a local file named <code>Dockerfile</code>.</li>
<li>Run the Docker <a href="https://docs.docker.com/engine/reference/commandline/build/">build command</a> to generate a local image.</li>
</ul>


<p><code>sh
docker build -t tfjs .
</code></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/tag/">Tag the local image</a> with a remote username and repository.</li>
</ul>


<p><code>sh
docker tag tfjs &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p><em>Replace <code>&lt;USERNAME&gt;</code> with your Docker Hub username.</em></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/push/">Push the local image</a> to Docker Hub</li>
</ul>


<p><code>sh
 docker push &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p>Once the image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/">is available</a> on Docker Hub, actions can be created using that runtime image. üòé</p>

<h2>Example Code</h2>

<p>This source code implements image classification as an OpenWhisk action. Image files are provided as a Base64 encoded string using the <code>image</code> property on the event parameters. Classification results are returned as the <code>results</code> property in the response.</p>

<script src="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5.js"></script>


<h3>Caching Loaded Models</h3>

<p>Serverless platforms initialise runtime environments on-demand to handle invocations. Once a runtime environment has been created, it will be <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">re-used for further invocations</a> with some limits. This improves performance by removing the initialisation delay ("cold start") from request processing.</p>

<p>Applications can exploit this behaviour by using global variables to maintain state across requests. This is often use to <a href="https://blog.rowanudell.com/database-connections-in-lambda/">cache opened database connections</a> or store initialisation data loaded from external systems.</p>

<p>I have used this pattern to <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L80-L82">cache the MobileNet model</a> used for classification. During cold invocations, the model is loaded from the filesystem and stored in a global variable. Warm invocations then use the existence of that global variable to skip the model loading process with further requests.</p>

<p>Caching the model reduces the time (and therefore cost) for classifications on warm invocations.</p>

<h3>Memory Leak</h3>

<p>Running the Node.js script from blog post on IBM Cloud Functions was possible with minimal modifications. Unfortunately, performance testing revealed a memory leak in the handler function. üò¢</p>

<p><em>Reading more about <a href="https://js.tensorflow.org/tutorials/core-concepts.html">how TensorFlow.js works</a> on Node.js uncovered the issue...</em></p>

<p>TensorFlow.js's Node.js extensions use a native C++ library to execute the Tensors on a CPU or GPU engine. Memory allocated for Tensor objects in the native library is retained until the application explicitly releases it or the process exits. TensorFlow.js provides a <code>dispose</code> method on the individual objects to free allocated memory. There is also a <code>tf.tidy</code> method to automatically clean up all allocated objects within a frame.</p>

<p>Reviewing the code, tensors were being created as <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L51-L59">model input from images</a> on each request. These objects were not disposed before returning from the request handler. This meant native memory grew unbounded. Adding an explicit <code>dispose</code> call to free these objects before returning <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L91">fixed the issue</a>.</p>

<h3>Profiling &amp; Performance</h3>

<p>Action code records memory usage and elapsed time at different stages in classification process.</p>

<p>Recording <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L12-L20">memory usage</a> allows me to modify the maximum memory allocated to the function for optimal performance and cost. Node.js provides a <a href="https://nodejs.org/docs/v0.4.11/api/all.html#process.memoryUsage">standard library API</a> to retrieve memory usage for the current process. Logging these values allows me to inspect memory usage at different stages.</p>

<p>Timing <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L71">different tasks</a> in the classification process, i.e. model loading, image classification, gives me an insight into how efficient classification is compared to other methods. Node.js has a <a href="https://nodejs.org/api/console.html#console_console_time_label">standard library API</a> for timers to record and print elapsed time to the console.</p>

<h2>Demo</h2>

<h3>Deploy Action</h3>

<ul>
<li>Run the following command with the <a href="https://console.bluemix.net/openwhisk/learn/cli">IBM Cloud CLI</a> to create the action.</li>
</ul>


<p><code>sh
ibmcloud fn action create classify --docker &lt;IMAGE_NAME&gt; index.js
</code></p>

<p><em>Replace <code>&lt;IMAGE_NAME&gt;</code> with the public Docker Hub image identifier for the custom runtime. Use <code>jamesthomas/action-nodejs-v8:tfjs</code> if you haven't built this manually.</em></p>

<h3>Testing It Out</h3>

<ul>
<li>Download <a href="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG">this image</a> of a Panda from Wikipedia.</li>
</ul>


<p>{% img https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG %}</p>

<p><code>sh
wget http://bit.ly/2JYSal9 -O panda.jpg
</code></p>

<ul>
<li>Invoke the action with the Base64 encoded image as an input parameter.</li>
</ul>


<p><code>sh
 ibmcloud fn action invoke classify -r -p image $(base64 panda.jpg)
</code></p>

<ul>
<li>Returned JSON message contains classification probabilities. üêºüêºüêº</li>
</ul>


<p>```json
{
  "results":  [{</p>

<pre><code>className: 'giant panda, panda, panda bear, coon bear',
probability: 0.9993536472320557
</code></pre>

<p>  }]
}
```</p>

<h3>Activation Details</h3>

<ul>
<li>Retrieve logging output for the last activation to show performance data.</li>
</ul>


<p><code>sh
ibmcloud fn activation logs --last
</code></p>

<p><strong><em>Profiling and memory usage details are logged to stdout</em></strong></p>

<p><code>sh
prediction function called.
memory used: rss=150.46 MB, heapTotal=32.83 MB, heapUsed=20.29 MB, external=67.6 MB
loading image and model...
decodeImage: 74.233ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=40.63 MB
imageByteArray: 5.676ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=45.51 MB
imageToInput: 5.952ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.06 MB, external=45.51 MB
mn_model.classify: 274.805ms
memory used: rss=149.83 MB, heapTotal=24.33 MB, heapUsed=20.57 MB, external=45.51 MB
classification results: [...]
main: 356.639ms
memory used: rss=144.37 MB, heapTotal=24.33 MB, heapUsed=20.58 MB, external=45.51 MB
</code></p>

<p><code>main</code> is the total elapsed time for the action handler. <code>mn_model.classify</code> is the elapsed time for the image classification. Cold start requests print an extra log message with model loading time, <code>loadModel: 394.547ms</code>.</p>

<h2>Performance Results</h2>

<p>Invoking the <code>classify</code> action 1000 times for both cold and warm activations (using 256MB memory) generated the following performance results.</p>

<h3>warm invocations</h3>

<p>{% img /images/tfjs-serverless/warm-activations.png Warm Activation Performance Results %}</p>

<p>Classifications took an average of <strong>316 milliseconds to process when using warm environments</strong>. Looking at the timing data, converting the Base64 encoded JPEG into the input tensor took around 100 milliseconds. Running the model classification task was in the 200 - 250 milliseconds range.</p>

<h3>cold invocations</h3>

<p>{% img /images/tfjs-serverless/cold-activations.png Cold Activation Performance Results %}</p>

<p>Classifications took an average of <strong>1260 milliseconds to process when using cold environments</strong>. These requests incur penalties for initialising new runtime containers and loading models from the filesystem. Both of these tasks took around 400 milliseconds each.</p>

<p><em>One disadvantage of using custom runtime images in Apache OpenWhisk is the lack of <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">pre-warmed containers</a>. Pre-warming is used to reduce cold start times by starting runtime containers before they are needed. This is not supported for non-standard runtime images.</em></p>

<h3>classification cost</h3>

<p>IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">provides a free tier</a> of 400,000 GB/s per month. Each further second of execution is charged at $0.000017 per GB of memory allocated. Execution time is rounded up to the nearest 100ms.</p>

<p>If all activations were warm, a user could execute <strong>more than 4,000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 600,000 further invocations would cost just over $1.</p>

<p>If all activations were cold, a user could execute <strong>more than 1,2000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 180,000 further invocations would cost just over $1.</p>

<h2>Conclusion</h2>

<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>

<p>Getting a local script to run image classification was relatively simple, but converting to a serverless function came with more challenges! Apache OpenWhisk restricts the maximum application size to 50MB and native libraries dependencies were much larger than this limit.</p>

<p>Fortunately, Apache OpenWhisk's custom runtime support allowed us to resolve all these issues. By building a custom runtime with native dependencies and models files, those libraries can be used on the platform without including them in the deployment package.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Monitoring Dashboards With Kibana For IBM Cloud Functions]]></title>
    <link href="http://jamesthom.as/blog/2018/07/18/monitoring-dashboards-with-kibana-for-ibm-cloud-functions/"/>
    <updated>2018-07-18T16:08:00+01:00</updated>
    <id>http://jamesthom.as/blog/2018/07/18/monitoring-dashboards-with-kibana-for-ibm-cloud-functions</id>
    <content type="html"><![CDATA[<p>Following all the events from the World Cup can be hard. So many matches, so many goals. Rather than manually refreshing BBC Football to check the scores, I decided to created a <a href="https://twitter.com/WC2018_Goals">Twitter bot</a> that would automatically tweet out each goal.</p>

<p>{% img /images/monitoring-goalbot/wcgoalbot.png World Cup Goal Bot %}</p>

<p><a href="https://github.com/jthomas/goalbot">The Twitter bot</a> runs on <a href="https://console.bluemix.net/openwhisk">IBM Cloud Functions</a>. It is called once a minute to check for new goals, using the <a href="https://github.com/apache/incubator-openwhisk-package-alarms">alarm trigger feed</a>. If new goals have been scored, it calls another action to send the tweet messages.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">‚öΩÔ∏è GOAL ‚öΩÔ∏è<br>üë® Harry MAGUIRE (Û†Åøüè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø ) @ 30&#39;. üë®<br>üèü Sweden üá∏üá™ (0) v England Û†Åøüè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø (1) üèü<a href="https://twitter.com/hashtag/WorldCup?src=hash&amp;ref_src=twsrc%5Etfw">#WorldCup</a></p>&mdash; WC 2018 Goal Bot (@WC2018_Goals) <a href="https://twitter.com/WC2018_Goals/status/1015604110006120448?ref_src=twsrc%5Etfw">July 7, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Once it was running, I need to ensure it was working correctly for the duration of the tournament. Using the <a href="https://console.bluemix.net/catalog/services/log-analysis">IBM Cloud Logging</a> service, I built a custom monitoring dashboard to help to me recognise and diagnose issues.</p>

<p>{% img /images/monitoring-goalbot/dashboard-overview.png Monitoring Dashboard %}</p>

<p>The dashboard showed counts for successful and failed activations, when they occurred and a list of failed activations. If issues have occurred, I can retrieve the failed activation identifiers and investigate further.</p>

<p><em>Let's walk through the steps used to create this dashboard to help you create custom visualisations for serverless applications running on IBM Cloud Functions...</em></p>

<h2>IBM Cloud Logging</h2>

<p><a href="https://console.bluemix.net/docs/services/CloudLogAnalysis/index.html">IBM Cloud Logging</a> can be accessed <a href="https://console.bluemix.net/docs/openwhisk/openwhisk_logs.html#openwhisk_logs">using the link</a> on the IBM Cloud Functions dashboard. This will open the logging service for the current organisation and space.</p>

<p>{% img /images/monitoring-goalbot/open-logging.gif Opening Logging Service %}</p>

<p>All activation records and application logs are automatically forwarded to the logging service by IBM Cloud Functions.</p>

<p>{% img /images/monitoring-goalbot/discover.png Kibana Discover Screen %}</p>

<h3>Log Message Fields</h3>

<p>Activation records and application log messages have a number of common record fields.</p>

<ul>
<li><code>activationId_str</code> - <em>activation identifier for log message.</em></li>
<li><code>timestamp</code> - <em>log draining time.</em></li>
<li><code>@timestamp</code> - <em>message ingestion time.</em></li>
<li><code>action_str</code> - <em>fully qualified action name</em></li>
</ul>


<p>Log records for different message types are identified using the <code>type</code> field. This is either <code>activation_record</code> or <code>user_logs</code> for IBM Cloud Functions records.</p>

<p>Activation records have the following custom fields.</p>

<ul>
<li><code>duration_int</code> - <em>activation duration in milliseconds</em></li>
<li><code>status_str</code> - <em>activation status response (non-zero for errors)</em></li>
<li><code>message</code> - <em>activation response returned from action</em></li>
<li><code>time_date</code> - <em>activation record start time</em></li>
<li><code>end_date</code> - <em>activation record end time</em></li>
</ul>


<p>Applications log lines, written to stdout or stderr, are forwarded as individual records. One application log line per record. Log message records have the following custom fields.</p>

<ul>
<li><code>message</code> - <em>single application log line output</em></li>
<li><code>stream_str</code> - <em>log message source, either <code>stdout</code> or <code>stderr</code></em></li>
<li><code>time_date</code> - <em>timestamp parsed from application log line</em></li>
</ul>


<h3>Finding Log Messages For One Activation</h3>

<p>Use this query string in the "<a href="https://www.elastic.co/guide/en/kibana/current/discover.html"><em>Discover</em>"</a> tab to retrieve all logs messages from a particular activation.</p>

<p><code>
activationId_str: &lt;ACTIVATION_ID&gt;
</code></p>

<p>Search queries are executed against log records within a <a href="https://www.elastic.co/guide/en/kibana/current/set-time-filter.html">configurable time window</a>.</p>

<h2>Monitoring Dashboard</h2>

<p>{% img /images/monitoring-goalbot/dashboard-overview.png Monitoring Dashboard %}</p>

<p>This is the monitoring dashboard I created. It contains visualisations showing counts for successful and failed activations, histograms of when they occurred and a list of the recent failed activation identifiers.</p>

<p>It allows me to quickly review the previous 24 hours activations for issues. If there are notable issues, I can retrieve the failed activation identifiers and investigate further.</p>

<p>Before being able to create the dashboard, I needed to define two resources: <strong><em>saved searches</em></strong> and <strong><em>visualisations</em></strong>.</p>

<h3>Saved Searches</h3>

<p>{% img /images/monitoring-goalbot/saved-searches.png Saving Search Queries %}</p>

<p>Kibana supports saving and referring to search queries from visualisations using explicit names.</p>

<p>Using <a href="https://www.elastic.co/guide/en/kibana/current/managing-saved-objects.html">saved searches</a> with visualisations, rather than explicit queries, removes the need to manually update visualisations' configuration when queries change.</p>

<p>This dashboard uses two custom queries in visualisations. Queries are needed to find activation records from both successful and failed invocations.</p>

<ul>
<li>Create a new <em>"Saved Search"</em> named <em>"activation records (success)"</em> using the following search query.</li>
</ul>


<p><code>
type: activation_record AND status_str: 0
</code></p>

<ul>
<li>Create a new <em>"Saved Search"</em> named <em>"activation records (failed)"</em> using the following search query.</li>
</ul>


<p><code>
type: activation_record AND NOT status_str: 0
</code></p>

<p><em>The <code>status_str</code> field is set to a non-zero value for failures. Using the <code>type</code> field ensures log messages from other sources are excluded from the results.</em></p>

<h3>Indexed Fields</h3>

<p>Before referencing log record fields in visualisations, those fields <a href="https://www.elastic.co/guide/en/kibana/current/index-patterns.html#reload-fields">need to be indexed</a> correctly. Use these instructions to verify activation records fields are available.</p>

<ul>
<li>Check IBM Cloud Functions logs are available in IBM Cloud Logging using the "<em>Discover</em>" tab.</li>
<li>Click the "‚öôÔ∏è <em>(Management)</em>" menu item on the left-hand drop-down menu in IBM Cloud Logging.</li>
<li>Click the "<em>Index Patterns</em>" link.</li>
<li>Click the üîÑ button to refresh the field list.</li>
</ul>


<p>{% img /images/monitoring-goalbot/refresh-fields.gif Refresh field index %}</p>

<h3>Visualisations</h3>

<p>Three types of <a href="https://www.elastic.co/guide/en/kibana/current/visualize.html">visualisation</a> are used on the monitoring dashboard. Metric displays are used for the activation counts, vertical bar charts for the activation times and a data table to list failed activations.</p>

<p><em>Visualisations <a href="https://www.elastic.co/guide/en/kibana/current/createvis.html">can be created</a> by opening the "Visualize" menu item and select a new visualisation type under the "Create New Visualization" menu.</em></p>

<p>Create five different visualisations, using the instructions below, before moving on to create the dashboard.</p>

<h4>Activation Counts</h4>

<p>Counts for successful and failed activations are displayed as singular <a href="https://www.elastic.co/guide/en/kibana/current/metric-chart.html">metric values</a>.</p>

<ul>
<li>Select the "Metric" visualisation from the visualisation type list.</li>
<li>Use the "activation records (success)" saved search as the data source.</li>
<li>Ensure the Metric Aggregation is set to "Count"</li>
<li>Set the "Font Size" under the Options menu to 120pt.</li>
<li>Save the visualisation as "Activation Counts (Success)"</li>
</ul>


<p>{% img /images/monitoring-goalbot/metrics-success.png Activation success metric %}</p>

<ul>
<li>Repeat this process to create the failed activation count visualisation.</li>
<li>Use the "activation records (failed)" saved search as the data source.</li>
<li>Save the visualisation as "Activation Counts (Failed)".</li>
</ul>


<p>{% img /images/monitoring-goalbot/metrics-fail.png Activation failed metric %}</p>

<h4>Activation Times</h4>

<p>Activation counts over time, for successful and failed invocations, are displayed in <a href="https://www.elastic.co/guide/en/kibana/current/xy-chart.html">vertical bar charts</a>.</p>

<ul>
<li>Select the "Vertical bar chart" visualisation from the visualisation type list.</li>
<li>Use the "activation records (success)" saved search as the data source.</li>
<li>Set the "Custom Label" to Invocations</li>
<li>Add an "X-Axis" bucket type under the Buckets section.</li>
<li>Choose "Date Histogram" for the aggregation, "@timestamp" for the field and "Minute" for the interval.</li>
<li>Save the visualisation as "Activation Times (Success)"</li>
</ul>


<p>{% img /images/monitoring-goalbot/activation-times.png Activation times chart %}</p>

<ul>
<li>Repeat this process to create the failed activation times visualisation.</li>
<li>Use the "activation records (failed)" saved search as the data source.</li>
<li>Save the visualisation as "Activation Times (Failed)"</li>
</ul>


<h4>Failed Activations List</h4>

<p>Activation identifiers for failed invocations are shown using a <a href="https://www.elastic.co/guide/en/kibana/current/data-table.html">data table</a>.</p>

<ul>
<li>Select the "Data table" visualisation from the visualisation type list.</li>
<li>Use the "activation records (failed)" saved search as the data source.</li>
<li>Add a "Split Rows" bucket type under the Buckets section.</li>
<li>Choose "Date Histogram" for the aggregation, "@timestamp" for the field and "Second" for the interval.</li>
<li>Add a "sub-bucket" with the "Split Rows" type.</li>
<li>Set sub aggregation to "Terms", field to "activationId_str" and order by "Term".</li>
<li>Save the visualisation as "Errors Table"</li>
</ul>


<p>{% img /images/monitoring-goalbot/activations-table.png Failed activation table %}</p>

<h3>Creating the dashboard</h3>

<p>Having created the individual visualisations components, the <a href="https://www.elastic.co/guide/en/kibana/current/dashboard.html">monitoring dashboard</a> can be constructed.</p>

<ul>
<li>Click the "Dashboard" menu item from the left-and menu panel.</li>
<li>Click the "Add" button to import visualisations into the current dashboard.</li>
<li>Add each of the five visualisations created above.</li>
</ul>


<p><em>Hovering the mouse cursor over visualisations will reveal icons for moving and re-sizing.</em></p>

<ul>
<li>Re-order the visualisations into the following rows:

<ul>
<li>Activations Metrics</li>
<li>Activation Times</li>
<li>Failed Activations List</li>
</ul>
</li>
<li>Select the "Last 24 hours" time window, available from the relative time ranges menu.</li>
<li>Save the dashboard as "<em>Cloud Functions Monitoring</em>". Tick the "<em>store time with dashboard</em>" option.</li>
</ul>


<p>{% img /images/monitoring-goalbot/dashboard-overview.png Monitoring Dashboard %}</p>

<p>Having saved the dashboard with time window, re-opening the dashboard will show our visualisations with data for the previous 24 hours. This dashboard can be used to quickly review recent application issues.</p>

<h2>Conclusion</h2>

<p>Monitoring serverless applications is crucial to diagnosing issues on serverless platforms.</p>

<p><a href="https://console.bluemix.net/openwhisk/dashboard">IBM Cloud Functions</a> provides automatic integration with the <a href="https://console.bluemix.net/catalog/services/log-analysis">IBM Cloud Logging</a> service. All activation records and application logs from serverless applications are automatically forwarded as log records. This makes it simple to build custom monitoring dashboards using these records for serverless applications running on IBM Cloud Functions.</p>

<p>Using this service with World Cup Twitter bot allowed me to easily monitor the application for issues. This was much easier than manually retrieving and reviewing activation records using the CLI!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Debugging Node.js OpenWhisk Actions]]></title>
    <link href="http://jamesthom.as/blog/2018/07/10/debugging-node-dot-js-openwhisk-actions/"/>
    <updated>2018-07-10T09:00:00+01:00</updated>
    <id>http://jamesthom.as/blog/2018/07/10/debugging-node-dot-js-openwhisk-actions</id>
    <content type="html"><![CDATA[<p>Debugging serverless applications is one of the <a href="https://www.stackery.io/blog/the-serverless-learning-curve/">most challenging issues</a> developers face when using serverless platforms. How can you use debugging tools without any access to the runtime environment?</p>

<p>Last week, I worked out <a href="https://twitter.com/thomasj/status/1013006648439443458">how to expose the Node.js debugger</a> in the Docker environment used for the application runtime in Apache OpenWhisk.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Want to use Node.js debugger for <a href="https://twitter.com/openwhisk?ref_src=twsrc%5Etfw">@openwhisk</a> actions? Start runtime container locally with this command to expose v8 inspector.<br>$ docker run -p 8080:8080 -p 9229:9229 -it openwhisk/action-nodejs-v8 node --inspect=0.0.0.0:9229 app.js<br>Then connect Chrome Dev Tools or <a href="https://twitter.com/code?ref_src=twsrc%5Etfw">@code</a>. üíØ <a href="https://t.co/X4i01QEOmg">pic.twitter.com/X4i01QEOmg</a></p>&mdash; James Thomas (@thomasj) <a href="https://twitter.com/thomasj/status/1013006648439443458?ref_src=twsrc%5Etfw">June 30, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Using the remote debugging service, we can set breakpoints and step through action handlers live, rather than just being reliant on logs and metrics to diagnose bugs.</p>

<p><strong>So, how does this work?</strong></p>

<p><em>Let's find out more about how Apache OpenWhisk executes serverless functions...</em></p>

<h2>Background</h2>

<p><a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> is the open-source serverless platform which powers <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>. OpenWhisk <a href="https://medium.com/openwhisk/uncovering-the-magic-how-serverless-platforms-really-work-3cb127b05f71">uses Docker containers</a> to create isolated runtime environments for executing serverless functions.</p>

<p>Containers are started on-demand as invocation requests arrive. Serverless function source files are dynamically injected into the runtime and executed for each invocation. Between invocations, containers are paused and kept in a cache for re-use with further invocations.</p>

<p>The benefit of using an open-source serverless platform is that the <a href="https://github.com/search?q=incubator-openwhisk-runtime">build files</a> used to create runtime images are also open-source. OpenWhisk also automatically builds and publishes all <a href="https://hub.docker.com/r/openwhisk/">runtime images externally</a> on Docker Hub. Running containers using these images allows us to simulate the remote serverless runtime environment.</p>

<h3>Runtime Images</h3>

<p>All OpenWhisk runtime images are <a href="https://hub.docker.com/r/openwhisk/">published externally</a> on Docker Hub.</p>

<p>Runtime images <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">start a HTTP server</a> which listens on port 8080. This HTTP server must implement two API endpoints (<code>/init</code> &amp; <code>/run</code>) accepting HTTP POST requests. The platform uses these endpoints to initialise the runtime with action code and then invoke the action with event parameters.</p>

<p>More details on the API endpoints can be found in this <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">blog post</a> on creating Docker-based actions.</p>

<h3>Node.js Runtime Image</h3>

<p>This repository contains the source code used to create <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">Node.js runtime environment image</a>.</p>

<p><a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">https://github.com/apache/incubator-openwhisk-runtime-nodejs</a></p>

<p>Both <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejs8Action">Node.js 8</a> and <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejs6Action">6 runtimes</a>  are built from a <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejsActionBase">common base image</a>. This base image contains an <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/app.js">Express.js server</a> which handles the platform API requests. The <code>app.js</code> file containing the server <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejs8Action/Dockerfile#L28">is executed</a> when the containers starts.</p>

<p>JavaScript code is injected into the runtime using the <code>/init</code> API. Actions created from source code are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L76">dynamically evaluated</a> to instantiate the code in the runtime. Actions created from zip files are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L54">extracted into a temporary directory</a> and <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61">imported as a Node.js module</a>.</p>

<p>Once instantiated, actions are executed using the <code>/run</code> API. Event parameters are come from the request body. Each time a new request is received, the server <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L95">calls the action handler with event parameters</a>. Returned values are serialised as the JSON body in the API response.</p>

<h3>Starting Node.js Runtime Containers</h3>

<p><a href="https://docs.docker.com/engine/reference/commandline/run/">Use this command</a> to start the Node.js runtime container locally.</p>

<p><code>
$ docker run -it -p 8080:8080 openwhisk/action-nodejs-v8
</code></p>

<p>Once the container has started, port 8080 on localhost will be mapped to the HTTP service exposed by the runtime environment. This can be used to inject serverless applications into the runtime environment and invoke the serverless function handler with event parameters.</p>

<h2>Node.js Remote Debugging</h2>

<p>Modern versions of the Node.js runtime have a command-line flag (<code>--inspect</code>) to expose a <a href="https://nodejs.org/api/debugger.html#debugger_advanced_usage">remote debugging service</a>. This service runs a WebSocket server on localhost which implements the <a href="https://chromedevtools.github.io/devtools-protocol/">Chrome DevTools Protocol</a>.</p>

<p><code>
$ node --inspect index.js
Debugger listening on 127.0.0.1:9229.
</code></p>

<p>External tools can connect to this port to provide debugging capabilities for Node.js code.</p>

<p>Docker images for the OpenWhisk Node.js runtimes use the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejs8Action/Dockerfile#L28">following command</a> to start the internal Node.js process. <strong>Remote debugging is not enabled by default.</strong></p>

<p><code>
node --expose-gc app.js
</code></p>

<p>Docker allows containers to override the default image start command using a <a href="https://docs.docker.com/engine/reference/run/">command line argument</a>.</p>

<p><strong>This command will start the OpenWhisk Node.js runtime container with the remote debugging service enabled.</strong> Binding the HTTP API and WebSocket ports to the host machine allows us to access those services remotely.</p>

<p><code>
docker run -p 8080:8080 -p 9229:9229 -it openwhisk/action-nodejs-v8 node --inspect=0.0.0.0:9229 app.js
</code></p>

<p><em>Once a container from the runtime image has started, we can connect our favourite debugging tools...</em></p>

<h3>Chrome Dev Tools</h3>

<p>To connect <a href="https://developers.google.com/web/tools/chrome-devtools/">Chrome Dev Tools</a> to the remote Node.js debugging service, follow these steps.</p>

<ul>
<li>Open the following page in Chrome: <a href="chrome://inspect/#devices">chrome://inspect/#devices</a></li>
</ul>


<p>{% img /images/debugging/devtools.png Chrome Dev Tools %}</p>

<p>Chrome Dev Tools is configured to open a connection on port 9229 on localhost. If the web socket connection succeeds, the debugging target should be listed in the "Remote Target" section.</p>

<ul>
<li>Click the "<em>Open dedicated DevTools for Node</em>" link.</li>
</ul>


<p>In the "Sources" panel the JavaScript files loaded by the Node.js process are available.</p>

<p>{% img /images/debugging/devtools-debugging.png Chrome Dev Tools Debugging %}</p>

<p>Setting breakpoints in the <code>runner.js</code> file will allow you to halt execution for debugging upon invocations.</p>

<h3>VSCode</h3>

<p><a href="https://code.visualstudio.com/">Visual Studio Code</a> supports remote debugging of Node.js code using the Chrome Dev Tools protocol. Follow these steps to connect the editor to the remote debugging service.</p>

<ul>
<li>Click the menu item "<em>Debug -> Add Configuration</em>"</li>
<li>Select the "<em>Node.js: Attach to Remote Program</em>" from the Intellisense menu.</li>
<li>Edit the default configuration to have the following values.</li>
</ul>


<p><code>json
{
  "type": "node",
  "request": "attach",
  "name": "Attach to Remote",
  "address": "127.0.0.1",
  "port": 9229,
  "localRoot": "${workspaceFolder}"
}
</code></p>

<p>{% img /images/debugging/vscode.png Visual Studio Code %}</p>

<ul>
<li>Choose the new "<em>attach to remote</em>" debugging profile and click the Run button.</li>
</ul>


<p>The "<em>Loaded Scripts</em>" window will show all the JavaScript files loaded by the Node.js process.</p>

<p>{% img /images/debugging/vscode-debugging.png Visual Studio Code Debugging %}</p>

<p>Setting breakpoints in the <code>runner.js</code> file will allow you to halt execution for debugging upon invocations.</p>

<h3>Breakpoint Locations</h3>

<p>Here are some useful locations to set breakpoints to catch errors in your serverless functions for the OpenWhisk Node.js runtime environments.</p>

<h4>Initialisation Errors - Source Actions</h4>

<p>If you are creating OpenWhisk actions from JavaScript source files, the code is dynamically evaluated during  the <code>/init</code> request at <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L76">this location</a>. Putting a breakpoint here will allow you to catch errors thrown during that <code>eval()</code> call.</p>

<h4>Initialisation Errors - Binary Actions</h4>

<p>If you are creating OpenWhisk actions from a zip file containing JavaScript modules, <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L54">this location</a> is where the archive is extracted in the runtime filesystem. Putting a breakpoint here will catch errors from the extraction call and runtime checks for a valid JavaScript module.</p>

<p><a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61">This code</a> is where the JavaScript module is imported once it has been extracted. Putting a breakpoint here will catch errors thrown importing the module into the Node.js environment.</p>

<h4>Action Handler Errors</h4>

<p>For both source file and zipped module actions, <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L95">this location</a> is where the action handler is invoked on each <code>/run</code> request. Putting a breakpoint here will catch errors thrown from within action handlers.</p>

<h2>Invoking OpenWhisk Actions</h2>

<p>Once you have attached the debugger to the remote Node.js process, you need to send the API requests to simulate the platform invocations. Runtime containers use separate HTTP endpoints to import the action source code into the runtime environment (<code>/init</code>)  and then fire the invocation requests (<code>/run</code>).</p>

<h4>Generating Init Request Body - Source Files</h4>

<p>If you are creating OpenWhisk actions from JavaScript source files, send the following JSON body in the HTTP POST to the <code>/init</code> endpoint.</p>

<p>```json
{
  "value": {</p>

<pre><code>"main": "&lt;FUNCTION NAME IN SOURCE FILE&gt;",
"code": "&lt;INSERT SOURCE HERE&gt;"
</code></pre>

<p>  }
}
```</p>

<p><code>code</code> is the JavaScript source to be evaluated which contains the action handler. <code>main</code> is the function name in the source file used for the action handler.</p>

<p>Using the <code>jq</code> <a href="https://stedolan.github.io/jq/">command-line tool</a>, we can create the JSON body for the source code in <code>file.js</code>.</p>

<p><code>sh
$ cat file.js | jq -sR  '{value: {main: "main", code: .}}'
</code></p>

<h4>Generating Init Request Body - Zipped Modules</h4>

<p>If you are creating OpenWhisk actions from a zip file containing JavaScript modules, send the following JSON body in the HTTP POST to the <code>/init</code> endpoint.</p>

<p>```json
{
  "value": {</p>

<pre><code>"main": "&lt;FUNCTION NAME ON JS MODULE&gt;",
"code": "&lt;INSERT BASE64 ENCODED STRING FROM ZIP FILE HERE&gt;",
"binary": true
</code></pre>

<p>  }
}
```</p>

<p><code>code</code> must be a Base64 encoded string for the zip file. <code>main</code> is the function name returned in the imported JavaScript module to call as the action handler.</p>

<p>Using the <code>jq</code> <a href="https://stedolan.github.io/jq/">command-line tool</a>, we can create the JSON body for the zip file in <code>action.zip</code>.</p>

<p><code>sh
$ base64 action.zip | tr -d '\n' | jq -sR '{value: {main: "main", binary: true, code: .}}'
</code></p>

<h4>Sending Init Request</h4>

<p>The <a href="https://httpie.org/">HTTPie</a> tool makes it simple to send HTTP requests from the command-line.</p>

<p>Using this tool, the following command will initialise the runtime container with an OpenWhisk action.</p>

<p>```sh
$ http post localhost:8080/init &lt; init.json
HTTP/1.1 200 OK
...
{</p>

<pre><code>"OK": true
</code></pre>

<p>}
```</p>

<p>If this HTTP request returns without an error, the action is ready to be invoked.</p>

<p><em>No further initialisation requests are needed unless you want to modify the action deployed.</em></p>

<h4>Generating Run Request Body</h4>

<p>Invocations of the action handler functions are triggered from a HTTP POST to the <code>/run</code> API endpoint.</p>

<p>Invocations parameters are sent in the JSON request body, using a JSON object with a <code>value</code> field.</p>

<p>```json
{
  "value": {</p>

<pre><code>"some-param-name": "some-param-value",
"another-param-name": "another-param-value",
</code></pre>

<p>  }
}
```</p>

<h4>Sending Run Request</h4>

<p>Using the <a href="https://httpie.org/">HTTPie</a> tool, the following command will invoke the OpenWhisk action.</p>

<p>```sh
$ http post localhost:8080/run &lt; run.json
HTTP/1.1 200 OK
...
{</p>

<pre><code>"msg": "Hello world"
</code></pre>

<p>}
```</p>

<p>Returned values from the action handler are serialised as the JSON body in the HTTP response. Issuing further HTTP POST requests to the <code>/run</code> endpoint allows us to re-invoke the action.</p>

<h2>Conclusion</h2>

<p>Lack of debugging tools is one of the biggest complaints from developers migrating to serverless platforms.</p>

<p>Using an open-source serverless platform helps with this problem, by making it simple to run the same containers locally that are used for the platform's runtime environments. Debugging tools can then be started from inside these local environments to simulate remote access.</p>

<p>In this example, this approach was used to enable the remote debugging service from the OpenWhisk Node.js runtime environment. The same approach could be used for any language and debugging tool needing local access to the runtime environment.</p>

<p>Having access to the Node.js debugger is huge improvement when debugging challenging issues, rather than just being reliant on logs and metrics collected by the platform.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Binding IAM Services To IBM Cloud Functions]]></title>
    <link href="http://jamesthom.as/blog/2018/06/05/binding-iam-services-to-ibm-cloud-functions/"/>
    <updated>2018-06-05T09:47:00+01:00</updated>
    <id>http://jamesthom.as/blog/2018/06/05/binding-iam-services-to-ibm-cloud-functions</id>
    <content type="html"><![CDATA[<p><a href="https://console.bluemix.net/docs/openwhisk/binding_services.html#binding_services">Binding service credentials</a> to actions and packages is a much better approach to handling authentication credentials in <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>, than manually updating (and maintaining) <a href="https://console.bluemix.net/docs/openwhisk/parameters.html#default-params-action">default parameters</a> üîê.</p>

<p>IBM Cloud Functions supports binding credentials from <a href="https://console.bluemix.net/docs/iam/index.html#iamoverview">IAM-based</a> and <a href="https://console.bluemix.net/docs/iam/cfaccess.html#cfaccess">Cloud Foundry provisioned</a> services.</p>

<p><a href="https://console.bluemix.net/docs/openwhisk/binding_services.html#binding_services">Documentation</a> and <a href="https://lornajane.net/posts/2018/bind-services-to-openwhisk-packages">blog posts</a> demonstrating service binding focuses on traditional platform services, created using the Cloud Foundry service broker. As IBM Cloud integrates IAM across the platform, more platform services will migrate to use the IAM service for managing authentication credentials.</p>

<p>{% blockquote %}</p>

<p>How do we bind credentials for IAM-based services to IBM Cloud Functions? ü§î</p>

<p>{% endblockquote %}</p>

<p>Binding IAM-based services to IBM Cloud Functions works the same as traditional platform services, but has some differences in how to retrieve details needed for the <code>service bind</code> command.</p>

<p><em>Let's look at how this works...</em></p>

<h2>Binding IAM Credentials</h2>

<h3>Requirements</h3>

<p>Before binding an IAM-based service to IBM Cloud Functions, the following conditions must be met.</p>

<ul>
<li><a href="https://console.bluemix.net/docs/overview/ui.html">Service instance has been provisioned</a>.</li>
<li><a href="https://console.bluemix.net/docs/resources/service_credentials.html#service_credentials">Service credentials have been created for that instance</a>.</li>
</ul>


<p>You will need the following information to bind a service credentials.</p>

<ul>
<li>Service name.</li>
<li><em>(Optional)</em> Instance name.</li>
<li><em>(Optional)</em> Credentials identifier.</li>
</ul>


<h3>Using the CLI</h3>

<p>Use the <code>ibmcloud wsk service bind</code> command to <a href="https://console.bluemix.net/docs/openwhisk/binding_services.html#binding_services">bind service credentials</a> to actions or packages.</p>

<p><code>
bx wsk service bind &lt;SERVICE_NAME&gt; &lt;ACTION|PACKAGE&gt; --instance &lt;INSTANCE&gt; --keyname &lt;KEY&gt;
</code></p>

<p>This command supports the following (optional) flags: <code>--instance</code> and <code>--keyname</code>.</p>

<p><em>If the instance and/or key names are not specified, the CLI uses the first instance and credentials returned from the system for the service identifier.</em></p>

<h3>Accessing from actions</h3>

<p>Credentials are stored as <a href="https://console.bluemix.net/docs/openwhisk/parameters.html#default-params-action">default parameters</a> on the action or package.</p>

<p>The command uses a special parameter name (<code>__bx_creds</code>) to store all credentials. Individual service credentials are indexed using the service name.</p>

<p>```json
{
   "__bx_creds":{</p>

<pre><code>  "service-name":{
     "apikey":"&lt;API_KEY&gt;",
     ...
  }
</code></pre>

<p>   }
}
```</p>

<p>Default parameters are automatically merged into the request parameters during invocations.</p>

<h2>Common Questions</h2>

<h4>How can I tell whether a service instance uses IAM-based authentication?</h4>

<p>Running the <code>ibmcloud resource service-instances</code> command will return the IAM-based service instances provisioned.</p>

<p>Cloud Foundry provisioned services are available using a different command: <code>ibmcloud service list</code>.</p>

<p><em>Both service types can be bound using the CLI but the commands to retrieve the necessary details are different.</em></p>

<h4>How can I find the service name for an IAM-based service instance?</h4>

<p>Run the <code>ibmcloud resource service-instance &lt;INSTANCE_NAME&gt;</code> command.</p>

<p>Service names are shown as the <code>Service Name:</code> field value.</p>

<h4>How can I list available service credentials for an IAM-based service instance?</h4>

<p>Use the <code>ibmcloud resource service-keys --instance-name &lt;NAME&gt; </code> command.</p>

<p>Replace the <code>&lt;NAME&gt;</code> value with the service instance returned from the <code>ibmcloud service list</code> command.</p>

<h4>How can I manually retrieve IAM-based credentials for an instance?</h4>

<p>Use the <code>ibmcloud resource service-key &lt;CREDENTIALS_NAME&gt;</code> command.</p>

<p>Replace the <code>&lt;CREDENTIALS_NAME&gt;</code> value with credential names returned from the <code>ibmcloud service service-keys</code> command.</p>

<h4>How can I create new service credentials?</h4>

<p>Credentials can be created through the service management page on <a href="https://console.bluemix.net">IBM Cloud</a>.</p>

<p>You can also use the CLI to create credentials using the <code>ibmcloud resource service-key-create</code> command. This command needs a name for the credentials, IAM role and service instance identifier.</p>

<h2>Example - Cloud Object Storage</h2>

<p><em>Having explained how to bind IAM-based services to IBM Cloud Functions, let's look at an example....</em></p>

<p><a href="https://console.bluemix.net/catalog/services/cloud-object-storage">Cloud Object Storage</a> is the service used to <a href="http://jamesthom.as/blog/2018/05/31/using-cloud-object-storage-from-ibm-cloud-functions-node-dot-js/">manage files for serverless applications</a> on IBM Cloud. This service supports the newer IAM-based authentication service.</p>

<p><strong>Let's look at how to bind authentication credentials for an instance of this service to an action.</strong></p>

<p>Using the CLI, we can check an instance of this service is available...</p>

<p><code>sh
$ ibmcloud resource service-instances
Retrieving service instances in resource group default..
OK
Name                     Location   State    Type               Tags
my-cos-storage           global     active   service_instance
</code></p>

<p>In this example, we have a single instance of IBM Cloud Object Storage provisioned as <code>my-cos-storage</code>.</p>

<p>Retrieving instance details will show us the service name to use in the service binding command.</p>

<p>```sh
$ ibmcloud resource service-instance my-cos-storage
Retrieving service instance my-cos-storage in resource group default..
OK</p>

<p>Name:                  my-cos-storage
ID:                    crn:v1:bluemix:public:cloud-object-storage:global:<GUID>:
GUID:                  <GUID>
Location:              global
Service Name:          cloud-object-storage
Service Plan Name:     lite
Resource Group Name:   default
State:                 active
Type:                  service_instance
Tags:
```</p>

<p>The IBM Cloud Object Storage service name is <code>cloud-object-storage</code>.</p>

<p>Before we can bind service credentials, we need to verify service credentials are available for this instance.</p>

<p><code>
$ ibmcloud resource service-keys --instance-name my-cos-storage
Retrieving service keys in resource group default...
OK
Name                     State    Created At
serverless-credentials   active   Tue Jun  5 09:11:06 UTC 2018
</code></p>

<p>This instance has a single service key available, named <code>serverless-credentials</code>.</p>

<p>Retrieving the service key details shows us the API secret for this credential.</p>

<p>```
$ ibmcloud resource service-key serverless-credentials
Retrieving service key serverless-credentials in resource group default...
OK</p>

<p>Name:          serverless-credentials
ID:            <ID>
Created At:    Tue Jun  5 09:11:06 UTC 2018
State:         active
Credentials:</p>

<pre><code>           ...
           apikey:                   &lt;SECRET_API_KEY_VALUE&gt;
</code></pre>

<p>```</p>

<p><em><code>apikey</code> denotes the secret API key used to authenticate calls to the service API.</em></p>

<p>Having retrieved the service name, instance identifier and available credentials, we can use these values to bind credentials to an action.</p>

<p><code>
$ bx wsk service bind cloud-object-storage params --instance my-cos-storage --keyname serverless-credentials
Credentials 'serverless-credentials' from 'cloud-object-storage' service instance 'my-cos-storage' bound to 'params'.
</code></p>

<p>Retrieving action details shows default parameters bound to an action. These will now include the API key for the Cloud Object Storage service.</p>

<p>```
$ bx wsk action get params
ok: got action params
{
  ...
  "parameters": [{</p>

<pre><code>"key": "__bx_creds",
"value": {
  "cloud-object-storage": {
    "apikey": "&lt;API_KEY_SECRET&gt;",
    ...
  }
}
</code></pre>

<p>  }]
}
```</p>

<p>Under the <code>__bx_creds</code> default parameter, there is a <code>cloud-object-storage</code> property with the API key amongst other service credential values.</p>
]]></content>
  </entry>
  
</feed>
