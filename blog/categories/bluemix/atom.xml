<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: bluemix | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/bluemix/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2015-09-03T16:38:42+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[running one-off tasks in cloud foundry]]></title>
    <link href="http://jamesthom.as/blog/2015/09/01/running-one-off-tasks-in-cloud-foundry/"/>
    <updated>2015-09-01T16:07:00+01:00</updated>
    <id>http://jamesthom.as/blog/2015/09/01/running-one-off-tasks-in-cloud-foundry</id>
    <content type="html"><![CDATA[<p>Whether making changes to a database schema, bulk importing data to initialise
a database or setting up a connected service, there are often administrative
tasks that needed to be carried out for an application to run correctly.</p>

<p>These tasks usually need finishing before starting the application and should not be
executed more than once.</p>

<p>Previously, the <a href="https://github.com/cloudfoundry/cli">CF CLI</a> provided commands, <em>tunnel</em> and <em>console</em>, to help running
one-off tasks manually. These commands were
<a href="http://stackoverflow.com/questions/32332319/exposing-ports-502-and-1002-from-nodejs-using-bluemix/32333386#32333386">deprecated</a>
with the upgrade from <em>v5</em> to <em>v6</em>
to discourage <a href="http://martinfowler.com/bliki/SnowflakeServer.html">snowflake environments</a>.</p>

<p>It is still possible, with a bit of hacking, to run one-off tasks manually from the application
container.</p>

<p>A better way is to describe <em>tasks as code</em> and run them automatically during normal
deployments. This results in applications that can be recreated without
manual intervention.</p>

<p>We'll look at both options before introducing a new library, <a href="https://github.com/IBM-Bluemix/oneoff">oneoff</a>, that automates
running administration tasks for Node.js applications.</p>

<h2>Running Tasks Manually</h2>

<h2>Local Environment</h2>

<p>Rather than running administrative tasks from the application console, we can
run them from a local development environment by remotely connecting to
the bound services.</p>

<p>This will be dependent on the provisioned services allowing remote access.
Many "built-in" platform services, e.g. MySQL, Redis, do not allow this.</p>

<p>Third-party services generally do.</p>

<p>Using the <em>cf env</em> command we can list service credentials for an application.
These authentication details can often be used locally by connecting through a client
library running in a local development environment.</p>

<p>For example, to access a provisioned Cloudant instance locally, we can grab the credentials
and use with a Node.js client library.</p>

<p>``` sh
[15:48:22 ~/code/sample]$ cf env sample-demo-app
Getting env variables for app sample-demo-app in org james.thomas@uk.ibm.com / space dev as james.thomas@uk.ibm.com...
OK</p>

<p>System-Provided:
{
 "VCAP_SERVICES": {
  "cloudantNoSQLDB": [
   {</p>

<pre><code>"credentials": {
 "host": "1234-bluemix.cloudant.com",
 "password": "sample_password",
 "port": 443,
 "url": "https://1234-bluemix:sample_password@1234-bluemix.cloudant.com",
 "username": "1234-bluemix"
}
</code></pre>

<p>....</p>

<p>[15:48:22 ~/code/sample]$ cat connect.js
var Cloudant = require('cloudant');</p>

<p>var me = '1234-bluemix';
var password = 'sample_password';</p>

<p>// Initialize the library with my account.
var cloudant = Cloudant({account:me, password:password});</p>

<p>cloudant.db.list(function(err, allDbs) {
  console.log('All my databases: %s', allDbs.join(', '))
  // Run administrative tasks
});
[15:48:22 ~/code/sample]$ node connect.js
All my databases: example_db, jasons_stuff, scores
```</p>

<h2>Remote Environment</h2>

<p>When provisioned services don't allow external access, the
<a href="https://github.com/cloudfoundry-community/cf-ssh">cf-ssh</a> project creates SSH
access to application containers running within Cloud Foundry.</p>

<p><strong>How does this work?!</strong></p>

<p><blockquote><p>cf-ssh deploys a new Cloud Foundry application, containing the same bits as your target application, with the same bound services.<br/>This new application's container does not start your web application as per normal. Instead, it starts an outbound reverse SSH tunnel to a public proxy.<br/>The local cf-ssh client then launches an interactive ssh connect to the public proxy, which tunnels through to the application container.</p><footer><strong>Dr. Nic</strong> <cite><a href='https://blog.starkandwayne.com/2014/10/28/how-does-cf-ssh-get-you-an-ssh-session-into-cloud-foundry/'>blog.starkandwayne.com/2014/10/&hellip;</a></cite></footer></blockquote></p>

<p>See the explanation <a href="https://blog.starkandwayne.com/2014/10/28/how-does-cf-ssh-get-you-an-ssh-session-into-cloud-foundry/">here</a> for full details.</p>

<p>This approach will let you connect to services from within the Cloud Foundry platform environment.</p>

<p>This video from <a href="https://starkandwayne.com/">Stark &amp; Wayne's</a> <a href="http://drnicwilliams.com/">Dr. Nic</a> shows the command in action...</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/eWJCfAa1_x8" frameborder="0" allowfullscreen></iframe>


<h2>IBM Bluemix Console (Java and Node.js)</h2>

<p><em>This technique is only for the IBM Bluemix platform.</em></p>

<p>If you are deploying Node.js and Java applications on <a href="https://bluemix.net">IBM Bluemix</a>, the platform provides
the following tools to assist with <a href="https://www.ng.bluemix.net/docs/#manageapps/app_management.html#appmanagement">application management</a>.</p>

<ul>
<li><em>proxy</em>: Minimal application management that serves as a proxy between your application and Bluemix.</li>
<li><em>devconsole</em>: Enables the development console utility.</li>
<li><em>shell</em>: Enables a web-based shell.</li>
<li><em>trace</em>: (Node.js only) Dynamically set trace levels if your application is using log4js, ibmbluemix, or bunyan logging modules.</li>
<li><em>inspector</em>: (Node.js only) Enables node inspector debugger.</li>
<li><em>debug</em>: (Liberty only) Enables clients to establish a remote debugging session with the application.</li>
<li><em>jmx</em>: (Liberty only) Enables the JMX REST Connector to allow connections from remote JMX clients</li>
</ul>


<p>The tools are enabled by setting the environment variable (<em>BLUEMIX_APP_MGMT_ENABLE</em>) with the
desired utilities.</p>

<p><code>sh
$ cf set-env myApp BLUEMIX_APP_MGMT_ENABLE devconsole+shell+trace
</code></p>

<p>Applications must be restarted for the changes to take effect.</p>

<p>If we enable the <em>shell</em> utility, the following web-based console will be available at https://your-app-name.mybluemix.net/bluemix-debug/shell.</p>

<p><img src="https://developer.ibm.com/bluemix/wp-content/uploads/sites/20/2015/06/shell.jpg"></p>

<h2>Cloud Foundry Diego Runtime</h2>

<p><a href="http://www.activestate.com/blog/2014/09/cloud-foundry-diego-explained-onsi-fakhouri">Diego</a> is the next-generation
runtime that will power upcoming versions of Cloud Foundry. Diego will provide many benefits
over the existing runtime, e.g. Docker support, including enabling SSH access to containers without the workarounds needed above.</p>

<p><strong>Yay!</strong></p>

<p>Follow the instructions <a href="https://github.com/cloudfoundry-incubator/diego-design-notes/blob/master/ssh-access-and-policy.md">here</a>
for details on SSH access to applications running on the new runtime.</p>

<p><em>Access to this feature will be dependent on your Cloud Foundry provider migrating to the new runtime.</em></p>

<h2>Running Tasks Automatically </h2>

<p>Manually running one-off administrative tasks for Cloud Foundry applications is a <a href="http://martinfowler.com/bliki/SnowflakeServer.html">bad idea</a>.</p>

<p>It affects your ability to do continuous delivery and encourages snowflake environments.</p>

<p>Alternatively, defining <em>tasks as code</em> means they can run automatically during normal deployments.
No more manual steps are required to deploy applications.</p>

<p>There are <a href="http://flywaydb.org/">many</a> <a href="https://github.com/ruby/rake">different</a> <a href="https://github.com/seomoz/shovel">libraries</a>
for <a href="https://phinx.org/">every</a> <a href="https://github.com/mattes/migrate">language</a> to help you programmatically define, manage and run tasks.</p>

<p>With <em>tasks defined as code</em>, you need to configure your <a href="https://docs.cloudfoundry.org/devguide/deploy-apps/manifest.html">application manifest</a>
to run these automatically during deployments.</p>

<p>Cloud Foundry uses the <a href="https://docs.cloudfoundry.org/devguide/deploy-apps/manifest.html#start-commands"><em>command</em> parameter</a>,
set in the manifest or through the
command-line, to allow applications to specify a custom start command. We can
use this parameter to execute the task library command during deployment.</p>

<p>The Cloud Foundry documentation also details these approaches, with slightly different
implementations <a href="https://docs.cloudfoundry.org/devguide/services/migrate-db.html">here</a>
and specifically for Ruby developers <a href="https://docs.cloudfoundry.org/buildpacks/ruby/ruby-tips.html#rake">here</a>.</p>

<h2>Temporary Task Deploy</h2>

<p>For applications which only need occasional administrative tasks, it's often
easier to push a temporary deploy with a custom start command. This deploy
runs your tasks without then starting your application. Once the tasks have
completed, redeploy your application normally, destroying the task instance.</p>

<p>The following command will deploy a temporary instance for this purpose:</p>

<p><code>sh
$ cf push -c 'YOUR_TASK_LIB_COMMAND &amp;&amp; sleep infinity' -i 1 --no-route
</code></p>

<p>We're overriding the default start command, setting it to run the command for
our task library, e.g. rake db:migrate.</p>

<p>The <em>sleep infinity</em> command stops the application exiting once the task runner
has finished. If this happens, the platform will assume that application has
crashed and restart it.</p>

<p>Also, the task runner will not be binding to a port so
we need to use the <em>--no-route</em> argument to stop the platform assuming the
deploy has timed out.</p>

<p>Setting the deploy to a single instance stops the command being executed more than once.</p>

<p>Checking the logs to verify the task runner has finished correctly, we can now
redeploy our application. Using the <em>null</em> start command will force the platform to use the buildpack default
rather than our previous option.</p>

<p><code>sh
$ cf push -c 'null'
</code></p>

<h2>Running Tasks Before Startup</h2>

<p>If we're regularly running administrative tasks, we should incorporate the
task execution into our normal application startup. Once the
task command has finished successfully, we start the application as normal.</p>

<p>Applications may have multiple instances running, we need to ensure
the tasks are only executed by one instance.</p>

<p>The following custom start command will execute tasks during startup,
using the CF_INSTANCE_ID environment variable to enforce execution at most-once.</p>

<pre>
[ $CF_INSTANCE_INDEX -eq 0 ]] && node lib/tasks/runner.js; node app.js
</pre>


<p>With this approach, tasks will be automatically executed during regular deployments
without any manual intervention.</p>

<p><strong>Hurrah!</strong></p>

<h2>Managing tasks for Node.js applications</h2>

<p>If you're running Node.js applications on Cloud Foundry, <a href="https://github.com/IBM-Bluemix/oneoff">oneoff</a> is a task library that helps
you define <em>tasks as code</em> and integrates with the Cloud Foundry runtime. The module handles
all the complexities with automating tasks during deployments across multi-instance applications.</p>

<p><blockquote><p>oneoff provides the following features...</p></p><p><ul><br/><li>ensure tasks are completed before application startup</li><br/><li>coordinating app instances to ensure at-most once task execution</li><br/><li>automagically discovering tasks from the task directory</li><br/><li>dependency ordering, ensure task a completes before task b starts</li><br/><li>parallel task execution</li><br/><li>ignore completed tasks in future deployments</p></blockquote></li>
</ul>


<p>Check it out to help make writing <em>tasks as code</em> for Node.js applications much easier!</p>

<p>Full details on usage are available in the <a href="https://github.com/IBM-Bluemix/oneoff/blob/master/README.md">README</a>.</p>

<h2>Conclusion</h2>

<p>Running one-off tasks for application configuration is a normal part of any development project.</p>

<p>Carrying out these tasks manually used to be the norm, but with the devops movement we now prefer
automated configuration rather manual intervention. Relying on manual configuration steps to deploy applications restricts
our ability to implement continuous delivery.</p>

<p>Cloud Foundry is an opinionated platform, actively discouraging the creation of snowflake environments.</p>

<p>Whilst it is still possible to manually run administrative tasks, either by connecting to bound services locally or using
a remote console, it's preferable to describe our tasks as code and let the platform handle it.</p>

<p>Using custom start commands, we can deploy applications which run tasks automatically during their normal startup procedure.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GeoPix Live Photos]]></title>
    <link href="http://jamesthom.as/blog/2015/07/16/geopix-live-photos/"/>
    <updated>2015-07-16T13:26:00+01:00</updated>
    <id>http://jamesthom.as/blog/2015/07/16/geopix-live-photos</id>
    <content type="html"><![CDATA[<p><a href="http://www.tricedesigns.com/about/">Andrew Trice</a> wrote a great sample
application for <a href="bluemix.net">IBM Bluemix</a> called <a href="http://www.tricedesigns.com/2015/03/27/geopix-a-native-ios-app-powered-by-ibm-mobilefirst-for-bluemix/">GeoPix</a>.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/376h2yjnf6Q" frameborder="0" allowfullscreen></iframe>


<p><blockquote><p>GeoPix uses the IBM MobileFirst services to provide a native iOS application which allows users to capture images from their mobile phones, storing them on the local device with automatic syncing to the cloud when online.</p></p><p><p>Using a web application, the user can view their images over a map based upon their location when the photo was taken.</p><footer><strong>Andrew Trice</strong> <cite><a href='http://www.tricedesigns.com/2015/03/27/geopix-a-native-ios-app-powered-by-ibm-mobilefirst-for-bluemix/'>www.tricedesigns.com/2015/03/27/&hellip;</a></cite></footer></blockquote></p>

<p>I've been using the demonstration to highlight the <a href="https://console.ng.bluemix.net/solutions/mobilefirst">mobile capabilities</a> of IBM Bluemix and had an idea for an
enhancement...</p>

<p><strong><em>Could the web page update with new pictures without having to refresh the page?</em></strong></p>

<p>Looking at the <a href="https://github.com/IBM-Bluemix/MobileFirst-Offline-Apps">source code</a>, the web application
is a Node.js application using the <a href="http://leafletjs.com/">Leaflet</a> JavaScript library to create interactive
maps. Images captured from mobile devices are <a href="https://www.ng.bluemix.net/docs/services/data/index.html#replicate">synchronised</a>
to a remote <a href="http://couchdb.apache.org/">CouchDB</a> database. When the user visits the <a href="http://geopix-web.mybluemix.net">GeoPix</a> site, the application queries this database
for all mobile images and renders the HTML using the <a href="http://jade-lang.com/">Jade</a> templating language.</p>

<p>Adding support for live photos will require two new features...</p>

<ul>
<li><em>Triggering backend events when new photos are available</em></li>
<li><em>Sending these photos in real-time to the web page</em></li>
</ul>


<h2>Change Notifications Using CouchDB</h2>

<p>CouchDB comes with built-in support for listening to changes in a database, <a href="http://guide.couchdb.org/draft/notifications.html">change notifications</a>.
The <a href="http://docs.couchdb.org/en/latest/api/database/changes.html"><em>_changes</em> feed</a> for a database is an activity stream publishing all document modifications.</p>

<p>GeoPix uses the following CouchDB <a href="https://www.npmjs.com/package/cloudant">client library</a>, to interact with our database from NodeJS. This library provides an <a href="https://github.com/dscape/nano#nanodbfollowname-params-callback">API</a>
to start following database changes and register callbacks for updates.</p>

<p>Modifying our <a href="https://github.com/IBM-Bluemix/MobileFirst-Offline-Apps/blob/master/Node.js/app.js#L42-L51">application code</a>, upon connecting to the CouchDB database, we register a change notification
handler. We follow all changes that occur in the future (<em>since: "now"</em>) and include the full document contents
in the change event (<em>include_docs: true</em>).</p>

<p>``` javascript
Cloudant({account:credentials.username, password:credentials.password}, function(err, cloudant) {</p>

<pre><code>var geopix = cloudant.use(database);
var feed = geopix.follow({include_docs: true, since: "now"});

feed.on('change', function (change) {
  // ....we can now send this data to the web pages
});

feed.follow();
</code></pre>

<p>})
```</p>

<p><strong>Now, every time a user sync their local photos to the cloud, the registered callback will be executed.</strong></p>

<p><em>How do we send new photos to the web page over a real-time stream?</em></p>

<h2>Real-time Web with Socket.IO</h2>

<p>Introducing <a href="">Socket.IO</a>...</p>

<p><blockquote><p>Socket.IO enables real-time bidirectional event-based communication.<br/>It works on every platform, browser or device, focusing equally on reliability and speed.</p></blockquote></p>

<p>Sounds great!</p>

<p>By embedding this library into our application, we can open a real-time event stream between the server and client. This channel
will be used by the client to listen for new images and then update the page.</p>

<p>The library has great <a href="http://socket.io/docs/">documentation</a> and provides both <a href="http://socket.io/docs/server-api/">server</a> and <a href="http://socket.io/docs/client-api/">client</a> modules. It also integrates with <a href="http://expressjs.com">ExpressJS</a>, the web framework used in GeoPix.
Socket.IO can use either WebSocket or long-polling transport protocols.</p>

<p>Socket.IO supports running under ExpressJS with minimal configuration, here are the changes needed to start our real-time stream in GeoPix:</p>

<p>``` javascript
var express = require('express');
var app = express();
var server = require('http').Server(app);
var io = require('socket.io')(server);</p>

<p>// ...snipped out the app routes for express</p>

<p>io.on('connection', function (socket) {</p>

<pre><code>console.log('New Client WSS Connection.')
</code></pre>

<p>});</p>

<p>var port = (process.env.VCAP_APP_PORT || 3000);
server.listen(port);
```</p>

<p><em>When a document change event is fired, executing the handle we registered above, we want to send this data to all connected clients.</em></p>

<p>Using the <a href="http://socket.io/docs/server-api/#server#emit"><em>emit</em> call</a> from the server-side API will do this for us.</p>

<p>``` javascript
feed.on('change', function (change) {</p>

<pre><code>io.sockets.emit('image', change);
</code></pre>

<p>});
```</p>

<p><strong>Now we're sending changes to the clients, we need to modify the client-side to listen for events and update the page.</strong></p>

<p>Socket.IO provides a <a href="http://socket.io/download/">JavaScript client library</a> that exposes a simple API for listening to events from the server-side stream.
Once we've included the script tag pointing to the client library, we can register a callback for <em>image</em> events and update the DOM
with the new elements.</p>

<p>We're sending the full database document associated with each photo to the client. The raw image bytes are stored as an
<a href="https://wiki.apache.org/couchdb/HTTP_Document_API#Attachments">attachment</a>.</p>

<p>``` javascript
var socket = io(); // TIP: io() with no args does auto-discovery
socket.on('connect', function () {</p>

<pre><code>console.log('WSS Connected');

socket.on('image', function (image) { // TIP: you can avoid listening on `connect` and listen on events directly too!
    var attachment = Object.keys(image.doc._attachments)[0]
    var url = "/image/" + image.doc._id + "/" + attachment;
    add_new_image(url, image.doc.clientDate, 'latitude: '
        + image.doc.latitude + ', longitude: '
        + image.doc.longitude + ', altitude: '
        + image.doc.altitude);
});
</code></pre>

<p>});
```</p>

<p>...and that's it! Now our web pages will automatically update with new photos whenever the mobile application syncs with the cloud.</p>

<h2>CouchDB + Socket.IO = Real-time Awesome!</h2>

<p>Adding <em>real-time</em> photos to our application was amazingly simple by combining
CouchDB with Socket.IO.</p>

<p>CouchDB's <em>_changes</em> API provided an easy way to follow
all modifications to database documents in real-time. Socket.IO made the
configuration and management of real-time event streams between our server and
client straightforward.</p>

<p><em>With minimal code changes, we simply connected these two technologies to create
a real-time photo stream for our GeoPix application. <strong>Awesome</strong></em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AlchemyAPI &amp; Updated Watson Nodes for Node-RED]]></title>
    <link href="http://jamesthom.as/blog/2015/07/15/alchemyapi-and-updated-watson-nodes-for-node-red/"/>
    <updated>2015-07-15T16:07:00+01:00</updated>
    <id>http://jamesthom.as/blog/2015/07/15/alchemyapi-and-updated-watson-nodes-for-node-red</id>
    <content type="html"><![CDATA[<p><img src="/images/node-red-updates.png"></p>

<p>I've recently been working on a <a href="https://github.com/node-red/node-red-bluemix-nodes/commit/56007e60d3414da3eb5c1dac766b23bdd96dd149">number</a> of <a href="https://github.com/node-red/node-red-bluemix-nodes/commit/4d0bcfe34f5e107e3b9e0684cd26ae701f913253">updates</a> to the Node-RED <a href="https://github.com/node-red/node-red-bluemix-nodes">nodes</a> for the IBM Bluemix platform...</p>

<p>Highlights below:</p>

<h2>New AlchemyAPI Nodes</h2>

<p>There are two new nodes (Feature Extract and Image Extract) in the package, allowing users to call services from the AlchemyAPI platform.</p>

<ul>
<li><p><em>Feature Extract.</em> This node will analyse external URLs, HTML or text content with features for text-based analysis
from the AlchemyAPI service, e.g. keywords, sentiment, relationships, etc.</p></li>
<li><p><em>Image Analysis.</em> This node will analyse images, passed in as external URLs or raw image bytes, to extract faces, content and URLs.</p></li>
</ul>


<p>Configuration for each node is available through the node editor panel.</p>

<p>For full details on all the capabilities of the AlchemyAPI platform, please see their <a href="http://www.alchemyapi.com/api">documentation</a>.</p>

<h2>Updated IBM Watson Nodes</h2>

<p>With the <a href="https://developer.ibm.com/watson/blog/2015/07/06/ibm-watson-language-translation-and-speech-services-general-availability/">recent changes</a>
to the IBM Watson services, there were a number of changes needed to support the API changes. All the IBM Watson nodes now work with the GA versions
of the services.</p>

<p><strong>Users must ensure they are using GA versions of the service with the nodes. Details on migration steps are available on the IBM Watson
<a href="https://developer.ibm.com/watson/blog/2015/07/06/ibm-watson-language-translation-and-speech-services-general-availability/">blog post</a> about the updates.</strong></p>

<h2>Running Locally</h2>

<p>When running Node-RED on IBM Bluemix, credentials for the services bound to the application are automatically registered. Previously, running the nodes
outside of IBM Bluemix required complex configuration to register service credentials. With this release, users will
be prompted to input the service credentials in the node editor panel if the application isn't running on IBM Bluemix. Much easier!</p>

<p><strong>If you have questions or encounter issues, please ask over on <a href="http://stackoverflow.com/questions/tagged/node-red">Stackoverflow</a> or raise <a href="https://github.com/node-red/node-red-bluemix-nodes/issues">issues</a> in Github</strong></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Debugging Cloud Foundry Stack Issues]]></title>
    <link href="http://jamesthom.as/blog/2015/07/10/debugging-cloud-foundry-stack-issues/"/>
    <updated>2015-07-10T15:27:00+01:00</updated>
    <id>http://jamesthom.as/blog/2015/07/10/debugging-cloud-foundry-stack-issues</id>
    <content type="html"><![CDATA[<p>Recent <a href="https://groups.google.com/a/cloudfoundry.org/forum/#!topic/vcap-dev/gU7rpD8MSC4">changes</a> to the Cloud Foundry stacks supported by IBM Bluemix have led to a number of <a href="http://stackoverflow.com/questions/31057357/static-buildpack-deploy-now-failing-due-to-unsupported-stack">issues</a> <a href="http://stackoverflow.com/questions/31085626/fail-to-push-static-site-to-bluemix-using-third-party-buildpack/31089127#31089127">for</a> <a href="http://stackoverflow.com/questions/31268155/bluemix-libstdc-so-6-version-glibcxx-3-4-20-not-found/31288182#31288182">users</a>. I've helped users diagnose and fix issues
that have occurred due to a mistmatches between the platform stack, applications and the buildpack. Learning a number of techniques for helping to discover and resolve these
issues and I wanted to share them with everyone else.</p>

<p>Running on Cloud Foundry's <em>Platform-as-a-Service</em> solution, we take for granted that low-level concepts like operating systems are abstracted away from the developer.</p>

<p>However, when we
run into issues it can be necessary to jump into the weeds and find out what's going on under the hood...</p>

<h2>What are Cloud Foundry "stacks"?</h2>

<p>According to the <a href="https://docs.cloudfoundry.org/concepts/stacks.html">documentation</a>...</p>

<p><blockquote><p>A stack is a prebuilt root filesystem (rootfs) which works in tandem with a buildpack and is used to support running applications.</p><footer><strong>Cloud Foundry Concepts</strong> <cite><a href='https://docs.cloudfoundry.org/concepts/stacks.html'>docs.cloudfoundry.org/concepts/&hellip;</a></cite></footer></blockquote></p>

<p>Think of the <em>stack</em> as the underlying operating-system running your application. This will be combined with the buildpack to instantiate the runtime
environment.</p>

<p>Most users don't have to care which <em>stack</em> they are running on.</p>

<p>However, if your application needs a specific version of a system library or you want to verify a specific command line application is installed, you
may need to dig deeper...</p>

<h2>What "stacks" does my platform support?</h2>

<p>Using the Cloud Foundry CLI, issue the following command to see what <em>stacks</em> are available on the platform.</p>

<p>``` sh
[16:27:30 ~]$ cf stacks
Getting stacks in org james.thomas@uk.ibm.com / space dev as james.thomas@uk.ibm.com...
OK</p>

<p>name         description
lucid64      Ubuntu 10.04
seDEA        private
cflinuxfs2   Ubuntu 14.04.2 trusty
```</p>

<p>Stack information contains the unique name for each stack and the underlying operating system version.</p>

<h2>Which "stack" is my application running on?</h2>

<p>Since <a href="https://github.com/cloudfoundry/cli/releases/tag/v6.11.0">v6.11.0</a>, the <em>stack</em> for an application has been shown in the CLI application info output.</p>

<p>``` sh
[16:34:39 ~]$ cf app debug-testing
Showing health and status for app debug-testing in org james.thomas@uk.ibm.com / space dev as james.thomas@uk.ibm.com...
OK</p>

<p>requested state: started
instances: 1/1
usage: 512M x 1 instances
urls: debug-testing.mybluemix.net
last uploaded: Tue Jun 16 15:47:21 UTC 2015
stack: lucid64
buildpack: SDK for Node.js(TM)</p>

<pre><code> state     since                    cpu    memory           disk           details
</code></pre>

<h1>0   running   2015-06-30 08:53:57 PM   0.0%   242.5M of 512M   196.8M of 1G</h1>

<p>```</p>

<h2>How can I choose the "stack" my application runs on?</h2>

<p>Users can set the <em>stack</em> for an application using the <em>-s</em> command-line parameter during deployment.
The stack identifier should match one of the names shown in the output from the <em>cf stacks</em> command.</p>

<p><code>sh
$ cf push -s stack_identifier
</code></p>

<h2>How are the "stacks" defined?</h2>

<p>This <a href="https://github.com/cloudfoundry/stacks">Github repository</a> contains the source files for building the <em>stacks</em>. There's a
<a href="https://docs.docker.com/reference/builder/">Dockerfile</a> for the current <a href="https://github.com/cloudfoundry/stacks/blob/master/cflinuxfs2/Dockerfile">cflinuxfs2</a> stack
to build the image used in Cloud Foundry.</p>

<h2>How can I poke around inside a "stack" locally?</h2>

<p>Using Docker, we can easily pull down the same "base" operating system used for a specifc "stack" and run locally.</p>

<p>For the <em>cflinuxfs2</em> stack, we can pull down the <a href="http://releases.ubuntu.com/14.04/">Ubuntu Trusty</a> image and run a terminal inside it.</p>

<p><code>sh
$ docker pull ubuntu:trusty
$ docker run -i -t ubuntu:trusty /bin/bash
</code></p>

<h2>How can I easily migrate existing applications to a new stack?</h2>

<p>Rather than having to re-deploy each application separately, there's a great <a href="https://github.com/simonleung8/cli-stack-changer">CF CLI plugin</a> to automatically migrate all your applications from <em>lucid64</em> to <em>cflinuxfs2</em>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Making Logs Awesome - Elasticsearch in the Cloud using Docker]]></title>
    <link href="http://jamesthom.as/blog/2015/07/08/making-logs-awesome-with-elasticsearch-and-docker/"/>
    <updated>2015-07-08T10:34:00+01:00</updated>
    <id>http://jamesthom.as/blog/2015/07/08/making-logs-awesome-with-elasticsearch-and-docker</id>
    <content type="html"><![CDATA[<p><img src="/images/Logs.png"></p>

<h3><strong>Logs are boring.</strong></h3>

<p>It used to be the only time you'd be looking at your application logs was when something went wrong.</p>

<p>Logs filled up disk space until they rotated out of existence.</p>

<p>...but now businesses are increasingly focused on using data to <a href="http://www.slideshare.net/mikebrittain/metrics-driven-engineering-at-etsy">drive decisions</a>.</p>

<p><em>Which advert leads to the highest click-through rates?</em></p>

<p><em>How did that last website change affect user retention?</em></p>

<p><em>What customer devices should our website support?</em></p>

<p>Guess where the answers lie?</p>

<h4><strong>Logs.</strong></h4>

<p>Storing, processing and querying logs effectively is <a href="http://www.slideshare.net/mikebrittain/take-my-logs-please">helping businesses succeed</a>.</p>

<h2>Introducing the ELK (Elasticsearch, Logstash, Kibana) stack...</h2>

<p><img src="https://www.elastic.co/assets/blt48dcfa0db3efb772/BQIielHCAAAs2So.png"></p>

<p>Five years ago, <a href="https://www.elastic.co/products/elasticsearch">Elasticsearch</a>, an open-source full-text search engine, was
released. It's now the second most popular enterprise search engine.
Complementing this project were <a href="https://www.elastic.co/products/logstash">Logstash</a> and <a href="https://www.elastic.co/products/kibana">Kibana</a>.
Logstash was a log
processing pipeline that could normalize streaming logs into a centralised
Elasticsearch cluster. Kibana was an analytics and visualisation platform for
turning those logs into actionable insights.</p>

<p>These tools were commonly used together, now known as the ELK stack, to deliver...</p>

<p><blockquote><p>"an end-to-end stack that delivers actionable insights in real time from almost any type of structured and unstructured data source."</p></blockquote></p>

<h4><strong>ELK, making logs awesome!</strong></h4>

<p><em><em>Manually installing and configuring Elasticsearch, Logstash and Kibana is not a <a href="https://gist.github.com/ashrithr/c5c03950ef631ac63c43">trivial task</a>.</em></em></p>

<p>Luckily, there is a better way...</p>

<h2>Docker </h2>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/7/79/Docker<em>(container_engine)</em>logo.png"></p>

<p><blockquote><p>"Docker allows you to pack, ship and run any application as a lightweight container".</p></blockquote></p>

<p><a href="https://www.docker.com/">Docker</a> <em>images</em> define pre-configured environments that containers
are started from.  <a href="https://hub.docker.com/">Docker Hub</a> is the public image registry, where anyone can
publish, search and retrieve new images.</p>

<p><img src="/images/Docker%20Hub.png"></p>

<p>Rather than having to install and configure individual software packages, we
can pull down one of the many existing Docker images for the <a href="https://registry.hub.docker.com/search?q=elk">ELK stack</a>.</p>

<p><em>With one command, we can spin up an entire ELK instance on any platform with no extra configuration needed.</em></p>

<p>Magic.</p>

<h2>IBM Containers</h2>

<p>IBM recently announced <a href="https://developer.ibm.com/bluemix/2015/06/22/ibm-containers-on-bluemix/">Docker support</a> for their Platform-as-a-Service cloud service, <a href="https://console.ng.bluemix.net/">IBM Bluemix</a>. Developers can now deploy and manage Docker containers on a scalable cloud platform.</p>

<p><a href="https://developer.ibm.com/bluemix/2015/06/22/ibm-containers-on-bluemix/">IBM Containers</a> provides the following services:</p>

<ul>
<li>Private image registry</li>
<li>Elastic scaling and auto-recovery</li>
<li>Persistent storage and advanced networking configuration</li>
<li>Automated security scans</li>
<li>Integration with the IBM Bluemix cloud services.</li>
</ul>


<p><em>Using this service, we can build and test a custom ELK container in our local
development environment and "web-scale" it by pushing to the IBM Bluemix cloud platform.</em></p>

<h2>Manging Application Logs</h2>

<p>Once our ELK instance is running, we can then start to push application logs
from other applications running on IBM Bluemix into the service. We'll look at
automatically setting up a log drain to forward all applications logs into a
centralised Elasticsearch service. We can then start to drive business
decisions using data rather than intuition using Kibana, the visualisation
dashboard.</p>

<p><strong><em>This blog post will explain the technical details of using Docker to create a
customised ELK service that can be hosted on a scalable cloud platform.</em></strong></p>

<h2>Running ELK instances Using Docker </h2>

<p>Docker Hub has over forty five thousands public images available. There are multiple public images we can pull
down with a pre-configured ELK stack. Looking at the options, we're going to use the <a href="https://registry.hub.docker.com/u/sebp/elk/">sebp/elk</a>
repository because it's popular and easily modifiable with a custom configuration.</p>

<p>We're going to start by pulling the image into our local machine and running a container to check it's working...</p>

<p><code>sh
$ docker pull sebp/elk
$ docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk sebp/elk
</code></p>

<p>That last command will start a new container from the <em>sebp/elk</em> image,
exposing the ports for Kibana (5601), Elasticsearch (9200) and Logstash (5000)
for external access. The container has been started with the <em>-i</em> flag,
interactive mode, allowing us to monitor the container logs in the console.
When the instance has started, we can view the status output from command line.</p>

<p><code>sh
$ docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                                                                              NAMES
42d40d1fb59c        sebp/elk:latest     "/usr/local/bin/star   27 seconds ago      Up 26 seconds       0.0.0.0:5000-&gt;5000/tcp, 0.0.0.0:5601-&gt;5601/tcp, 0.0.0.0:9200-&gt;9200/tcp, 9300/tcp   elk
</code></p>

<p>Using Mac OS X for local development, we're using the <a href="http://boot2docker.io/">Boot2Docker project</a> to host a Linux VM for deploying Docker containers locally.
With the following command, we can discover the virtual IP address for the ELK container.</p>

<p><code>sh
$ boot2docker ip
192.168.59.103
</code></p>

<p>Opening a web browser, we can now visit <em>http://192.168.59.103:5601</em> to show the Kibana application.
For now, this isn't very useful because Elasticsearch has no logs!</p>

<p>Let's fix that...</p>

<h2>Draining Logs from Cloud Foundry</h2>

<p><a href="https://www.cloudfoundry.org">Cloud Foundry</a>, the open-source project powering IBM Bluemix, supports <a href="https://docs.cloudfoundry.org/devguide/services/log-management.html">setting up a syslog drain</a>
to forward all applications logs to a third-party logging service. Full details on configuring this will be <a href="#config">shown later</a>.</p>

<p>Scott Frederick has already written an <a href="http://scottfrederick.cfapps.io/blog/2014/02/20/cloud-foundry-and-logstash">amazing blog post</a> about configuring Logstash
to support the log format used by the Cloud Foundry. Logstash expects the older RFC3164 syslog formatting by default, whilst Cloud Foundry emits log lines that follow
the newer RFC5424 standard.</p>

<p>Scott provides the following configuration file that sets up the syslog input channels, running on port 5000, along with a custom filter that converts the incoming RFC5424 logs into
an acceptable format.</p>

<p>``` sh
input {
  tcp {</p>

<pre><code>port =&gt; 5000
type =&gt; syslog
</code></pre>

<p>  }
  udp {</p>

<pre><code>port =&gt; 5000
type =&gt; syslog
</code></pre>

<p>  }
}</p>

<p>filter {
  if [type] == "syslog" {</p>

<pre><code>grok {
  match =&gt; { "message" =&gt; "%{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(?:%{NOTSPACE:syslog5424_app}|-) +(?:%{NOTSPACE:syslog5424_proc}|-) +(?:%{WORD:syslog5424_msgid}|-) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|) +%{GREEDYDATA:syslog5424_msg}" }
}
syslog_pri { }
date {
  match =&gt; [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
}
if !("_grokparsefailure" in [tags]) {
  mutate {
    replace =&gt; [ "@source_host", "%{syslog_hostname}" ]
    replace =&gt; [ "@message", "%{syslog_message}" ]
  }
}
mutate {
  remove_field =&gt; [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
}
</code></pre>

<p>  }
}</p>

<p>output {
  elasticsearch { }
}
```</p>

<p>Using this configuration, Logstash will accept and index our application logs into Elasticsearch.</p>

<p><em>Note: There is also a <a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-syslog.html">custom plugin</a> to enable RFC5424 support.</em></p>

<h2>Building Custom Docker Images</h2>

<p>Using the custom Logstash configuration relies on building a new Docker image
with this configuration baked in. We could download the Git repository
containing the image <a href="https://github.com/spujadas/elk-docker">source files</a>, modify those and rebuild from scratch.
However, an easier way uses the existing image as a <em>base</em>, applies our
modifications on top and then generates a brand new image.</p>

<p><strong>So, how do we build our own Docker images? Using a Dockerfile.</strong></p>

<p><blockquote><p>A Dockerfile is a text document that contains all the commands you would<br/>normally execute manually in order to build a Docker image.<br/></p></blockquote></p>

<p>Reviewing the <a href="https://registry.hub.docker.com/u/sebp/elk/dockerfile/">Dockerfile</a> for the <em>sebp/elk</em> image, configuration for logstash is
stored in the <em>/etc/logstash/conf.d/</em> directory. All we need to do is replace these files with our custom configuration.</p>

<p>Creating the custom configuration locally, we define a Dockerfile with instructions for building our image.</p>

<p><code>sh
$ ls
01-syslog-input.conf 10-syslog.conf       Dockerfile
$ cat Dockerfile
FROM sebp/elk
RUN rm /etc/logstash/conf.d/01-lumberjack-input.conf
ADD ./01-syslog-input.conf /etc/logstash/conf.d/01-syslog-input.conf
ADD ./10-syslog.conf /etc/logstash/conf.d/10-syslog.conf
</code></p>

<p>The Dockerfile starts with the "sebp/elk" image as a base layer. Using the RUN command, we execute a command to remove existing input configuration. After this
the ADD command copies files from our local directory into the image.</p>

<p>We can now run the Docker build system to generate our new image.</p>

<p><code>sh
$ docker build -t jthomas/elk .
Sending build context to Docker daemon 4.608 kB
Sending build context to Docker daemon
Step 0 : FROM sebp/elk
 ---&gt; 2b71e915297f
Step 1 : RUN rm /etc/logstash/conf.d/01-lumberjack-input.conf
 ---&gt; Using cache
 ---&gt; f196b6833121
Step 2 : ADD ./01-syslog-input.conf /etc/logstash/conf.d/01-syslog-input.conf
 ---&gt; Using cache
 ---&gt; 522ba2c76b00
Step 3 : ADD ./10-syslog.conf /etc/logstash/conf.d/10-syslog.conf
 ---&gt; Using cache
 ---&gt; 79256ffaac3b
Successfully built 79256ffaac3b
$ docker images jthomas/elk
REPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
jthomas/elk         latest              79256ffaac3b        26 hours ago        1.027 GB
</code></p>

<p><em>...and that's it! We have a customised Docker image with our configuration changes ready for running.</em></p>

<h2>Testing Our Custom Image</h2>

<p>Before pushing this image to the cloud, we need to check it's working correctly.
Let's start by starting a new container from our custom image locally.</p>

<p><code>sh
$ docker run -p 5601:5601 -p 9200:9200 -p 5000:5000 -it --name elk jthomas/elk
</code></p>

<p>Now, use the <a href="https://github.com/cloudfoundry/cli">CF CLI</a> to access recent logs for a sample application and paste the output into
a telnet connection to port 5000 on our container.</p>

<p>``` sh
$ cf logs APP_NAME --recent
Connected, dumping recent logs for app debug-testing in org james.thomas@uk.ibm.com / space dev as james.thomas@uk.ibm.com...</p>

<p>2015-07-02T17:14:47.58+0100 [RTR/1]      OUT nodered-app.mybluemix.net - [02/07/2015:16:14:47 +0000] "GET / HTTP/1.1" 200 0 7720 "-" "Java/1.7.0" 75.126.70.42:56147 x_forwarded_for:"-" vcap_request_id:1280fe18-e53a-4bd4-40a9-2aaf7c53cc54 response_time:0.003247100 app_id:f18c2dea-7649-4567-9532-473797b0818d
2015-07-02T17:15:44.56+0100 [RTR/2]      OUT nodered-app.mybluemix.net - [02/07/2015:16:15:44 +0000] "GET / HTTP/1.1" 200 0 7720 "-" "Java/1.7.0" 75.126.70.43:38807 x_forwarded_for:"-" vcap_request_id:4dd96d84-c61d-45ec-772a-289ab2f37c67 response_time:0.003848360 app_id:f18c2dea-7649-4567-9532-473797b0818d
2015-07-02T17:16:29.61+0100 [RTR/2]      OUT nodered-app.mybluemix.net - [02/07/2015:16:14:29 +0000] "GET /red/comms HTTP/1.1" 101 0 0 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36" 75.126.70.42:54826 x_forwarded_for:"75.126.70.42" vcap_request_id:15c2d4f8-e6ba-4a20-77b7-345aafd32e95 response_time:MissingFinishedAt app_id:f18c2dea-7649-4567-9532-473797b0818d
$ telnet 192.168.59.103 5000
Trying 192.168.59.103...
Connected to 192.168.59.103.
Escape character is '<sup>]'.</sup>
// PASTE LOG LINES....
```
Starting a web browser and opening the Kibana page, port 5601, the log lines are now available in the dashboard. Success!</p>

<h2>Pushing Docker Images To The Cloud </h2>

<p>Having successfully built and tested our custom Docker image locally, we want
to push this image to our cloud platform to allow us to start new containers
based on this image.</p>

<p>Docker supports pushing local images to the <a href="http://hub.docker.com">public registry</a> using the <em>docker push</em> command.
We can choose to use a <a href="https://blog.docker.com/2013/07/how-to-use-your-own-registry/">private registry</a>
by creating a new image tag which prefixes the repository location in the name.</p>

<p><em>IBM Containers' private registry is available at the following address, <strong>registry.ng.bluemix.net</strong>.</em></p>

<p>Let's push our custom image to the IBM Containers private registry...</p>

<p><code>sh
$ docker tag jthomas/elk registry.ng.bluemix.net/jthomas/elk
$ docker images
REPOSITORY                                     TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
jthomas/elk                                   latest              79256ffaac3b        43 hours ago        1.027 GB
registry.ng.bluemix.net/jthomas/elk           latest              79256ffaac3b        43 hours ago        1.027 GB
$ docker push registry.ng.bluemix.net/jthomas/elk
The push refers to a repository [registry.ng.bluemix.net/jthomas/elk] (len: 1)
Sending image list
Pushing repository registry.ng.bluemix.net/jthomas/elk (1 tags)
511136ea3c5a Image successfully pushed
...
79256ffaac3b: Image successfully pushed
Pushing tag for rev [79256ffaac3b] on {https://registry.ng.bluemix.net/v1/repositories/jthomas/elk/tags/latest}
</code></p>

<p>Pushing custom images from a local environment can be a slow process. For the <em>elk</em> image, this means transferring over one gigabyte of data
to the external registry.</p>

<p><em>We can speed this up by using IBM Containers to create our image from the Dockerfile, rather than uploading the built image.</em></p>

<p>Doing this from the command line requires the use of the IBM Containers command-line application.</p>

<h2>Managing IBM Containers</h2>

<p>IBM Containers enables you to manage your containers from the command-line with <a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov.html">two options</a>...</p>

<ul>
<li><em><a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov.html#installcontainercfplugin">IBM Containers Plug-in</a> for the Cloud Foundry CLI.</em></li>
<li><em><a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov_ice.html">IBM Containers Extension</a>, standalone command-line application.</em></li>
</ul>


<p>Both approaches handle the interactions between the local and remote Docker hosts, while providing
extra functionality not supported natively by Docker.</p>

<p><em>Full details on the differences and installation procedures
for the two applications are available <a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov.html">here</a>.</em></p>

<h2>Building Images Using IBM Containers</h2>

<p>Building our image using the IBM Containers service uses the same syntax as <a href="https://docs.docker.com/reference/commandline/build/">Docker build</a>. Local files
from the current directory will be sent with the Dockerfile to the remote service. Once the image has
been built, we can verify it's available in the remote repository.</p>

<p><code>sh
$ ice build -t registry.ng.bluemix.net/jthomas/elk .
zipped tar size: 706
Posting 706 bytes... It may take a while...
Step 0 : FROM sebp/elk
 ---&gt; 2b71e915297f
Step 1 : RUN rm /etc/logstash/conf.d/01-lumberjack-input.conf
 ---&gt; Using cache
 ---&gt; ed13d91e0197
Step 2 : ADD ./01-syslog-input.conf /etc/logstash/conf.d/01-syslog-input.conf
 ---&gt; Using cache
 ---&gt; 808a4c7410c7
Step 3 : ADD ./10-syslog.conf /etc/logstash/conf.d/10-syslog.conf
 ---&gt; Using cache
 ---&gt; 117e4454b015
Successfully built 117e4454b015
The push refers to a repository [registry.ng.bluemix.net/jthomas/elk] (len: 1)
Sending image list
Pushing repository registry.ng.bluemix.net/jthomas/elk (1 tags)
Image 117e4454b015 already pushed, skipping
Pushing tag for rev [117e4454b015] on {https://registry.ng.bluemix.net/v1/repositories/jthomas/elk/tags/latest}
$ ice images
REPOSITORY                                TAG                 IMAGE ID            CREATED             VIRTUAL SIZE
registry.ng.bluemix.net/jthomas/elk       latest              5454d3ec-0f3        44 hours ago        0 B
registry.ng.bluemix.net/ibmliberty        latest              3724d2e0-06d        9 days ago          0 B
registry.ng.bluemix.net/ibmnode           latest              9435349e-8b4        9 days ago          0 B
</code></p>

<p>All private repositories on IBM Bluemix have two official images for supported versions of <a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov.html#container_images_node">NodeJS</a> and <a href="https://www.ng.bluemix.net/docs/starters/container_cli_ov.html#container_images_liberty">Websphere Liberty</a>.</p>

<p><em>We can now see the third image is the custom ELK stack that was built.</em></p>

<h2>Starting ELK Containers</h2>

<p>Starting containers from images in the IBM Containers registry can be done using the command-line applications or through the <a href="http://bluemix.net">IBM Bluemix UI</a>.
In this example, we'll be using the IBM Bluemix UI to start and configure a new ELK container from our pre-configured image.</p>

<p>Logging into the IBM Bluemix, the <em>Catalogue</em> page shows the list of available images used to create new containers. We have both the official
images from IBM Containers and our custom ELK service.</p>

<p><img src="/images/Container%20Images.png"></p>

<p>Selecting the <em>ELK</em> image, we can configure and run a new container from this
image. Setting up a new container with a public IP address,
memory limit to 1GB and expose the same ports as running locally (5000, 5601
and 9200).</p>

<p><img src="/images/Create%20Container.png"></p>

<p>Clicking the <em>Create</em> button, IBM Bluemix will provision and start our
new container.</p>

<p>Once the container has started, we can view the <em>Dashboard</em> page for this instance. Here we can view
details about the container instance, modify the running state and access monitoring and logs tools.</p>

<p><img src="/images/Container%20Overview.png">
<img src="/images/Container%20Monitoring.png"></p>

<p><em>...and that's it! We now have our ELK service running using IBM Containers
ready to start processing logs from our applications.</em></p>

<p>Visiting the external IP address assigned to the container on the Kibana
application port (5601) shows the Kibana web interface demonstrating our
container has started correctly.</p>

<h2><a name="config"></a>Draining Cloud Foundry Logs</h2>

<p>Cloud Foundry supports draining applications logs to a <a href="http://docs.cloudfoundry.org/devguide/services/log-management.html">third-party syslog service</a>.
The ELK container has a syslog drain configured on port 5000 of the public IP address bound to the instance.</p>

<p>Binding this custom syslog drain to Cloud Foundry applications uses a <a href="https://docs.cloudfoundry.org/devguide/services/user-provided.html">custom user-provided service</a>.
Creating user-provided services using the CF CLI, there is a special flag, <em>-l</em>, that notifies the platform this service is a syslog drain. Binding this special syslog
drain service to an application will automatically set up log forwarding. Once the application has been restarted, logs will start to flow into the external service.</p>

<p><code>sh
$ cf cups logstash-drain -l syslog://[CONTAINER_IP]:5000
$ cf bind-service [app-name] logstash-drain
$ cf restart [app-name]
</code></p>

<p><em>Cloud Foundry supports multiple syslog drains for the same application.</em></p>

<p>Testing this out is as simple as visiting our application to generate sample logs and then looking at the Kibana page to see they are showing up.
Here is a screenshot of the expected output when our ELK container is successfully processing logs from a Cloud Foundry application.</p>

<p><img src="/images/Kibana.png"></p>

<h2>Conclusion</h2>

<p>Elastic Search, Kibana and Logstash is the modern log processing framework.
Using Docker, we've been able to create a custom ELK service without manually
installing and configuring a multitude of different software packages. Pushing
this image to the IBM Containers platform means we can spin up new ELK
containers on-demand within minutes!</p>

<p><blockquote><p>Elasticsearch, Docker and IBM Containers... Making Logs Awesome.</p></blockquote></p>
]]></content>
  </entry>
  
</feed>
