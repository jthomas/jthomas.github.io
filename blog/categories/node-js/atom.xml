<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: node.js | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/node-js/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2019-08-08T10:33:32+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Connecting to IBM Cloud Databases for Redis from Node.js]]></title>
    <link href="http://jamesthom.as/blog/2019/07/22/connecting-to-ibm-cloud-databases-for-redis-from-node-dot-js/"/>
    <updated>2019-07-22T12:31:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/07/22/connecting-to-ibm-cloud-databases-for-redis-from-node-dot-js</id>
    <content type="html"><![CDATA[<p>This blog post explains how to connect to an <a href="https://www.ibm.com/cloud/databases-for-redis">IBM Cloud Databases for Redis</a> instance from a <a href="https://nodejs.org/en/">Node.js</a> application. There is a (small) difference between the connection details needed for an IBM Cloud Databases for Redis instance compared to a local instance of the open-source database. This is due to all IBM Cloud Databases using <a href="https://cloud.ibm.com/docs/services/databases-for-redis?topic=databases-for-redis-external-app#driver-tls-and-self-signed-certificate-support">secured TLS connections</a> with <a href="https://en.wikipedia.org/wiki/Self-signed_certificate">self-signed certificates</a>.</p>

<p><em>I keep running into this issue (and forgetting how to fix it</em> ü§¶‚Äç‚ôÇÔ∏è<em>), so I'm documenting the solution here to help myself (and others) who might run into it‚Ä¶</em> ü¶∏‚Äç‚ôÇÔ∏è</p>

<h2>Connecting to Redis (without TLS connections)</h2>

<p>Most Node.js application use the <code>redis</code> <a href="https://www.npmjs.com/package/redis">NPM library</a> to interact with an instance of the database. This library has a <code>createClient</code> <a href="">method</a> which returns an instance of the client. The Node.js application passes a connection string into the <code>createClient</code> method. This string contains the hostname, port, username and password for the database instance.</p>

<p><code>javascript
const redis = require("redis"),
const url = 'redis://user:secret@localhost:6379/'
const client = redis.createClient(url);
</code></p>

<p>The client fires a <code>connect</code> event once the <a href="https://github.com/NodeRedis/node_redis#connection-and-other-events">connection is established</a> or an <code>error</code> event if <a href="https://github.com/NodeRedis/node_redis#connection-and-other-events">issues are encountered</a>.</p>

<h2>IBM Cloud Databases for Redis Service Credentials</h2>

<p>IBM Cloud Databases for Redis provide service credentials through the <a href="https://cloud.ibm.com/docs/services/databases-for-redis?topic=databases-for-redis-connection-strings#the-_service-credentials_-panel">instance management console</a>. Service credentials are JSON objects with connection properties for client libraries, the CLI and other tools. Connection strings for the Node.js client library are available in the <code>connection.rediss.composed</code> field.</p>

<p><strong><em>So, I just copy this field value and use with the <code>redis.createClient</code> method? Not so fast...</em></strong></p>

<p>IBM Cloud Databases for Redis uses TLS to secure all connections to the Redis instances. This is denoted by the connection string using the <code>rediss://</code> URL prefix, rather than <code>redis://</code>. Using that connection string (without further connection properties), will lead to the following error being thrown by the Node.js application.</p>

<p><code>
Error: Redis connection to &lt;id&gt;.databases.appdomain.cloud:port failed - read ECONNRESET
  at TCP.onread (net.js:657:25) errno: 'ECONNRESET', code: 'ECONNRESET', syscall: 'read'
</code></p>

<p>If the <code>createClient</code> forces a TLS connection to be used <code>createClient(url, { tls: {} })</code>, this error will be replaced with a different one about self-signed certificates.</p>

<p>```
Error: Redis connection to <id>.databases.appdomain.cloud:port failed failed - self signed certificate in certificate chain</p>

<pre><code>at TLSSocket.onConnectSecure (_tls_wrap.js:1055:34)
at TLSSocket.emit (events.js:182:13)
at TLSSocket._finishInit (_tls_wrap.js:635:8) code: 'SELF_SIGNED_CERT_IN_CHAIN'
</code></pre>

<p>```</p>

<p><em>Hmmmm, how to fix this?</em> ü§î</p>

<h2>Connecting to Redis (with TLS connections)</h2>

<p>All connections to IBM Cloud Databases are secured with TLS using self-signed certificates. Public certificates for the signing authorities are provided as Base64 strings in the service credentials. These certificates can be provided in the client constructor to support self-signed TLS connections.</p>

<p><strong><em>Here are the steps needed to use those self-signed certificates with the client library...</em></strong></p>

<ul>
<li>Extract the <code>connection.rediss.certificate.certificate_base64</code> value from the service credentials.</li>
</ul>


<p><img src="/images/redis-certificates.png" alt="Redis Service Credentials" /></p>

<ul>
<li>Decode the Base64 string in Node.js to extract the PEM certificate string.</li>
</ul>


<p><code>javascript
const ca = Buffer.from(cert_base64, 'base64').toString('utf-8')
</code></p>

<ul>
<li>Provide the certificate file string as the <code>ca</code> property in the <code>tls</code> object for the client constructor.</li>
</ul>


<p><code>javascript
const tls = { ca };
const client = redis.createClient(url, { tls });
</code></p>

<ul>
<li>‚Ä¶Relax! üòé</li>
</ul>


<p><em>The <code>tls</code> property is passed through to the <code>tls.connect</code> <a href="https://nodejs.org/api/tls.html#tls_tls_connect_options_callback">method</a> in Node.js, which is used to setup the TLS connection. This method supports a <code>ca</code> parameter to extend the trusted CA certificates pre-installed in the system. By providing the self-signed certificate using this property, the errors above will not be seen.</em></p>

<h2>Conclusion</h2>

<p>It took me a while to <a href="https://compose.com/articles/ssl-connections-arrive-for-redis-on-compose/">work out</a> how to connect to TLS-secured Redis instances from a Node.js application. <a href="https://stackoverflow.com/questions/10888610/ignore-invalid-self-signed-ssl-certificate-in-node-js-with-https-request/39099130#39099130">Providing the self-signed certificate</a> in the client constructor is a much better solution than having to <a href="https://stackoverflow.com/a/21961005/1427084">disable  all unauthorised TLS connections</a>!</p>

<p>Since I don't write new Redis client code very often, I keep forgetting the correct constructor parameters to make this work. Turning this solution into a blog post will (hopefully) embed it in my brain (or at least provide a way to find the answer instead of having to grep through old project code). This might even be useful to others Googling for a solution to those error messages...</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Accessing Long-Running Apache OpenWhisk Actions Results]]></title>
    <link href="http://jamesthom.as/blog/2019/05/14/accessing-long-running-openwhisk-actions-results/"/>
    <updated>2019-05-14T11:35:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/05/14/accessing-long-running-openwhisk-actions-results</id>
    <content type="html"><![CDATA[<p><a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> actions are invoked by sending HTTP POST requests to the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md">platform API</a>. Invocation requests have two <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/rest_api.md#actions">different modes</a>: <strong>blocking</strong> and <strong>non-blocking</strong>.</p>

<p><strong>Blocking invocations</strong> mean the platform won't send the HTTP response until the action finishes. This allows it to include the action result in the response.  Blocking invocations are used when you want to invoke an action and wait for the result.</p>

<p>```
$ wsk action invoke my_action --blocking
ok: invoked /_/my_action with id db70ef682fae4f8fb0ef682fae2f8fd5
{</p>

<pre><code>"activationId": "db70ef682fae4f8fb0ef682fae2f8fd5",
...
"response": {
    "result": { ... },
    "status": "success",
    "success": true
},
...
</code></pre>

<p>}
```</p>

<p><strong>Non-blocking invocations</strong> return as soon as the platform processes the invocation request. This is before the action has finished executing. HTTP responses from non-blocking invocations only include activation identifiers, as the action result is not available.</p>

<p><code>
$ wsk action invoke my_action
ok: invoked /_/my_action with id d2728aaa75394411b28aaa7539341195
</code></p>

<p><strong>HTTP responses from a blocking invocation will only wait for a limited amount of time before returning.</strong> This defaults to 65 seconds in the <a href="https://github.com/apache/incubator-openwhisk/blob/master/core/controller/src/main/resources/application.conf#L21">platform configuration file</a>. If an action invocation has not finished before this timeout limit, a HTTP 5xx status response is returned.</p>

<p>Hmmm‚Ä¶ ü§î</p>

<p><strong><em>"So, how can you invoke an action and wait for the result when actions take longer than this limit?"</em></strong></p>

<p>This question comes up regularly from developers building applications using the platform. I've decided to turn my answer into a blog post to help others struggling with this issue (after answering this question again this week üòé).</p>

<h3>solution</h3>

<ul>
<li><em>Invoke the action using a <a href="https://github.com/apache/incubator-openwhisk-client-js#invoke-action">non-blocking invocation</a>.</em></li>
<li><em>Use the returned activation identifier to poll the <a href="https://github.com/apache/incubator-openwhisk-client-js#retrieve-resource">activation result API</a>.</em></li>
<li><em>The HTTP response for the activation result will return a HTTP 404 response until the action finishes.</em></li>
</ul>


<p>When polling for activation results from non-blocking invocations, you should enforce a limit on the maximum polling time allowed. This is because HTTP 404s can be returned due to other scenarios (e.g. invalid activation identifiers). Enforcing a time limit ensures that, in the event of issues in the application code or the platform, the polling loop with eventually stop!</p>

<p><em>Setting the maximum polling time to the action timeout limit (plus a small offset) is a good approach.</em></p>

<p>An action cannot run for longer than its timeout limit. If the activation record is not available after this duration has elapsed (plus a small offset to handle internal platform delays), something has gone wrong. Continuing to poll after this point runs the risk of turning the polling operation into an infinite loop...</p>

<h3>example code</h3>

<p>This example provides an implementation of this approach for Node.js using the <a href="https://github.com/apache/incubator-openwhisk-client-js">JavaScript Client SDK</a>.</p>

<p>```javascript
"use strict";</p>

<p>const openwhisk = require('openwhisk')</p>

<p>const options = { apihost: <API_HOST>, api_key: <API_KEY> }
const ow = openwhisk(options)</p>

<p>// action duration limit (+ small offset)
const timeout_ms = 85000
// delay between polling requests
const polling_delay = 1000
// action to invoke
const action = 'delay'</p>

<p>const now = () => (new Date().getTime())
const max_polling_time = now() + timeout_ms</p>

<p>const delay = async ms => new Promise(resolve => setTimeout(resolve, ms))</p>

<p>const activation = await ow.actions.invoke({name: action})
console.log(<code>new activation id: ${activation.activationId}</code>)</p>

<p>let result = null</p>

<p>do {
  try {</p>

<pre><code>result = await ow.activations.get({ name: activation.activationId })
console.log(`activation result (${activation.activationId}) now available!`)
</code></pre>

<p>  } catch (err) {</p>

<pre><code>if (err.statusCode !== 404) {
  throw err
}
console.log(`activation result (${activation.activationId}) not available yet`)
</code></pre>

<p>  }</p>

<p>  await delay(polling_delay)
} while (!result &amp;&amp; now() &lt; max_polling_time)</p>

<p>console.log(<code>activation result (${activation.activationId})</code>, result)
```</p>

<h3>testing it out</h3>

<p>Here is the source code for an action which will not return until 70 seconds have passed. Blocking invocations firing this action will result in a HTTP timeout before the response is returned.</p>

<p>```javascript
const delay = async ms => new Promise(resolve => setTimeout(resolve, ms))</p>

<p>function main() {
  return delay(70*1000)
}
```</p>

<p>Using the script above, the action result will be retrieved from a non-blocking invocation.</p>

<ul>
<li><em>Create an action from the source file in the example above.</em></li>
</ul>


<p><code>
wsk action create delay delay.js --timeout 80000 --kind nodejs:10
</code></p>

<ul>
<li><em>Run the Node.js script to invoke this action and poll for the activation result.</em></li>
</ul>


<p><code>
node script.js
</code></p>

<p>If the script runs correctly, log messages will display the polling status and then the activation result.</p>

<p><code>
$ node script.js
new activation id: d4efc4641b544320afc4641b54132066
activation result (d4efc4641b544320afc4641b54132066) not available yet
activation result (d4efc4641b544320afc4641b54132066) not available yet
activation result (d4efc4641b544320afc4641b54132066) not available yet
...
activation result (d4efc4641b544320afc4641b54132066) now available!
activation result (d4efc4641b544320afc4641b54132066) { ... }
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Saving Money and Time With Node.js Worker Threads in Serverless Functions]]></title>
    <link href="http://jamesthom.as/blog/2019/05/08/node-dot-js-worker-threads-with-serverless-functions/"/>
    <updated>2019-05-08T12:17:00+01:00</updated>
    <id>http://jamesthom.as/blog/2019/05/08/node-dot-js-worker-threads-with-serverless-functions</id>
    <content type="html"><![CDATA[<p>Node.js v12 was <a href="https://foundation.nodejs.org/announcements/2019/04/24/node-js-foundation-and-js-foundation-merge-to-form-openjs-foundation-2">released last month</a>. This new version includes support for <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a>, that are enabled by default. Node.js <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a> make it simple to execute JavaScript code in parallel using threads. üëèüëèüëè</p>

<p>This is useful for Node.js applications with CPU-intensive workloads. Using Worker Threads, JavaScript code can be executed code concurrently using multiple CPU cores. This reduces execution time compared to a non-Worker Threads version.</p>

<p>If serverless platforms provide Node.js v12 on multi-core environments, functions can use this feature to reduce execution time and, therefore, lower costs. Depending on the workload, functions can utilise all available CPU cores to parallelise work, rather than executing more functions concurrently. üí∞üí∞üí∞</p>

<p><strong>In this blog post, I'll explain how to use Worker Threads from a serverless function. I'll be using <a href="https://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> (<a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) as the example platform but this approach is applicable for any serverless platform with Node.js v12 support and a multi-core CPU runtime environment.</strong></p>

<h2>Node.js v12 in IBM Cloud Functions (Apache OpenWhisk)</h2>

<p><em>This section of the blog post is specifically about using the new <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js v12 runtime</a> on IBM Cloud Functions (powered by <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>). If you are using a different serverless platform, feel free to skip ahead to the next section‚Ä¶</em></p>

<p>I've recently <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/pull/126">been working</a> on adding the Node.js v12 runtime to Apache OpenWhisk.</p>

<p>Apache OpenWhisk uses <a href="https://hub.docker.com/u/openwhisk">Docker containers</a> as runtime environments for serverless functions. All runtime images are maintained in separate repositories for each supported language, e.g. <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js</a>, <a href="https://github.com/apache/incubator-openwhisk-runtime-java">Java</a>, <a href="https://github.com/apache/incubator-openwhisk-runtime-python">Python</a>, etc. Runtime images are automatically built and pushed to <a href="https://hub.docker.com/r/openwhisk/">Docker Hub</a> when the repository is updated.</p>

<h3>node.js v12 runtime image</h3>

<p>Here is <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/pull/126">the PR</a> used to add the new Node.js v12 runtime image to Apache OpenWhisk. This led to the following <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v12">runtime image</a> being exported to Docker Hub: <code>openwhisk/action-nodejs-v12</code>.</p>

<p>Having this image available as a native runtime in Apache OpenWhisk requires <a href="https://github.com/apache/incubator-openwhisk/pull/4472">upstream changes</a> to the project's runtime manifest. After this happens, developers will be able to use the <code>--kind</code> CLI flag to select this runtime version.</p>

<p><code>
ibmcloud wsk action create action_name action.js --kind nodejs:12
</code></p>

<p><a href="http://cloud.ibm.com/openwhisk">IBM Cloud Functions</a> is powered by <a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>. It will eventually pick up the upstream project changes to include this new runtime version. Until that happens, Docker support allows usage of this new runtime before it is built-in the platform.</p>

<p><code>
ibmcloud wsk action create action_name action.js --docker openwhisk/action-nodejs-v12
</code></p>

<h3>example</h3>

<p>This Apache OpenWhisk action returns the version of Node.js used in the runtime environment.</p>

<p>```javascript
function main () {
  return {</p>

<pre><code>version: process.version
</code></pre>

<p>  }
}
```</p>

<p>Running this code on IBM Cloud Functions, using the Node.js v12 runtime image, allows us to confirm the new Node.js version is available.</p>

<p>```
$ ibmcloud wsk action create nodejs-v12 action.js --docker openwhisk/action-nodejs-v12
ok: created action nodejs-v12
$ ibmcloud wsk action invoke nodejs-v12 --result
{</p>

<pre><code>"version": "v12.1.0"
</code></pre>

<p>}
```</p>

<h2>Worker Threads in Serverless Functions</h2>

<p><a href="https://medium.com/@Trott/using-worker-threads-in-node-js-80494136dbb6">This is a great introdution blog post</a> to Workers Threads. It uses an example of generating prime numbers as the CPU intensive task to benchmark. Comparing the performance of the single-threaded version to multiple-threads - the performance is improved as a factor of the threads used (up to the number of CPU cores available).</p>

<p>This code can be ported to run in a serverless function. Running with different input values and thread counts will allow benchmarking of the performance improvement.</p>

<h3>non-workers version</h3>

<p>Here is the <a href="https://gist.github.com/jthomas/71c76d62ddfd146c4bf763f5b2f0eec1">sample code</a> for a serverless function to generate prime numbers. It does not use Worker Threads. It will run on the main <a href="https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/">event loop</a> for the Node.js process. This means it will only utilise a single thread (and therefore single CPU core).</p>

<p>```javascript
'use strict';</p>

<p>const min = 2</p>

<p>function main(params) {
  const { start, end } = params
  console.log(params)
  const primes = []
  let isPrime = true;
  for (let i = start; i &lt; end; i++) {</p>

<pre><code>for (let j = min; j &lt; Math.sqrt(end); j++) {
  if (i !== j &amp;&amp; i%j === 0) {
    isPrime = false;
    break;
  }
}
if (isPrime) {
  primes.push(i);
}
isPrime = true;
</code></pre>

<p>  }</p>

<p>  return { primes }
}
```</p>

<h3>porting the code to use worker threads</h3>

<p>Here is the prime number calculation code which uses Worker Threads. Dividing the total input range by the number of Worker Threads generates individual thread input values. Worker Threads are spawned and passed chunked input ranges. Threads calculate primes and then send the result back to the parent thread.</p>

<script src="https://gist.github.com/Trott/7bb7ee55c247047d030b4c427434ef51.js"></script>


<p>Reviewing the code to start converting it to a serverless function, I realised there were two issues running this code in serverless environment: <strong>worker thread initialisation</strong> and <strong>optimal worker thread counts</strong>.</p>

<h4>How to initialise Worker Threads?</h4>

<p>This is how the existing source code <a href="https://nodejs.org/dist/latest-v12.x/docs/api/worker_threads.html#worker_threads_new_worker_filename_options">initialises the Worker Threads</a>.</p>

<p><code>javascript
 threads.add(new Worker(__filename, { workerData: { start: myStart, range }}));
</code></p>

<p> <em><code>__filename</code> is a special global variable in Node.js which contains the currently executing script file path.</em></p>

<p>This means the Worker Thread will be initialised with a copy of the currently executing script. Node.js provides a special variable to indicate whether the script is executing in the parent or child thread. This can be used to branch script logic.</p>

<p><strong>So, what's the issue with this?</strong></p>

<p>In the Apache OpenWhisk Node.js runtime, action source files are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61-L79">dynamically imported</a> into the runtime environment. The script used to start the Node.js runtime process is for the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js">platform handler</a>, not the action source files. This means the <code>__filename</code> variable does not point to the action source file.</p>

<p>This issue is fixed by separating the serverless function handler and worker thread code into separate files. Worker Threads can be started with a reference to the worker thread script source file, rather than the currently executing script name.</p>

<p><code>javascript
 threads.add(new Worker("./worker.js", { workerData: { start: myStart, range }}));
</code></p>

<h4>How Many Worker Threads?</h4>

<p>The next issue to resolve is how many Worker Threads to use. In order to maximise parallel processing capacity, there should be a Worker Thread for each CPU core. This is the maximum number of threads that can run concurrently.</p>

<p>Node.js provides CPU information for the runtime environment using the <code>os.cpus()</code> <a href="https://nodejs.org/api/os.html#os_os_cpus">function</a>. The result is an array of objects (one per logical CPU core), with model information, processing speed and elapsed processing times. The length of this array will determine number of Worker Threads used. This ensures the number of Worker Threads will always match the CPU cores available.</p>

<p><code>javascript
const threadCount = os.cpus().length
</code></p>

<h3>workers threads version</h3>

<p>Here is the serverless version of the prime number generation algorithm which uses Worker Threads.</p>

<p>The code is split over two files - <code>primes-with-workers.js</code> and <code>worker.js</code>.</p>

<h4>primes-with-workers.js</h4>

<p><a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43">This file</a> contains the serverless function handler used by the platform. Input ranges (based on the <code>min</code> and <code>max</code> action parameters) are divided into chunks, based upon the number of Worker Threads. The handler function creates a Worker Thread for each chunk and waits for the message with the result. Once all the results have been retrieved, it returns all those primes numbers as the invocation result.</p>

<p>```javascript
'use strict';</p>

<p>const { Worker } = require('worker_threads');
const os = require('os')
const threadCount = os.cpus().length</p>

<p>const compute_primes = async (start, range) => {
  return new Promise((resolve, reject) => {</p>

<pre><code>let primes = []
console.log(`adding worker (${start} =&gt; ${start + range})`)
const worker = new Worker('./worker.js', { workerData: { start, range }})

worker.on('error', reject)
worker.on('exit', () =&gt; resolve(primes))
worker.on('message', msg =&gt; {
  primes = primes.concat(msg)
})
</code></pre>

<p>  })
}</p>

<p>async function main(params) {
  const { min, max } = params
  const range = Math.ceil((max - min) / threadCount)
  let start = min &lt; 2 ? 2 : min
  const workers = []</p>

<p>  console.log(<code>Calculating primes with ${threadCount} threads...</code>);</p>

<p>  for (let i = 0; i &lt; threadCount - 1; i++) {</p>

<pre><code>const myStart = start
workers.push(compute_primes(myStart, range))
start += range
</code></pre>

<p>  }</p>

<p>  workers.push(compute_primes(start, max - start))</p>

<p>  const primes = await Promise.all(workers)
  return { primes: primes.flat() }
}</p>

<p>exports.main = main
```</p>

<h4>workers.js</h4>

<p><a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43#file-workers-js">This is the script</a> used in the Worker Thread. The <code>workerData</code> value is used to receive number ranges to search for prime numbers. Primes numbers are sent back to the parent thread using the <code>postMessage</code> function. Since this script is only used in the Worker Thread, it does need to use the <code>isMainThread</code> value to check if it is a child or parent process.</p>

<p>```javascript
'use strict';
const { Worker, isMainThread, parentPort, workerData } = require('worker_threads');</p>

<p>const min = 2</p>

<p>function generatePrimes(start, range) {
  const primes = []
  let isPrime = true;
  let end = start + range;
  for (let i = start; i &lt; end; i++) {</p>

<pre><code>for (let j = min; j &lt; Math.sqrt(end); j++) {
  if (i !== j &amp;&amp; i%j === 0) {
    isPrime = false;
    break;
  }
}
if (isPrime) {
  primes.push(i);
}
isPrime = true;
</code></pre>

<p>  }</p>

<p>  return primes
}</p>

<p>const primes = generatePrimes(workerData.start, workerData.range);
parentPort.postMessage(primes)
```</p>

<h4>package.json</h4>

<p>Source files deployed from a zip file also need to include a <code>package.json</code> file in the archive. The <code>main</code> property is used to determine the script to import as the exported package module.</p>

<p><code>json
{
  "name": "worker_threads",
  "version": "1.0.0",
  "main": "primes-with-workers.js",
}
</code></p>

<h2>Performance Comparison</h2>

<p>Running both functions with the same input parameters allows execution time comparison. The Worker Threads version should improve performance by a factor proportional to available CPU cores. Reducing execution time also means reduced costs in a serverless platform.</p>

<h3>non-workers performance</h3>

<p>Creating a new serverless function (<code>primes</code>) from the non-worker threads source code, using the Node.js v12 runtime, I can test with small values to check correctness.</p>

<p>```sh
$ ibmcloud wsk action create primes primes.js --docker openwhisk/action-nodejs-v12
ok: created action primes
$ ibmcloud wsk action invoke primes --result -p start 2 -p end 10
{</p>

<pre><code>"primes": [ 2, 3, 5, 7 ]
</code></pre>

<p>}
```</p>

<p>Playing with sample input values, 10,000,000 seems like a useful benchmark value. This takes long enough with the single-threaded version to benefit from parallelism.</p>

<p>```sh
$ time ibmcloud wsk action invoke primes --result -p start 2 -p end 10000000 > /dev/null</p>

<p>real    0m35.151s
user    0m0.840s
sys 0m0.315s
```</p>

<p><strong>Using the simple single-threaded algorithm it takes the serverless function around ~35 seconds to calculate primes up to ten million.</strong></p>

<h3>workers threads performance</h3>

<p>Creating a new serverless function, from the worker threads-based source code using the Node.js v12 runtime, allows me to verify it works as expected for small input values.</p>

<p>```
$ ibmcloud wsk action create primes-workers action.zip --docker openwhisk/action-nodejs-v12
ok: created action primes-workers
$ ibmcloud wsk action invoke primes-workers --result -p min 2 -p max 10
{</p>

<pre><code>"primes": [ 2, 3, 5, 7 ]
</code></pre>

<p>}
```</p>

<p>Hurrah, it works.</p>

<p>Invoking the function with an <code>max</code> parameter of 10,000,000 allows us to benchmark against the non-workers version of the code.</p>

<p>```sh
$ time ibmcloud wsk action invoke primes-workers --result -p min 2 -p max 10000000 --result > /dev/null</p>

<p>real    0m8.863s
user    0m0.804s
sys 0m0.302s
```</p>

<p><strong>The workers versions only takes ~25% of the time of the single-threaded version!</strong></p>

<p>This is because IBM Cloud Functions' runtime environments provide access to four CPU cores. Unlike other platforms, CPU cores are not tied to memory allocations. Utilising all available CPU cores concurrently allows the algorithm to run 4x times as fast. Since serverless platforms charge based on execution time, reducing execution time also means reducing costs.</p>

<p><strong>The worker threads version also costs 75% less than the single-threaded version!</strong></p>

<h2>Conclusion</h2>

<p><a href="https://foundation.nodejs.org/announcements/2019/04/24/node-js-foundation-and-js-foundation-merge-to-form-openjs-foundation-2">Node.js v12</a> was released in April 2019. This version included support for <a href="https://nodejs.org/api/worker_threads.html">Worker Threads</a>, that were enabled by default (rather than needing an optional runtime flag). Using multiple CPU cores in Node.js applications has never been easier!</p>

<p>Node.js applications with CPU-intensive workloads can utilise this feature to reduce execution time. Since serverless platforms charge based upon execution time, this is especially useful for Node.js serverless functions. Utilising multiple CPU cores leads, not only to improved performance, but also lower bills.</p>

<p>PRs have been <a href="https://github.com/apache/incubator-openwhisk/pull/4472">opened</a> to enable Node.js v12 as a built-in runtime to the Apache OpenWhisk project. This Docker <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v12">image</a> for the new runtime version is already available on Docker Hub. This means it can be used with any Apache OpenWhisk instance straight away!</p>

<p>Playing with Worker Threads on IBM Cloud Functions allowed me to demonstrate how to speed up performance for CPU-intensive workloads by utilising multiple cores concurrently. Using <a href="https://gist.github.com/jthomas/154a039d52b97d5ed19d4ddac3ff9f43">an example of prime number generation</a>, calculating all primes up to ten million took ~35 seconds with a single thread and ~8 seconds with four threads. This represents a reduction in execution time and cost of 75%!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Finding photos on Twitter using face recognition with TensorFlow.js]]></title>
    <link href="http://jamesthom.as/blog/2018/10/30/finding-photos-on-twitter-using-face-recognition/"/>
    <updated>2018-10-30T09:34:00+00:00</updated>
    <id>http://jamesthom.as/blog/2018/10/30/finding-photos-on-twitter-using-face-recognition</id>
    <content type="html"><![CDATA[<p>As a developer advocate, I spend a lot of time at developer conferences (talking about serverless üòé). Upon returning from each trip, I need to compile a "trip report" on the event for my bosses. This helps demonstrate the value in attending events and that I'm not just accruing air miles and hotel points for fun... üõ´üè®</p>

<p>I always include any social media content people post about my talks in the trip report. This is usually tweets with photos of me on stage. If people are tweeting about your session, I assume they enjoyed it and wanted to share with their followers.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&quot;Servers kill your productivity&quot; <a href="https://twitter.com/thomasj?ref_src=twsrc%5Etfw">@thomasj</a> at <a href="https://twitter.com/hashtag/CodeMobileUK?src=hash&amp;ref_src=twsrc%5Etfw">#CodeMobileUK</a> <a href="https://t.co/Y4NsiBBSxT">pic.twitter.com/Y4NsiBBSxT</a></p>&mdash; Mihai C√ÆrlƒÉnaru (@MCirlanaru) <a href="https://twitter.com/MCirlanaru/status/981170555834441729?ref_src=twsrc%5Etfw">April 3, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p><strong>Finding tweets with photos about your talk from attendees is surprisingly challenging.</strong></p>

<p>Attendees often forget to include your twitter username in their tweets. This means the only way to find those photos is to manually scroll through all the results from the conference hashtag. This is problematic at conferences with thousands of attendees all tweeting during the event. <em>#devrelproblems</em>.</p>

<p>Having become bored of manually trawling through all the tweets for each conference, I had a thought...</p>

<blockquote><p><em>"Can't I write some code to do this for me?"</em></p></blockquote>

<p>This didn't seem like too ridiculous an idea. Twitter has <a href="https://developer.twitter.com/en/docs/tweets/search/overview">an API</a>, which would allow me to retrieve all tweets for a conference hashtag. Once I had all the tweet photos, couldn't I run some magic AI algorithm over the images to tell me if I was in them? ü§î</p>

<p>After a couple of weeks of hacking around (and overcoming numerous challenges) I had (to my own amazement) managed to <a href="https://github.com/jthomas/findme">build a serverless application</a> which can find unlabelled photos of a person on twitter using machine learning with <a href="https://github.com/tensorflow/tfjs">TensorFlow.js</a>.</p>

<p><img src="/images/face-recog/find-me-demo.gif" alt="FindMe Example" /></p>

<p><em>If you just want to try this application yourself, follow the instructions in the Github repo: <a href="https://github.com/jthomas/findme">https://github.com/jthomas/findme</a></em></p>

<h2>architecture</h2>

<p><img src="/images/face-recog/architecture.png" alt="FindMe Architecture Diagram" /></p>

<p>This application has four <a href="https://github.com/jthomas/findme/blob/master/serverless.yml">serverless functions</a> (two API handlers and two backend services) and a client-side application from a static web page. Users log into the <a href="https://github.com/jthomas/findme/tree/master/public">client-side application</a> using Auth0 with their Twitter account. This provides the backend application with the user's profile image and Twitter API credentials.</p>

<p>When the user invokes a search query, the client-side application invokes the API endpoint for the <code>register_search</code> <a href="https://github.com/jthomas/findme/blob/master/schedule_search.js">function</a> with the query terms and twitter credentials. This function registers a new search job in Redis and fires a new <code>search_request</code> trigger event with the query and job id. This job identifier is returned to the client to poll for real-time status updates.</p>

<p>The <code>twitter_search</code> <a href="https://github.com/jthomas/findme/blob/master/twitter_search.js">function</a> is connected to the <code>search_request</code> trigger and invoked for each event. It uses the Twitter Search API to retrieve all tweets for the search terms. If tweets retrieved from the API contain photos, those tweet ids (with photo urls) are fired as new <code>tweet_image</code> trigger events.</p>

<p>The <code>compare_images</code> <a href="https://github.com/jthomas/findme/blob/master/compare_images.js">function</a> is connected to the <code>tweet_image</code> trigger. When invoked, it downloads the user's twitter profile image along with the tweet image and runs face detection against both images, using the <code>face-api.js</code> <a href="https://github.com/justadudewhohacks/face-api.js">library</a>. If any faces in the tweet photo match the face in the user's profile image, tweet ids are written to Redis before exiting.</p>

<p>The client-side web page polls for real-time search results by polling the API endpoint for the <code>search_status</code>  <a href="https://github.com/jthomas/findme/blob/master/search_status.js">function</a> with the search job id. Tweets with matching faces are displayed on the web page using the <a href="https://developer.twitter.com/en/docs/twitter-for-websites/embedded-tweets/overview">Twitter JS library</a>.</p>

<h2>challenges</h2>

<p>Since I had found an <a href="https://github.com/justadudewhohacks/face-api.js">NPM library to handle face detection</a>, I could just use this on a serverless platform by including the library within the zip file used to create my serverless application? Sounds easy, right?!</p>

<p><strong>ahem - not so faas-t.... ‚úã</strong></p>

<p>As discussed in <a href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/">previous blog posts</a>, there are numerous challenges in using TF.js-based libraries on serverless platforms. Starting with making the packages available in the runtime and loading model files to converting images for classification, these libraries are not like using normal NPM modules.</p>

<p><em>Here are the main challenges I had to overcome to make this serverless application work...</em></p>

<h3>using tf.js libraries on a serverless platform</h3>

<p>The <a href="https://github.com/tensorflow/tfjs-node">Node.js backend drivers</a> for TensorFlow.js use a native shared C++ library  (<code>libtensorflow.so</code>) to execute models on the CPU or GPU. This native dependency is compiled for the platform during the <code>npm install</code> process. The shared library file is around 142MB, which is too large to include in the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">deployment package</a> for most serverless platforms.</p>

<p>Normal workarounds for this issue store large dependencies in an object store. These files are dynamically retrieved during cold starts and stored in the runtime filesystem, as shown in this pseudo-code. This workaround does add an additional delay to cold start invocations.</p>

<p>```javascript
let cold_start = false</p>

<p>const library = 'libtensorflow.so'</p>

<p>if (cold_start) {
  const data = from_object_store(library)
  write_to_fs(library, data)
  cold_start = true
}</p>

<p>// rest of function code‚Ä¶
```</p>

<p><strong>Fortunately, I had a better solution using Apache OpenWhisk's support for custom Docker runtimes!</strong></p>

<p>This feature allows serverless applications to use <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md#creating-native-actions">custom Docker images</a> as the runtime environment. Creating custom images with <a href="http://jamesthom.as/blog/2017/08/04/large-applications-on-openwhisk/">large libraries pre-installed</a> means they can be excluded from deployment packages. üíØ</p>

<p>Apache OpenWhisk publishes all existing <a href="https://hub.docker.com/r/openwhisk/">runtime images</a> on Docker Hub. Using existing runtime images as base images means Dockerfiles for custom runtimes are minimal. Here's the Dockerfile needed to build a custom runtime with the TensorFlow.js Node.js backend drivers pre-installed.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs-node
```</p>

<p>Once this image has been built and published on Dockerhub, you can use it when creating new functions.</p>

<p><em>I used this approach to build a <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/tags/">custom TensorFlow.js runtime</a> which is available on Docker Hub: <code>jamesthomas/action-nodejs-v8:tfjs-faceapi</code></em></p>

<p>OpenWhisk actions created using the <code>wsk</code> command-line use a configuration flag (<code>--docker</code>) to specify custom runtime images.</p>

<p><code>
wsk action create classify source.js --docker jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></p>

<p>The OpenWhisk provider plugin for The Serverless Framework also supports <a href="https://github.com/serverless/serverless-openwhisk#custom-runtime-images">custom runtime images</a> through a configuration parameter (<code>image</code>) under the function configuration.</p>

<p>```yaml
service: machine-learning</p>

<p>provider:
  name: openwhisk</p>

<p>functions:
  classify:</p>

<pre><code>handler: source.main
image: jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></pre>

<p>```</p>

<p>Having fixed the issue of library loading on serverless platforms, I could move onto the next problem, loading the pre-trained models... üíΩ</p>

<h3>loading pre-trained models</h3>

<p>Running the <a href="https://github.com/justadudewhohacks/face-api.js#usage-loading-models">example code</a> to load the pre-trained models for face recognition gave me this error:</p>

<p><code>
ReferenceError: fetch is not defined
</code></p>

<p>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I discovered how to manually load TensorFlow.js models from the filesystem using the <code>file://</code> URI prefix. Unfortunately, the <code>face-api.js</code> library doesn't support this feature. Models are <a href="https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/4a7d981dbb37e0d3dabc962e1cbfb6122e535263/src/dom/loadWeightMap.ts#L12">automatically loaded</a> using the <code>fetch</code> HTTP client. This HTTP client is available into modern browsers but not in the Node.js runtime.</p>

<p>Overcoming this issue relies on providing an instance of a compatible HTTP client in the runtime. The <code>node-fetch</code> library is a <a href="https://www.npmjs.com/package/node-fetch">implementation of the fetch client</a> API for the Node.js runtime. By manually installing this module and exporting as a global variable, the library can then use the HTTP client as expected.</p>

<p><code>javascript
// Make HTTP client available in runtime
global.fetch = require('node-fetch')
</code></p>

<p>Model configuration and weight files can then be loaded from the library's Github repository using this URL:</p>

<p>https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/</p>

<p><code>js
faceapi.loadFaceDetectionModel('&lt;GITHUB_URL&gt;')
</code></p>

<h3>face detection in images</h3>

<p>The <code>face-api.js</code> library has a <a href="https://github.com/justadudewhohacks/face-api.js#detecting-faces">utility function</a> (<code>models.allFaces</code>) to automatically detect and calculate descriptors for all faces found in an image. Descriptors are a feature vector (of 128 32-bit float values) which uniquely describes the characteristics of a persons face.</p>

<p><code>javascript
const results = await models.allFaces(input, minConfidence)
</code></p>

<p><em>The input to this function is the input tensor with the RGB values from an image. In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I explained how to convert an image from the filesystem in Node.js to the input tensor needed by the model.</em></p>

<p>Finding a user by comparing their twitter profile against photos from tweets starts by running face detection against both images. By comparing computed descriptor values, a measure of similarity can be established between faces from the images.</p>

<h3>face comparison</h3>

<p>Once the face descriptors have been calculated the library provides a utility function to compute the euclidian distance between two descriptors vectors. If the difference between two face descriptors is less than a threshold value, this is used to identify the same person in both images.</p>

<p>```javascript
const distance = faceapi.euclideanDistance(descriptor1, descriptor2)</p>

<p>if (distance &lt; 0.6)
  console.log('match')
else
  console.log('no match')
```</p>

<p>I've no idea why 0.6 is chosen as the threshold value but this seemed to work for me! Even small changes to this value dramatically reduced the precision and recall rates for my test data. I'm calling it the Goldilocks value, just use it...</p>

<h2>performance</h2>

<p>Once I had the end to end application working, I wanted to make it was fast as possible. By optimising the performance, I could improve the application responsiveness and reduce compute costs for my backend. Time is literally money with serverless platforms.</p>

<h3>baseline performance</h3>

<p>Before attempting to optimise my application, I needed to understand the baseline performance. Setting up experiments to record invocation durations gave me the following average test results.</p>

<ul>
<li><em>Warm invocations</em>: ~5 seconds</li>
<li><em>Cold invocations</em>: ~8 seconds</li>
</ul>


<p>Instrumenting the code with <code>console.time</code> statements revealed execution time was comprised of five main sections.</p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       3200 ms       </td>
<td>       2000 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 8 seconds</strong>   </td>
<td>   <strong>~ 5 seconds</strong>  </td>
</tr>
</tbody>
</table>


<p><em>Initialisation</em> was the delay during cold starts to create the runtime environment and load all the library files and application code. <em>Model Loading</em> recorded the time spent instantiating the TF.js models from the source files. <em>Image Loading</em> was the time spent converting the RGB values from images into input tensors, this happened twice, once for the twitter profile picture and again for the tweet photo. <em>Face Detection</em> is the elapsed time to execute the <code>models.allFaces</code> method and <code>faceapi.euclideanDistance</code> methods for all the detected faces. <em>Everything else</em> is well... everything else.</p>

<p>Since model loading was the largest section, this seemed like an obvious place to start optimising. üìàüìâ</p>

<h3>loading model files from disk</h3>

<p>Overcoming the initial model loading issue relied on manually exposing the expected HTTP client in the Node.js runtime. This allowed models to be dynamically loaded (over HTTP) from the external Github repository. Models files were about 36MB.</p>

<p>My first idea was to load these model files from the filesystem, which should be much faster than downloading from Github. Since I was already building a custom Docker runtime, it was a one-line change to include the model files within the runtime filesystem.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs-node</p>

<p>COPY weights weights
```</p>

<p>Having re-built the image and pushed to Docker Hub, the classification function's runtime environment now included models files in the filesystem.</p>

<p><strong>But how do we make the <code>face-api.js</code> library load models files from the filesystem when it is using a HTTP client?</strong></p>

<p>My solution was to write a <code>fetch</code> client that proxied calls to retrieve files from a HTTP endpoint to the local filesystem. üò± I'd let you decide whether this is a brilliant or terrible idea!</p>

<p>```javascript
global.fetch = async (file) => {
  return {</p>

<pre><code>json: () =&gt; JSON.parse(fs.readFileSync(file, 'utf8')),
arrayBuffer: () =&gt; fs.readFileSync(file)
</code></pre>

<p>  }
}</p>

<p>const model = await models.load('/weights')
```</p>

<p>The <code>face-api.js</code> library only used two methods (<code>json()</code> &amp; <code>arrayBuffer()</code>) from the HTTP client. Stubbing out these methods to proxy <code>fs.readFileSync</code> meant files paths were loaded from the filesystem. Amazingly, this seemed to just work, hurrah!</p>

<p><strong>Implementing this feature and re-running performance tests revealed this optimisation saved about 500 ms from the Model Loading section.</strong></p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       1500 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 4.5 seconds</strong>  </td>
</tr>
</tbody>
</table>


<p>This was less of an improvement than I'd expected. Parsing all the model files and instantiating the internal objects was more computationally intensive than I realised. This performance improvement did improve both cold and warm invocations, which was a bonus.</p>

<p><em>Despite this optimisation, model loading was still the largest section in the classification function...</em></p>

<h3>caching loaded models</h3>

<p>There's a good strategy to use when optimising serverless functions...</p>

<p><img src="/images/face-recog/cache-all-the-things.jpg" alt="CACHE ALL THE THINGS" /></p>

<p>Serverless runtimes re-use runtime containers for consecutive requests, known as warm environments. Using local state, like global variables or the runtime filesystem, to cache data between requests can be used to improve performance during those invocations.</p>

<p>Since model loading was such an expensive process, I wanted to cache initialised models. Using a global variable, I could control whether to trigger model loading or return the pre-loaded models. Warm environments would re-use pre-loaded models and remove model loading delay.</p>

<p>```javascript
const faceapi = require('face-api.js')</p>

<p>let LOADED = false</p>

<p>exports.load = async location => {
  if (!LOADED) {</p>

<pre><code>await faceapi.loadFaceDetectionModel(location)
await faceapi.loadFaceRecognitionModel(location)
await faceapi.loadFaceLandmarkModel(location)

LOADED = true
</code></pre>

<p>  }</p>

<p>  return faceapi
}
```</p>

<p><strong>This performance improvement had a significant impact of the performance for warm invocations. Model loading became "free".</strong> üëç</p>

<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       0 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms x 2      </td>
<td>     500 ms x 2      </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms x 2 </td>
<td> 700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 3 seconds</strong>  </td>
</tr>
</tbody>
</table>


<h3>caching face descriptors</h3>

<p>In the initial implementation, the face comparison function was executing face detection against both the user's twitter profile image and tweet photo for comparison. Since the twitter profile image was the same in each search request, running face detection against this image would always return the same results.</p>

<p>Rather than having this work being redundantly computed in each function, caching the results of the computed face descriptor for the profile image meant it could re-used across invocations. This would reduce by 50% the work necessary in the Image &amp; Model Loading sections.</p>

<p>The <code>face-api.js</code> library returns the face descriptor as a typed array with 128 32-bit float values. Encoding this values as a hex string allows them to be stored and retrieved from Redis. This code was used to convert float values to hex strings, whilst maintaining the exact precision of those float values.</p>

<p>```javascript
const encode = typearr => {
  const encoded = Buffer.from(typearr.buffer).toString('hex')<br/>
  return encoded
}</p>

<p>const decode = encoded => {
  const decoded = Buffer.from(encoded, 'hex')
  const uints = new Uint8Array(decoded)
  const floats = new Float32Array(uints.buffer)
  return floats
}
```</p>

<p>This optimisation improves the performance of most cold invocations and all warm invocations, removing over 1200 ms of computation time to compute the results.</p>

<table>
<thead>
<tr>
<th></th>
<th>                    </th>
<th align="center"> Cold Starts (Cached) </th>
<th align="center">    Warm Starts    </th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td> Initialisation     </td>
<td align="center">       1200 ms        </td>
<td align="center">       0 ms        |</td>
</tr>
<tr>
<td></td>
<td> Model Loading      </td>
<td align="center">       2700 ms        </td>
<td align="center">      1500 ms      |</td>
</tr>
<tr>
<td></td>
<td> Image Loading      </td>
<td align="center">        500 ms        </td>
<td align="center">      500 ms       |</td>
</tr>
<tr>
<td></td>
<td> Face Detection     </td>
<td align="center">   700 ms - 900 ms    </td>
<td align="center">  700 ms - 900 ms  |</td>
</tr>
<tr>
<td></td>
<td> Everything Else    </td>
<td align="center">       1000 ms        </td>
<td align="center">      500 ms       |</td>
</tr>
<tr>
<td></td>
<td> <strong>Total Duration</strong> </td>
<td align="center">   <strong>~ 6 seconds</strong>    </td>
<td align="center"> <strong>~ 2.5 seconds</strong> |</td>
</tr>
</tbody>
</table>


<table>
<thead>
<tr>
<th></th>
<th> Cold Starts </th>
<th> Warm Starts </th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation    </td>
<td>       1200 ms       </td>
<td>        0 ms</td>
</tr>
<tr>
<td>Model Loading     </td>
<td>       2700 ms       </td>
<td>       0 ms       </td>
</tr>
<tr>
<td>Image Loading     </td>
<td>     500 ms      </td>
<td>     500 ms       </td>
</tr>
<tr>
<td>Face Detection    </td>
<td> 700 ms - 900 ms  </td>
<td> 700 ms - 900 ms </td>
</tr>
<tr>
<td>Everything Else   </td>
<td>       1000 ms       </td>
<td>       500 ms        </td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td>   <strong>~ 7.5 seconds</strong>   </td>
<td>   <strong>~ 3 seconds</strong>  </td>
</tr>
</tbody>
</table>


<h3>final results + cost</h3>

<p>Application performance was massively improved with all these optimisations. As demonstrated in the video above, the application could process tweets in real-time, returning almost instant results. Average invocation durations were now.</p>

<ul>
<li><em>Warm invocations</em>: ~2.5 seconds</li>
<li><em>Cold invocations (Cached)</em>: ~6 seconds</li>
</ul>


<p>Serverless platforms charge for compute time by the millisecond, so these improvements led to cost savings of 25% for cold invocations (apart the first classification for a user) and 50% for warm invocations.</p>

<p>Classification functions used 512MB of RAM which meant IBM Cloud Functions would provide 320,000 "warm" classifications or 133,333 "cold" classifications within the free tier each month. Ignoring the free tier, 100,000 "warm" classifications would cost $5.10 and 100,000 "cold" classifications $2.13.</p>

<h2>conclusion</h2>

<p>Using TensorFlow.js with serverless cloud platforms makes it easy to build scalable machine learning applications in the cloud. Using the horizontal scaling capabilities of serverless platforms, thousands of model classifications can be ran in parallel. This can be more performant than having dedicated hardware with a GPU, especially with compute costs for serverless applications being so cheap.</p>

<p>TensorFlow.js is ideally suited to serverless application due to the JS interface, (relatively) small library size and availability of pre-trained models. Despite having no prior experience in Machine Learning, I was able to use the library to build a face recognition pipeline, processing 100s of images in parallel, for real-time results. This amazing library opens up machine learning to a whole new audience!</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Serverless Machine Learning With TensorFlow.js]]></title>
    <link href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/"/>
    <updated>2018-08-13T12:16:00+01:00</updated>
    <id>http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js</id>
    <content type="html"><![CDATA[<p>In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to use <a href="https://js.tensorflow.org/">TensorFlow.js</a> on Node.js to run <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d#file-script-js">visual recognition on images from the local filesystem</a>. TensorFlow.js is a JavaScript version of the open-source machine learning library from Google.</p>

<p>Once I had this working with a local Node.js script, my next idea was to convert it into a serverless function. Running this function on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> (<a href="https://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) would turn the script into my own visual recognition microservice.</p>

<p>{% img /images/tfjs-serverless/tf-js-example.gif Serverless TensorFlow.js Function %}</p>

<p>Sounds easy, right? It's just a JavaScript library? So, zip it up and away we go... <strong><em>ahem</em></strong> üëä</p>

<p><em>Converting the image classification script to run in a serverless environment had the following challenges...</em></p>

<ul>
<li><strong>TensorFlow.js libraries need to be available in the runtime.</strong></li>
<li><strong>Native bindings for the library must be compiled against the platform architecture.</strong></li>
<li><strong>Models files need to be loaded from the filesystem.</strong></li>
</ul>


<p>Some of these issues were more challenging than others to fix! Let's start by looking at the details of each issue, before explaining how <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">Docker support</a> in Apache OpenWhisk can be used to resolve them all.</p>

<h2>Challenges</h2>

<h3>TensorFlow.js Libraries</h3>

<p>TensorFlow.js libraries are not included in the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js runtimes</a> provided by the Apache OpenWhisk.</p>

<p>External libraries <a href="http://jamesthom.as/blog/2016/11/28/npm-modules-in-openwhisk/">can be imported</a> into the runtime by deploying applications from a zip file. Custom <code>node_modules</code> folders included in the zip file will be extracted in the runtime. Zip files are limited to a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">maximum size of 48MB</a>.</p>

<h4>Library Size</h4>

<p>Running <code>npm install</code> for the TensorFlow.js libraries used revealed the first problem... the resulting <code>node_modules</code> directory was 175MB. üò±</p>

<p>Looking at the contents of this folder, the <code>tfjs-node</code> module compiles a <a href="https://github.com/tensorflow/tfjs-node/tree/master/src">native shared library</a> (<code>libtensorflow.so</code>) that is 135M. This means no amount of JavaScript minification is going to get those external dependencies under the magic 48 MB limit. üëé</p>

<h4>Native Dependencies</h4>

<p>The <code>libtensorflow.so</code> native shared library must be compiled using the platform runtime. Running <code>npm install</code>  locally automatically compiles native dependencies against the host platform. Local environments may use different CPU architectures (Mac vs Linux) or link against shared libraries not available in the serverless runtime.</p>

<h3>MobileNet Model Files</h3>

<p>TensorFlow models files <a href="https://js.tensorflow.org/tutorials/model-save-load.html">need loading from the filesystem</a> in Node.js. Serverless runtimes do provide a temporary filesystem inside the runtime environment. Files from deployment zip files are automatically extracted into this environment before invocations. There is no external access to this filesystem outside the lifecycle of the serverless function.</p>

<p>Models files for the MobileNet model were 16MB. If these files are included in the deployment package, it leaves 32MB for the rest of the application source code. Although the model files are small enough to include in the zip file, what about the TensorFlow.js libraries? Is this the end of the blog post? Not so fast....</p>

<p><strong>Apache OpenWhisk's support for custom runtimes provides a simple solution to all these issues!</strong></p>

<h2>Custom Runtimes</h2>

<p>Apache OpenWhisk uses Docker containers as the runtime environments for serverless functions (actions). All platform runtime images are <a href="https://hub.docker.com/r/openwhisk/">published on Docker Hub</a>, allowing developers to start these environments locally.</p>

<p>Developers can also <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">specify custom runtime images</a> when creating actions. These images must be publicly available on Docker Hub. Custom runtimes have to expose the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">same HTTP API</a> used by the platform for invoking actions.</p>

<p>Using platform runtime images as <a href="https://docs.docker.com/glossary/?term=parent%20image">parent images</a> makes it simple to build custom runtimes. Users can run commands during the Docker build to install additional libraries and other dependencies. The parent image already contains source files with the HTTP API service handling platform requests.</p>

<h3>TensorFlow.js Runtime</h3>

<p>Here is the Docker build file for the Node.js action runtime with additional TensorFlow.js dependencies.</p>

<p>```
FROM openwhisk/action-nodejs-v8:latest</p>

<p>RUN npm install @tensorflow/tfjs @tensorflow-models/mobilenet @tensorflow/tfjs-node jpeg-js</p>

<p>COPY mobilenet mobilenet
```</p>

<p><code>openwhisk/action-nodejs-v8:latest</code> is the Node.js action runtime image <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">published by OpenWhisk</a>.</p>

<p>TensorFlow libraries and other dependencies are installed using <code>npm install</code> in the build process. Native dependencies for the <code>@tensorflow/tfjs-node</code> library are automatically compiled for the correct platform by installing during the build process.</p>

<p>Since I'm building a new runtime, I've also added the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model files</a> to the image. Whilst not strictly necessary, removing them from the action zip file reduces deployment times.</p>

<p><strong><em>Want to skip the next step? Use this image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/"><code>jamesthomas/action-nodejs-v8:tfjs</code></a> rather than building your own.</em></strong></p>

<h3>Building The Runtime</h3>

<p><em>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to download model files from the public storage bucket.</em></p>

<ul>
<li>Download a version of the MobileNet model and place all files in the <code>mobilenet</code> directory.</li>
<li>Copy the Docker build file from above to a local file named <code>Dockerfile</code>.</li>
<li>Run the Docker <a href="https://docs.docker.com/engine/reference/commandline/build/">build command</a> to generate a local image.</li>
</ul>


<p><code>sh
docker build -t tfjs .
</code></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/tag/">Tag the local image</a> with a remote username and repository.</li>
</ul>


<p><code>sh
docker tag tfjs &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p><em>Replace <code>&lt;USERNAME&gt;</code> with your Docker Hub username.</em></p>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/push/">Push the local image</a> to Docker Hub</li>
</ul>


<p><code>sh
 docker push &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></p>

<p>Once the image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/">is available</a> on Docker Hub, actions can be created using that runtime image. üòé</p>

<h2>Example Code</h2>

<p>This source code implements image classification as an OpenWhisk action. Image files are provided as a Base64 encoded string using the <code>image</code> property on the event parameters. Classification results are returned as the <code>results</code> property in the response.</p>

<script src="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5.js"></script>


<h3>Caching Loaded Models</h3>

<p>Serverless platforms initialise runtime environments on-demand to handle invocations. Once a runtime environment has been created, it will be <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">re-used for further invocations</a> with some limits. This improves performance by removing the initialisation delay ("cold start") from request processing.</p>

<p>Applications can exploit this behaviour by using global variables to maintain state across requests. This is often use to <a href="https://blog.rowanudell.com/database-connections-in-lambda/">cache opened database connections</a> or store initialisation data loaded from external systems.</p>

<p>I have used this pattern to <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L80-L82">cache the MobileNet model</a> used for classification. During cold invocations, the model is loaded from the filesystem and stored in a global variable. Warm invocations then use the existence of that global variable to skip the model loading process with further requests.</p>

<p>Caching the model reduces the time (and therefore cost) for classifications on warm invocations.</p>

<h3>Memory Leak</h3>

<p>Running the Node.js script from blog post on IBM Cloud Functions was possible with minimal modifications. Unfortunately, performance testing revealed a memory leak in the handler function. üò¢</p>

<p><em>Reading more about <a href="https://js.tensorflow.org/tutorials/core-concepts.html">how TensorFlow.js works</a> on Node.js uncovered the issue...</em></p>

<p>TensorFlow.js's Node.js extensions use a native C++ library to execute the Tensors on a CPU or GPU engine. Memory allocated for Tensor objects in the native library is retained until the application explicitly releases it or the process exits. TensorFlow.js provides a <code>dispose</code> method on the individual objects to free allocated memory. There is also a <code>tf.tidy</code> method to automatically clean up all allocated objects within a frame.</p>

<p>Reviewing the code, tensors were being created as <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L51-L59">model input from images</a> on each request. These objects were not disposed before returning from the request handler. This meant native memory grew unbounded. Adding an explicit <code>dispose</code> call to free these objects before returning <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L91">fixed the issue</a>.</p>

<h3>Profiling &amp; Performance</h3>

<p>Action code records memory usage and elapsed time at different stages in classification process.</p>

<p>Recording <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L12-L20">memory usage</a> allows me to modify the maximum memory allocated to the function for optimal performance and cost. Node.js provides a <a href="https://nodejs.org/docs/v0.4.11/api/all.html#process.memoryUsage">standard library API</a> to retrieve memory usage for the current process. Logging these values allows me to inspect memory usage at different stages.</p>

<p>Timing <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L71">different tasks</a> in the classification process, i.e. model loading, image classification, gives me an insight into how efficient classification is compared to other methods. Node.js has a <a href="https://nodejs.org/api/console.html#console_console_time_label">standard library API</a> for timers to record and print elapsed time to the console.</p>

<h2>Demo</h2>

<h3>Deploy Action</h3>

<ul>
<li>Run the following command with the <a href="https://console.bluemix.net/openwhisk/learn/cli">IBM Cloud CLI</a> to create the action.</li>
</ul>


<p><code>sh
ibmcloud fn action create classify --docker &lt;IMAGE_NAME&gt; index.js
</code></p>

<p><em>Replace <code>&lt;IMAGE_NAME&gt;</code> with the public Docker Hub image identifier for the custom runtime. Use <code>jamesthomas/action-nodejs-v8:tfjs</code> if you haven't built this manually.</em></p>

<h3>Testing It Out</h3>

<ul>
<li>Download <a href="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG">this image</a> of a Panda from Wikipedia.</li>
</ul>


<p>{% img https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG %}</p>

<p><code>sh
wget http://bit.ly/2JYSal9 -O panda.jpg
</code></p>

<ul>
<li>Invoke the action with the Base64 encoded image as an input parameter.</li>
</ul>


<p><code>sh
 ibmcloud fn action invoke classify -r -p image $(base64 panda.jpg)
</code></p>

<ul>
<li>Returned JSON message contains classification probabilities. üêºüêºüêº</li>
</ul>


<p>```json
{
  "results":  [{</p>

<pre><code>className: 'giant panda, panda, panda bear, coon bear',
probability: 0.9993536472320557
</code></pre>

<p>  }]
}
```</p>

<h3>Activation Details</h3>

<ul>
<li>Retrieve logging output for the last activation to show performance data.</li>
</ul>


<p><code>sh
ibmcloud fn activation logs --last
</code></p>

<p><strong><em>Profiling and memory usage details are logged to stdout</em></strong></p>

<p><code>sh
prediction function called.
memory used: rss=150.46 MB, heapTotal=32.83 MB, heapUsed=20.29 MB, external=67.6 MB
loading image and model...
decodeImage: 74.233ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=40.63 MB
imageByteArray: 5.676ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.05 MB, external=45.51 MB
imageToInput: 5.952ms
memory used: rss=141.8 MB, heapTotal=24.33 MB, heapUsed=19.06 MB, external=45.51 MB
mn_model.classify: 274.805ms
memory used: rss=149.83 MB, heapTotal=24.33 MB, heapUsed=20.57 MB, external=45.51 MB
classification results: [...]
main: 356.639ms
memory used: rss=144.37 MB, heapTotal=24.33 MB, heapUsed=20.58 MB, external=45.51 MB
</code></p>

<p><code>main</code> is the total elapsed time for the action handler. <code>mn_model.classify</code> is the elapsed time for the image classification. Cold start requests print an extra log message with model loading time, <code>loadModel: 394.547ms</code>.</p>

<h2>Performance Results</h2>

<p>Invoking the <code>classify</code> action 1000 times for both cold and warm activations (using 256MB memory) generated the following performance results.</p>

<h3>warm invocations</h3>

<p>{% img /images/tfjs-serverless/warm-activations.png Warm Activation Performance Results %}</p>

<p>Classifications took an average of <strong>316 milliseconds to process when using warm environments</strong>. Looking at the timing data, converting the Base64 encoded JPEG into the input tensor took around 100 milliseconds. Running the model classification task was in the 200 - 250 milliseconds range.</p>

<h3>cold invocations</h3>

<p>{% img /images/tfjs-serverless/cold-activations.png Cold Activation Performance Results %}</p>

<p>Classifications took an average of <strong>1260 milliseconds to process when using cold environments</strong>. These requests incur penalties for initialising new runtime containers and loading models from the filesystem. Both of these tasks took around 400 milliseconds each.</p>

<p><em>One disadvantage of using custom runtime images in Apache OpenWhisk is the lack of <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">pre-warmed containers</a>. Pre-warming is used to reduce cold start times by starting runtime containers before they are needed. This is not supported for non-standard runtime images.</em></p>

<h3>classification cost</h3>

<p>IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">provides a free tier</a> of 400,000 GB/s per month. Each further second of execution is charged at $0.000017 per GB of memory allocated. Execution time is rounded up to the nearest 100ms.</p>

<p>If all activations were warm, a user could execute <strong>more than 4,000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 600,000 further invocations would cost just over $1.</p>

<p>If all activations were cold, a user could execute <strong>more than 1,2000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 180,000 further invocations would cost just over $1.</p>

<h2>Conclusion</h2>

<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>

<p>Getting a local script to run image classification was relatively simple, but converting to a serverless function came with more challenges! Apache OpenWhisk restricts the maximum application size to 50MB and native libraries dependencies were much larger than this limit.</p>

<p>Fortunately, Apache OpenWhisk's custom runtime support allowed us to resolve all these issues. By building a custom runtime with native dependencies and models files, those libraries can be used on the platform without including them in the deployment package.</p>
]]></content>
  </entry>
  
</feed>
