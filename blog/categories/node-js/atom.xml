<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: node js | James Thomas]]></title>
  <link href="http://jamesthom.as/blog/categories/node-js/atom.xml" rel="self"/>
  <link href="http://jamesthom.as/"/>
  <updated>2017-08-04T16:17:47+01:00</updated>
  <id>http://jamesthom.as/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Cloud Foundry Custom Buildpacks]]></title>
    <link href="http://jamesthom.as/blog/2015/03/04/cloud-foundry-custom-buildpacks/"/>
    <updated>2015-03-04T13:25:00+00:00</updated>
    <id>http://jamesthom.as/blog/2015/03/04/cloud-foundry-custom-buildpacks</id>
    <content type="html"><![CDATA[<p>Cloud Foundry <a href="http://docs.cloudfoundry.org/buildpacks/">Buildpacks</a> provide runtime and framework support for applications. Users can rely on the built-in selection for Java, NodeJS, Python, etc. or additional <a href="https://github.com/cloudfoundry-community/cf-docs-contrib/wiki/Buildpacks">community buildpacks</a> from Github.</p>

<p>Buildpacks are open-source, making them simple to customise and include  libraries needed by your application.</p>

<p><a href="https://github.com/jthomas/doctor-watson">Doctor Watson</a> uses an <a href="https://www.npmjs.com/package/sox">NPM module</a> that relies on a command-line application, <a href="sox.sourceforge.net">SOX</a>, being installed in the runtime environment.</p>

<p>Making this command-line application available on the platform required the project to create a <a href="https://github.com/jthomas/nodejs-buildpack">custom NodeJS buildpack</a>.</p>

<p>This was the first time I've needed to create a custom buildpack. Documenting the steps below will hopefully provide a guide for other people wanting to do the same.</p>

<p>Overall, the process was straightforward and left me with a greater understanding of how buildpacks works.</p>

<h2>SOX Audio Processing Library</h2>

<p>We're using the SOX package within Doctor Watson to up-sample an audio file.
This module depends on the command-line SOX audio processing utility being installed and available on the command line. SOX is an open-source C application.</p>

<h2>Buildpack Internals</h2>

<p>Cloud Foundry Buildpacks are Git repositories which must contain three shell scripts under the "bin" directory.</p>

<ul>
<li>detect - Does this buildpack apply to this application?</li>
<li>compile - Build the runtime used to execute the application</li>
<li>release - Controls how the application should be executed</li>
</ul>


<p>These shell scripts can be modified to perform any task necessary for an application runtime.</p>

<p>We're starting with the <a href="https://github.com/cloudfoundry/nodejs-buildpack">default NodeJS buildpack</a>.</p>

<p>The <a href="https://github.com/cloudfoundry/nodejs-buildpack/blob/master/bin/compile">"bin/compile"</a> script installs the correct NodeJS version, NPM modules and sets up the runtime environment to start the application. When the script is ran, a command line argument will give a directory path to place files needed at runtime.</p>

<p>We will need to install the SOX binary and dependent libraries under this directory path.</p>

<p>One method for doing this would be downloading the SOX source code and compiling during deployment, before installing the created binaries into the correct location.</p>

<p>Unfortunately, compiling from source during each deployment would add an unacceptable delay.</p>

<p><em>Therefore, most buildpacks use pre-built binaries, which are downloaded and moved to the build directory during deployment, saving a huge amount of time.</em></p>

<h2>Creating the pre-built binary archive</h2>

<p>Rather than manually creating our binaries from source, we can pull them from the Ubuntu package manager which already maintains a pre-built set of binaries for the <a href="https://packages.debian.org/unstable/sound/sox">SOX package</a>.</p>

<p>Packaging the binary and any dynamic libraries dependencies into an archive file, this can be stored in the buildpack repository for extraction during deployment.</p>

<p>We need to ensure the pre-built binaries were compiled for the same host environment that Cloud Foundry will use to run our application.</p>

<p>Using the cf stacks command, we can see the platforms details.</p>

<p>``` sh
[13:51:45 ~]$ cf stacks
Getting stacks in org james.thomas@uk.ibm.com / space dev as james.thomas@uk.ibm.com...
OK</p>

<p>name      description
lucid64   Ubuntu 10.04
seDEA     private
[13:53:10 ~]$
```</p>

<p>Now we just need access to the same platform to run the package manager on...</p>

<p>Docker to the rescue!</p>

<h2>Using Docker</h2>

<p>We're going to use Docker to run a new container with the same operating system as the Cloud Foundry environment. Using this we can install the SOX package using 'apt-get' and extract all the installed files.</p>

<p>``` sh
[13:56:46 ~]$ docker run -t -i  ubuntu:10.04 /bin/bash
root@7fdb1e9047e1:/#
root@7fdb1e9047e1:/# apt-get install sox
root@7fdb1e9047e1:/# which sox
/usr/bin/sox
root@7fdb1e9047e1:/# ldd /usr/bin/sox</p>

<pre><code>linux-vdso.so.1 =&gt;  (0x00007fff2819f000)
libsox.so.1 =&gt; /usr/lib/libsox.so.1 (0x00007f0f32a94000)
libltdl.so.7 =&gt; /usr/lib/libltdl.so.7 (0x00007f0f3288a000)
libdl.so.2 =&gt; /lib/libdl.so.2 (0x00007f0f32685000)
libpng12.so.0 =&gt; /lib/libpng12.so.0 (0x00007f0f3245e000)
libmagic.so.1 =&gt; /usr/lib/libmagic.so.1 (0x00007f0f32242000)
libz.so.1 =&gt; /lib/libz.so.1 (0x00007f0f3202a000)
libgomp.so.1 =&gt; /usr/lib/libgomp.so.1 (0x00007f0f31e1c000)
libgsm.so.1 =&gt; /usr/lib/libgsm.so.1 (0x00007f0f31c0e000)
libm.so.6 =&gt; /lib/libm.so.6 (0x00007f0f3198a000)
libpthread.so.0 =&gt; /lib/libpthread.so.0 (0x00007f0f3176d000)
libc.so.6 =&gt; /lib/libc.so.6 (0x00007f0f313eb000)
/lib64/ld-linux-x86-64.so.2 (0x00007f0f32d28000)
librt.so.1 =&gt; /lib/librt.so.1 (0x00007f0f311e2000)
</code></pre>

<p>root@7fdb1e9047e1:/#
```</p>

<p>Now we have the location of the SOX binary along with a list of the dynamic libraries it depends on.</p>

<p><em>How do we know which of those libraries were already available in the operating system and those the package manager installed?</em></p>

<p>Using Docker diff, we can compare the container to the base image.</p>

<p><code>sh
[14:02:43 ~]$ docker diff 7fdb1e9047e1 | grep '\.so\.'
C /etc/ld.so.cache
C /etc/ld.so.conf.d
A /etc/ld.so.conf.d/libasound2.conf
C /lib/libgcc_s.so.1
A /usr/lib/libFLAC.so.8
A /usr/lib/libFLAC.so.8.2.0
A /usr/lib/libasound.so.2
A /usr/lib/libasound.so.2.0.0
A /usr/lib/libgomp.so.1
A /usr/lib/libgomp.so.1.0.0
....
</code></p>

<p>This command will output list of files that have been modified. Grepping this for the list of dependencies we have, it's easy to extract those which are new.</p>

<p>We can now copy the files needed from the container filesystem to our local host and bundle into an <a href="https://github.com/jthomas/nodejs-buildpack/blob/master/vendor/sox.tar.gz">archive in the "vendor" directory</a>.</p>

<p><code>sh
[14:02:43 ~]$ docker cp 7fdb1e9047e1:/usr/bin/sox .
</code></p>

<h2>Modifying the "bin/compile" script</h2>

<p>With the pre-built binary package available in the buildpack repository, we just need to extract this during deployment from the vendor directory into the build directory.</p>

<p>Modifying the PATH and LD_LIBRARY_PATH variables will expose the binary during runtime and ensure the dynamic libraries are recognised.</p>

<p>``` sh</p>

<h1>Add SOX binary and libraries to path</h1>

<p>status "Adding SOX library support"
tar xzf $bp_dir/vendor/sox.tar.gz -C $build_dir/vendor/</p>

<h1>Update the PATH</h1>

<p>status "Building runtime environment"
mkdir -p $build_dir/.profile.d
echo "export PATH=\"\$HOME/vendor/node/bin:\$HOME/bin:\$HOME/node_modules/.bin:\$HOME/vendor/:\$PATH\";" > $build_dir/.profile.d/nodejs.sh
echo "export LD_LIBRARY_PATH=\"\$HOME/vendor/libs/\";" >> $build_dir/.profile.d/nodejs.sh
```</p>

<h2>Using the custom buildpack</h2>

<p>Once the buildpack modifications have been committed to the <a href="https://github.com/jthomas/nodejs-buildpack">external Github repository</a>, the application manifest can be modified to point to this new location.</p>

<pre>
---
applications:
- name: doctor-watson
  memory: 256M 
  buildpack: https://github.com/jthomas/nodejs-buildpack.git
  command: node app.js
  services:
  - twilio
  - speech_to_text
  - question_and_answer
</pre>


<p>... at this point all we have to do is deploy our application again to take advantage of the modified runtime.</p>

<h2>Conclusion</h2>

<p>Buildpacks are a fantastic feature of the Cloud Foundry, allowing the platform to support for almost any runtime. Using open-source Git repositories means you can build on any existing buildpack.</p>

<p>For Doctor Watson, we were able to add a command line binary, built in another language, to the NodeJS runtime. Docker was a great tool when developing our custom buildpack.</p>

<p>If you want more information on customising buildpacks, check out the Cloud Foundary <a href="http://docs.cloudfoundry.org/buildpacks/custom.html">documentation</a>.</p>

<p>Source code for the custom buildpack we created is available <a href="https://github.com/jthomas/nodejs-buildpack">here</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Doctor Watson]]></title>
    <link href="http://jamesthom.as/blog/2015/02/27/doctor-watson/"/>
    <updated>2015-02-27T15:01:00+00:00</updated>
    <id>http://jamesthom.as/blog/2015/02/27/doctor-watson</id>
    <content type="html"><![CDATA[<p><img src="/images/doctor_watson.png"></p>

<p><a href="http://doctor-watson.mybluemix.net/">Doctor Watson</a> is an IBM Bluemix application to answer medical questions over the phone, using IBM Watson and Twilio.</p>

<p>Ringing an external phone number, the application will answer and ask for a medical question to help with. Translating your speech into text and using IBM Watson's Question and Answer service, Doctor Watson will query the medical corpus.</p>

<p>Top rated answers will be converted to speech and used as a response over the phone. Users can continue to ask more questions for further information.</p>

<p><em>Putting together this complex application, with voice recognition, telephony handling and a medical knowledge base, was incredibly simple using the IBM Bluemix platform.</em></p>

<p>Source code for the application: <a href="https://github.com/jthomas/doctor-watson">https://github.com/jthomas/doctor-watson</a></p>

<p>Fork and deploy the repository to have your own version of Doctor Watson!</p>

<p><a href="https://bluemix.net/deploy?repository=https://github.com/jthomas/doctor-watson" target="_blank"><img src="http://bluemix.net/deploy/button.png" alt="Bluemix button" /></a></p>

<p><strong>Want to know how the project was built? Read on...</strong></p>

<h2>Overview</h2>

<p>Doctor Watson is a NodeJS application using <a href="http://expressjs.com/">Express</a>, a framework for building web applications, to handle serving static content and creating REST APIs.</p>

<p>The front page gives an overview of the application, served from static HTML files with templating to inject the phone number at runtime.</p>

<p>HTTP endpoints handle the incoming messages from Twilio as users make new phone calls. The application's public URL will be bound to a phone number using Twilio's account administration.</p>

<p>Services for IBM Watson are exposed for the application using the IBM Bluemix platform. Text to Speech and Question and Answer services are used during phone call handling.</p>

<p>The application can be deployed on IBM Bluemix, IBM's public hosted Cloud Foundry platform.</p>

<h2>Handling phone calls</h2>

<p><a href="https://www.twilio.com/">Twilio</a> provides "telephony-as-a-service", making applications able to respond to telephone calls and text messages using a REST API.</p>

<p>Twilio has been made available on the IBM Bluemix platform. Binding this service to your application will provide the authentication credentials to use with the Twilio <a href="http://twilio.github.io/twilio-node/">client library</a>.</p>

<p>Users register external phone numbers through the service, which are bound to  external web addresses controlled by the user. When a person dials that number, the service makes a HTTP call with the details to the application. HTTP responses dictates how to handle the phone call, allowing you to record audio from the user, redirect to another number, play an audio file and much more.</p>

<p>We're using ExpressJS to expose the HTTP endpoints used to handle incoming Twilio messages. Twilio's client libraries abstract the low-level network requests behind a JavaScript interface.</p>

<p>This code snippet shows the outline for message processing.</p>

<p>``` javascript
app.post('/some/path', twilio.webhook(twilio_auth_token), function(req, res) {
  // req.body -> XML body parsed to JS object
  // do some processing</p>

<p>  var twiml = new twilio.TwimlResponse();
  twiml.command(...); // where command is a 'verb' from TwilML</p>

<p>  res.send(twiml);<br/>
});
```</p>

<p>TwilML is <a href="https://www.twilio.com/docs/api/twiml">Twilio Markup Language</a>, an XML message with instructions you can use to tell Twilio how to handle incoming phone calls and SMS messages.</p>

<p>TwimlResponse instances generate the XML message responses. Primary verbs from the TwilML specification are available as chainable function calls on the class instance.</p>

<h3>Doctor Watson Call Flow</h3>

<p>When the user first calls the phone number, Twilio sends a TwilML message over a HTTP POST request to http://doctor-watson.mybluemix.net/calls.</p>

<p>``` javascript
router.post('/', twilio.webhook(twilio_auth_token), function(req, res) {
  log.info(req.body.CallSid + "-> calls/");
  log.debug(req.body);</p>

<p>  var twiml = new twilio.TwimlResponse();
  twiml.say('Hello this is Doctor Watson, how can I help you? Press any key after you have finished speaking')</p>

<pre><code>.record({timeout: 60, action: "/calls/recording"});
</code></pre>

<p>  res.send(twiml);<br/>
})
```</p>

<p>We're using the TwilML response to give the user information on asking a question. Recording their response, the audio file with their question will be sent in another request to the '/calls/recording' location.</p>

<p>``` javascript
router.post('/recording', twilio.webhook(twilio_auth_token), function(req, res) {
  var twiml = new twilio.TwimlResponse();</p>

<p>  enqueue_question(req.body);</p>

<p>  twiml.say("Let me think about that.").redirect("/calls/holding");
  res.send(twiml);
})
```</p>

<p>Using the audio file with the user's question, available as the request body, we now schedule a call to the Watson services.</p>

<p>With the question answering request in-progress, we redirect the user into a holding loop.</p>

<p>``` javascript
router.post('/holding', twilio.webhook(twilio_auth_token), function(req, res) {
  var twiml = new twilio.TwimlResponse();
  twiml.pause({length: 5})</p>

<pre><code>.say("I'm still thinking")    
.redirect("/calls/holding");
</code></pre>

<p>  res.send(twiml);<br/>
});
```</p>

<p>Every five seconds, we relay a message over the phone call until the answer has been returned.</p>

<p>Within the callback for the Question and Answer service, we have the following code.</p>

<p><code>javascript
var forward_to = cfenv.getAppEnv().url + "/calls/answer";
client.calls(call_ssid).update({url: forward_to});    
</code></p>

<p>This uses the Twilio client to update the location for a live call, redirecting to the location which returns the answer.</p>

<p>``` javascript
router.post('/answer', twilio.webhook(twilio_auth_token), function(req, res) {
  var twiml = new twilio.TwimlResponse();</p>

<p>  twiml.say(answers[req.body.CallSid])</p>

<pre><code>.say("Do you have another question?")
.record({timeout: 60, action: "/calls/recording"});
</code></pre>

<p>  res.send(twiml);
})
```</p>

<p>Now we've shown how to handle phone calls, let's dig into the Watson services...</p>

<h2>Using the Watson Services</h2>

<p>IBM Bluemix continues to roll out <a href="https://developer.ibm.com/watson/blog/2015/02/04/new-watson-services-available/">more services</a> to the Watson catalogue. There are now fifteen services to help you create cognitive applications.</p>

<p>Doctor Watson uses <a href="http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/speech-to-text.html">Speech to Text</a> and <a href="http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/question-answer.html">Question and Answer</a>.</p>

<p>All services come with <a href="http://www.ibm.com/smarterplanet/us/en/ibmwatson/developercloud/doc/">great documentationn</a> to get help get you started, with sample code, API definitions and client libraries for different languages.</p>

<p>We're using the <a href="https://www.npmjs.com/package/watson-developer-cloud">NodeJS client library</a> in Doctor Watson.</p>

<h3>Converting audio recording to text</h3>

<p>When we have a new audio recording containing the user question, this needs converting into text to query the Watson Q&amp;A service.</p>

<p>Twilio makes the recording available at any external URL as a WAV file with an 8Khz sample rate. Watson's Speech to Text services has a minimum sample rate for audio input for 16Khz.</p>

<p>Before sending the file to the service, we need to up-sample the audio.</p>

<p>Searching for a Node package that might help, uncovered this <a href="https://www.npmjs.com/package/sox">library</a>. Using the SOX audio processing library, we can easily convert audio files between sample rates.</p>

<p>``` javascript
var job = sox.transcode(input, output, {</p>

<pre><code>sampleRate: 16000,
format: 'wav',
channelCount: 1
</code></pre>

<p>  });
  job.on('error', function(err) {</p>

<pre><code>log.error(err);
</code></pre>

<p>  });
  job.on('progress', function(amountDone, amountTotal) {</p>

<pre><code>log.debug("progress", amountDone, amountTotal);
</code></pre>

<p>  });</p>

<p>  job.on('end', function() {</p>

<pre><code>log.debug("Transcoding finished.");    
</code></pre>

<p>  });
  job.start();
```</p>

<p>This package relies on the SOX C library, which isn't installed on the default host environment in IBM Bluemix. Overcoming this hurdle meant creating a <a href="https://github.com/jthomas/nodejs-buildpack">custom NodeJS buildpack</a> which installed the pre-built binaries into the application runtime. I'm saving the details of this for another blog post...</p>

<p>Using the Watson client library, we can send this audio to the external service for converting to text.</p>

<p>``` javascript
var speech_to_text = watson.speech_to_text({
  username: USERNAME,
  password: PASSWORD,
  version: 'v1'
});</p>

<p>var params = {<br/>
  audio: fs.createReadStream(audio),
  content_type: 'audio/l16; rate=16000'
};</p>

<p>speech_to_text.recognize(params, function(err, res) {
  if (err) return;</p>

<p>  var question = res.results[res.result_index],
});
```</p>

<p>... which we can now use to query the healthcare corpus for the Watson Q&amp;A service.</p>

<h3>Getting answers</h3>

<p>When asking questions, we need to set the correct corpus to query for answers.</p>

<p>``` javascript
var question_and_answer_healthcare = watson.question_and_answer({
  username: USERNAME,
  password: PASSWORD,
  version: 'v1',
  dataset: 'healthcare'
});</p>

<p>exports.ask = function (question, cb) {
  question_and_answer_healthcare.ask({ text: question}, function (err, response) {</p>

<pre><code>  if (err) return ;

  var first_answer = response[0].question.evidencelist[0].text;  
  cb(first_answer);    
</code></pre>

<p>  });<br/>
};
```</p>

<p>This triggers the callback we've registered with the first answer in the returned results. Answers are stored as values in a map, with the key as the call ssid, before triggering the redirect to the '/calls/answer' location shown above.</p>

<h2>Deploying to Bluemix</h2>

<p>Hosting the application on IBM Bluemix uses the following manifest file to configure the application at runtime.</p>

<pre>
---
applications:
- name: doctor-watson
  memory: 256M 
  buildpack: https://github.com/jthomas/nodejs-buildpack.git
  command: node app.js
  services:
  - twilio
  - speech_to_text
  - question_and_answer
</pre>


<p>We're binding the services to the application and specifying the custom buildpack to expose the SOX library at runtime.</p>

<p>Following this...</p>

<p><code>sh
$ cf push // and Doctor Watson is live!
</code></p>

<p>Check out the source code here for further details. You can even run your own version of the application, follow the instructions in the README.md</p>
]]></content>
  </entry>
  
</feed>
