<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: node.js | James Thomas]]></title>
  <link href="http://jthomas.github.com/jthomas/blog/categories/node-js/atom.xml" rel="self"/>
  <link href="http://jthomas.github.com/jthomas/"/>
  <updated>2018-08-07T09:56:54+01:00</updated>
  <id>http://jthomas.github.com/jthomas/</id>
  <author>
    <name><![CDATA[James Thomas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Machine Learning In Node.js With TensorFlow.js]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/"/>
    <updated>2018-08-07T09:52:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js</id>
    <content type="html"><![CDATA[<p><a href="https://js.tensorflow.org/">TensorFlow.js</a> is a new version of the popular open-source library which brings deep learning to JavaScript. Developers can now define, train, and run machine learning models using the <a href="https://js.tensorflow.org/api/0.12.0/">high-level library API</a>.</p>

<p><a href="https://github.com/tensorflow/tfjs-models/">Pre-trained models</a> mean developers can now easily perform complex tasks like <a href="https://emojiscavengerhunt.withgoogle.com/">visual recognition</a>, <a href="https://magenta.tensorflow.org/demos/performance_rnn/index.html#2|2,0,1,0,1,1,0,1,0,1,0,1|1,1,1,1,1,1,1,1,1,1,1,1|1,1,1,1,1,1,1,1,1,1,1,1|false">generating music</a> or <a href="https://storage.googleapis.com/tfjs-models/demos/posenet/camera.html">detecting human poses</a> with just a few lines of JavaScript.</p>

<p>Having started as a front-end library for web browsers, recent updates added <a href="https://github.com/tensorflow/tfjs-node">experimental support</a> for Node.js. This allows TensorFlow.js to be used in backend JavaScript applications without having to use Python.</p>

<p><em>Reading about the library, I wanted to test it out with a simple task...</em> üßê</p>

<blockquote><p> <strong>Use TensorFlow.js to perform visual recognition on images using JavaScript from Node.js</strong></p></blockquote>

<p>Unfortunately, most of the <a href="https://js.tensorflow.org/#getting-started">documentation</a> and <a href="https://js.tensorflow.org/tutorials/webcam-transfer-learning.html">example code</a> provided uses the library in a browser. <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">Project utilities</a> provided to simplify loading and using pre-trained models have not yet been extended with Node.js support. Getting this working did end up with me spending a lot of time reading the Typescript source files for the library. üëé</p>

<p>However, after a few days' hacking, I managed to get <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d">this completed</a>! Hurrah! ü§©</p>

<p><em>Before we dive into the code, let's start with an overview of the different TensorFlow libraries.</em></p>

<h2>TensorFlow</h2>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> is an open-source software library for machine learning applications. TensorFlow can be used to implement neural networks and other deep learning algorithms.</p>

<p>Released by Google in November 2015, TensorFlow was originally a <a href="https://www.tensorflow.org/api_docs/python/">Python library</a>. It used either CPU or GPU-based computation for training and evaluating machine learning models. The library was initially designed to run on high-performance servers with expensive GPUs.</p>

<p>Recent updates have extended the software to run in resource-constrained environments like mobile devices and web browsers.</p>

<h3>TensorFlow Lite</h3>

<p><a href="https://www.tensorflow.org/mobile/tflite/">Tensorflow Lite</a>, a lightweight version of the library for mobile and embedded devices, was released in May 2017. This was accompanied by a new series of pre-trained deep learning models for vision recognition tasks, called <a href="https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html">MobileNet</a>. MobileNet models were designed to work efficiently in resource-constrained environments like mobile devices.</p>

<h3>TensorFlow.js</h3>

<p>Following Tensorflow Lite, <a href="https://medium.com/tensorflow/introducing-tensorflow-js-machine-learning-in-javascript-bf3eab376db">TensorFlow.js</a> was announced in March 2018. This version of the library was designed to run in the browser, building on an earlier project called <a href="https://twitter.com/deeplearnjs">deeplearn.js</a>. WebGL provides GPU access to the library. Developers use a JavaScript API to train, load and run models.</p>

<p>TensorFlow.js was recently extended to run on Node.js, using an <a href="https://github.com/tensorflow/tfjs-node">extension library</a> called <code>tfjs-node</code>.</p>

<p><em>The Node.js extension is an alpha release and still under active development.</em></p>

<h4>Importing Existing Models Into TensorFlow.js</h4>

<p>Existing TensorFlow and Keras models can be executed using the TensorFlow.js library. Models need converting to a new format <a href="https://github.com/tensorflow/tfjs-converter">using this tool</a> before execution. Pre-trained and converted models for image classification, pose detection and k-nearest neighbours are <a href="https://github.com/tensorflow/tfjs-models">available on Github</a>.</p>

<h2>Using TensorFlow.js in Node.js</h2>

<h3>Installing TensorFlow Libraries</h3>

<p>TensorFlow.js can be installed from the <a href="https://www.npmjs.com/">NPM registry</a>.</p>

<ul>
<li><code>@tensorflow/tfjs</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs">Core TensorFlow.js library</a></li>
<li><code>@tensorflow/tfjs-node</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs-node">TensorFlow.js Node.js extension</a></li>
<li><code>@tensorflow/tfjs-node-gpu</code> - <a href="https://www.npmjs.com/package/@tensorflow/tfjs-node-gpu">TensorFlow.js Node.js extension with GPU support</a></li>
</ul>


<p><code>
npm install @tensorflow/tfjs @tensorflow/tfjs-node
// or...
npm install @tensorflow/tfjs @tensorflow/tfjs-node-gpu
</code></p>

<p>Both Node.js extensions use native dependencies which will be compiled on demand.</p>

<h3>Loading TensorFlow Libraries</h3>

<p>TensorFlow's <a href="https://js.tensorflow.org/api/0.12.0/">JavaScript API</a> is exposed from the core library. Extension modules to enable Node.js support do not expose additional APIs.</p>

<p><code>javascript
const tf = require('@tensorflow/tfjs')
// Load the binding (CPU computation)
require('@tensorflow/tfjs-node')
// Or load the binding (GPU computation)
require('@tensorflow/tfjs-node-gpu')
</code></p>

<h3>Loading TensorFlow Models</h3>

<p>TensorFlow.js provides an <a href="https://github.com/tensorflow/tfjs-models">NPM library</a> (<code>tfjs-models</code>) to ease loading pre-trained &amp; converted models for <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">image classification</a>, <a href="https://github.com/tensorflow/tfjs-models/tree/master/posenet">pose detection</a> and <a href="https://github.com/tensorflow/tfjs-models/tree/master/knn-classifier">k-nearest neighbours</a>.</p>

<p>The <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model</a> used for image classification is a deep neural network trained to <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">identify 1000 different classes</a>.</p>

<p>In the project's README, the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet#via-npm">following example code</a> is used to load the model.</p>

<p>```javascript
import * as mobilenet from '@tensorflow-models/mobilenet';</p>

<p>// Load the model.
const model = await mobilenet.load();
```</p>

<p><strong>One of the first challenges I encountered was that this does not work on Node.js.</strong></p>

<p><code>
Error: browserHTTPRequest is not supported outside the web browser.
</code></p>

<p>Looking at the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L27">source code</a>, the <code>mobilenet</code> library is a wrapper around the underlying <code>tf.Model</code> class. When the <code>load()</code> method is called, it automatically downloads the correct model files from an external HTTP address and instantiates the TensorFlow model.</p>

<p>The Node.js extension does not yet support HTTP requests to dynamically retrieve models. Instead, models must be manually loaded from the filesystem.</p>

<p><em>After reading the source code for the library, I managed to create a work-around...</em></p>

<h4>Loading Models From a Filesystem</h4>

<p>Rather than calling the module's <code>load</code> method, if the <code>MobileNet</code> class is created manually, the auto-generated <code>path</code> variable which contains the HTTP address of the model can be overwritten with a local filesystem path. Having done this, calling the <code>load</code> method on the class instance will trigger the <a href="https://js.tensorflow.org/tutorials/model-save-load.html">filesystem loader class</a>, rather than trying to use the browser-based HTTP loader.</p>

<p><code>javascript
const path = "mobilenet/model.json"
const mn = new mobilenet.MobileNet(1, 1);
mn.path = `file://${path}`
await mn.load()
</code></p>

<p><strong>Awesome, it works!</strong></p>

<p><em>But how where do the models files come from?</em></p>

<h3>MobileNet Models</h3>

<p>Models for TensorFlow.js consist of two file types, a model configuration file stored in JSON and model weights in a binary format. Model weights are often sharded into multiple files for better caching by browsers.</p>

<p>Looking at the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L68-L76">automatic loading code</a> for MobileNet models, models configuration and weight shards are retrieved from a public storage bucket at this address.</p>

<p><code>
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v${version}_${alpha}_${size}/
</code></p>

<p>The template parameters in the URL refer to the model versions listed <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md#pre-trained-models">here</a>. Classification accuracy results for each version are also shown on that page.</p>

<p><em>According to the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L36">source code</a>, only MobileNet v1 models can be loaded using the <code>tensorflow-models/mobilenet</code> library.</em></p>

<p>The HTTP retrieval code loads the <code>model.json</code> file from this location and then recursively fetches all referenced model weights shards. These files are in the format <code>groupX-shard1of1</code>.</p>

<h4>Downloading Models Manually</h4>

<p>Saving all model files to a filesystem can be achieved by retrieving the model configuration file, parsing out the referenced weight files and downloading each weight file manually.</p>

<p><strong>I want to use the MobileNet V1 Module with 1.0 alpha value and image size of 224 pixels.</strong> This gives me the <a href="https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json">following URL</a> for the model configuration file.</p>

<p><code>
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/model.json
</code></p>

<p>Once this file has been downloaded locally, I can use the <a href="https://stedolan.github.io/jq/"><code>jq</code> tool</a> to parse all the weight file names.</p>

<p><code>
$ cat model.json | jq -r ".weightsManifest[].paths[0]"
group1-shard1of1
group2-shard1of1
group3-shard1of1
...
</code></p>

<p>Using the <code>sed</code> tool, I can prefix these names with the HTTP URL to generate URLs for each weight file.</p>

<p><code>
$ cat model.json | jq -r ".weightsManifest[].paths[0]" | sed 's/^/https:\/\/storage.googleapis.com\/tfjs-models\/tfjs\/mobilenet_v1_1.0_224\//'
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group1-shard1of1
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group2-shard1of1
https://storage.googleapis.com/tfjs-models/tfjs/mobilenet_v1_1.0_224/group3-shard1of1
...
</code></p>

<p>Using the <code>parallel</code> and <code>curl</code> commands, I can then download all of these files to my local directory.</p>

<p><code>
cat model.json | jq -r ".weightsManifest[].paths[0]" | sed 's/^/https:\/\/storage.googleapis.com\/tfjs-models\/tfjs\/mobilenet_v1_1.0_224\//' |  parallel curl -O
</code></p>

<h3>Classifying Images</h3>

<p><a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet#via-npm">This example code</a> is provided by TensorFlow.js to demonstrate returning classifications for an image.</p>

<p>```javascript
const img = document.getElementById('img');</p>

<p>// Classify the image.
const predictions = await model.classify(img);
```</p>

<p><strong>This does not work on Node.js due to the lack of a DOM.</strong></p>

<p>The <code>classify</code> <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L143-L155">method</a> accepts numerous DOM elements (<code>canvas</code>, <code>video</code>, <code>image</code>) and will automatically retrieve and convert image bytes from these elements into a <a href="https://js.tensorflow.org/api/latest/index.html#tensor3d"><code>tf.Tensor3D</code> class</a> which is used as the input to the model. Alternatively, the <code>tf.Tensor3D</code> input can be passed directly.</p>

<p><strong>Rather than trying to use an external package to simulate a DOM element in Node.js, I found it easier to construct the <code>tf.Tensor3D</code> manually.</strong></p>

<h4>Generating Tensor3D from an Image</h4>

<p>Reading the <a href="https://github.com/tensorflow/tfjs-core/blob/master/src/kernels/backend_cpu.ts#L126-L140">source code</a> for the method used to turn DOM elements into Tensor3D classes, the following input parameters are used to generate the Tensor3D class.</p>

<p><code>javascript
const values = new Int32Array(image.height * image.width * numChannels);
// fill pixels with pixel channel bytes from image
const outShape = [image.height, image.width, numChannels];
const input = tf.tensor3d(values, outShape, 'int32');
</code></p>

<p><code>pixels</code> is a 2D array of type (Int32Array) which contains a sequential list of channel values for each pixel. <code>numChannels</code> is the number of channel values per pixel.</p>

<h4>Creating Input Values For JPEGs</h4>

<p>The <a href="https://www.npmjs.com/package/jpeg-js"><code>jpeg-js</code> library</a> is a pure javascript JPEG encoder and decoder for Node.js. Using this library the RGB values for each pixel can be extracted.</p>

<p><code>javascript
const pixels = jpeg.decode(buffer, true);
</code></p>

<p>This will return a <code>Uint8Array</code> with four channel values (<code>RGBA</code>) for each pixel (<code>width * height</code>). The MobileNet model only uses the three colour channels (<code>RGB</code>) for classification, ignoring the alpha channel. This code converts the four channel array into the correct three channel version.</p>

<p>```javascript
const numChannels = 3;
const numPixels = image.width * image.height;
const values = new Int32Array(numPixels * numChannels);</p>

<p>for (let i = 0; i &lt; numPixels; i++) {
  for (let channel = 0; channel &lt; numChannels; ++channel) {</p>

<pre><code>values[i * numChannels + channel] = pixels[i * 4 + channel];
</code></pre>

<p>  }
}
```</p>

<h4>MobileNet Models Input Requirements</h4>

<p>The <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md#mobilenet_v1">MobileNet model</a> being used classifies images of width and height 224 pixels. Input tensors must contain float values, between -1 and 1, for each of the three channels pixel values.</p>

<p>Input values for images of different dimensions needs to be re-sized before classification. Additionally, pixels values from the JPEG decoder are in the range <em>0 - 255</em>, rather than <em>-1 to 1</em>. These values also need converting prior to classification.</p>

<p><strong>TensorFlow.js has library methods to make this process easier but, fortunately for us, the <code>tfjs-models/mobilenet</code> library <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/index.ts#L103-L114">automatically handles</a> this issue!</strong> üëç</p>

<p>Developers can pass in Tensor3D inputs of type <code>int32</code>  and different dimensions to the  <code>classify</code> method and it converts the input to the correct format prior to classification. Which means there's nothing to do... Super üï∫üï∫üï∫.</p>

<h4>Obtaining Predictions</h4>

<p>MobileNet models in Tensorflow are trained to recognise entities from the <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">top 1000 classes</a> in the <a href="http://image-net.org/">ImageNet</a> dataset. The models output the probabilities that each of those entities is in the image being classified.</p>

<p><em>The full list of trained classes for the model being used can be found in <a href="https://github.com/tensorflow/tfjs-models/blob/master/mobilenet/src/imagenet_classes.ts">this file</a>.</em></p>

<p>The <code>tfjs-models/mobilenet</code> library exposes a <code>classify</code> method on the <code>MobileNet</code> class to return the top X classes with highest probabilities from an image input.</p>

<p><code>javascript
const predictions = await mn_model.classify(input, 10);
</code></p>

<p><code>predictions</code> is an array of X classes and probabilities in the following format.</p>

<p><code>javascript
{
  className: 'panda',
  probability: 0.9993536472320557
}
</code></p>

<h2>Example</h2>

<p>Having worked how to use the TensorFlow.js library and MobileNet models on Node.js, <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d">this script</a> will classify an image given as a command-line argument.</p>

<h3>source code</h3>

<ul>
<li>Save this script file and package descriptor to local files.</li>
</ul>


<script src="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d.js"></script>


<h3>testing it out</h3>

<ul>
<li><p>Download the model files to a <code>mobilenet</code> directory using the instructions above.</p></li>
<li><p>Install the project dependencies using NPM
<code>
npm install
</code></p></li>
<li><p>Download a sample JPEG file to classify
<code>
wget http://bit.ly/2JYSal9 -O panda.jpg
</code>
<img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG"></p></li>
<li><p>Run the script with the model file and input image as arguments.
<code>
node script.js mobilenet/model.json panda.jpg
</code></p></li>
</ul>


<p><strong>If everything worked, the following output should be printed to the console.</strong></p>

<p>```javascript
classification results: [ {</p>

<pre><code>className: 'giant panda, panda, panda bear, coon bear',
probability: 0.9993536472320557 
</code></pre>

<p>} ]
```</p>

<p>The image is correctly classified as containing a Panda with 99.93% probability! üêºüêºüêº</p>

<h2>Conclusion</h2>

<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>

<p>Having been released as a browser-based library, TensorFlow.js has now been extended to work on Node.js, although not all of the tools and utilities support the new runtime. With a few days' hacking, I was able to use the library with the MobileNet models for visual recognition on images from a local file.</p>

<p>Getting this working in the Node.js runtime means I now move on to my next idea... making this run inside a serverless function! Come back soon to read about my next adventure with TensorFlow.js. üëã</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Debugging Node.js OpenWhisk Actions]]></title>
    <link href="http://jthomas.github.com/jthomas/blog/2018/07/10/debugging-node-dot-js-openwhisk-actions/"/>
    <updated>2018-07-10T09:00:00+01:00</updated>
    <id>http://jthomas.github.com/jthomas/blog/2018/07/10/debugging-node-dot-js-openwhisk-actions</id>
    <content type="html"><![CDATA[<p>Debugging serverless applications is one of the <a href="https://www.stackery.io/blog/the-serverless-learning-curve/">most challenging issues</a> developers face when using serverless platforms. How can you use debugging tools without any access to the runtime environment?</p>

<p>Last week, I worked out <a href="https://twitter.com/thomasj/status/1013006648439443458">how to expose the Node.js debugger</a> in the Docker environment used for the application runtime in Apache OpenWhisk.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Want to use Node.js debugger for <a href="https://twitter.com/openwhisk?ref_src=twsrc%5Etfw">@openwhisk</a> actions? Start runtime container locally with this command to expose v8 inspector.<br>$ docker run -p 8080:8080 -p 9229:9229 -it openwhisk/action-nodejs-v8 node --inspect=0.0.0.0:9229 app.js<br>Then connect Chrome Dev Tools or <a href="https://twitter.com/code?ref_src=twsrc%5Etfw">@code</a>. üíØ <a href="https://t.co/X4i01QEOmg">pic.twitter.com/X4i01QEOmg</a></p>&mdash; James Thomas (@thomasj) <a href="https://twitter.com/thomasj/status/1013006648439443458?ref_src=twsrc%5Etfw">June 30, 2018</a></blockquote>


<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>Using the remote debugging service, we can set breakpoints and step through action handlers live, rather than just being reliant on logs and metrics to diagnose bugs.</p>

<p><strong>So, how does this work?</strong></p>

<p><em>Let's find out more about how Apache OpenWhisk executes serverless functions...</em></p>

<h2>Background</h2>

<p><a href="http://openwhisk.incubator.apache.org/">Apache OpenWhisk</a> is the open-source serverless platform which powers <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a>. OpenWhisk <a href="https://medium.com/openwhisk/uncovering-the-magic-how-serverless-platforms-really-work-3cb127b05f71">uses Docker containers</a> to create isolated runtime environments for executing serverless functions.</p>

<p>Containers are started on-demand as invocation requests arrive. Serverless function source files are dynamically injected into the runtime and executed for each invocation. Between invocations, containers are paused and kept in a cache for re-use with further invocations.</p>

<p>The benefit of using an open-source serverless platform is that the <a href="https://github.com/search?q=incubator-openwhisk-runtime">build files</a> used to create runtime images are also open-source. OpenWhisk also automatically builds and publishes all <a href="https://hub.docker.com/r/openwhisk/">runtime images externally</a> on Docker Hub. Running containers using these images allows us to simulate the remote serverless runtime environment.</p>

<h3>Runtime Images</h3>

<p>All OpenWhisk runtime images are <a href="https://hub.docker.com/r/openwhisk/">published externally</a> on Docker Hub.</p>

<p>Runtime images <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">start a HTTP server</a> which listens on port 8080. This HTTP server must implement two API endpoints (<code>/init</code> &amp; <code>/run</code>) accepting HTTP POST requests. The platform uses these endpoints to initialise the runtime with action code and then invoke the action with event parameters.</p>

<p>More details on the API endpoints can be found in this <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">blog post</a> on creating Docker-based actions.</p>

<h3>Node.js Runtime Image</h3>

<p>This repository contains the source code used to create <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">Node.js runtime environment image</a>.</p>

<p><a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">https://github.com/apache/incubator-openwhisk-runtime-nodejs</a></p>

<p>Both <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejs8Action">Node.js 8</a> and <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejs6Action">6 runtimes</a>  are built from a <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/tree/master/core/nodejsActionBase">common base image</a>. This base image contains an <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/app.js">Express.js server</a> which handles the platform API requests. The <code>app.js</code> file containing the server <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejs8Action/Dockerfile#L28">is executed</a> when the containers starts.</p>

<p>JavaScript code is injected into the runtime using the <code>/init</code> API. Actions created from source code are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L76">dynamically evaluated</a> to instantiate the code in the runtime. Actions created from zip files are <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L54">extracted into a temporary directory</a> and <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61">imported as a Node.js module</a>.</p>

<p>Once instantiated, actions are executed using the <code>/run</code> API. Event parameters are come from the request body. Each time a new request is received, the server <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L95">calls the action handler with event parameters</a>. Returned values are serialised as the JSON body in the API response.</p>

<h3>Starting Node.js Runtime Containers</h3>

<p><a href="https://docs.docker.com/engine/reference/commandline/run/">Use this command</a> to start the Node.js runtime container locally.</p>

<p><code>
$ docker run -it -p 8080:8080 openwhisk/action-nodejs-v8
</code></p>

<p>Once the container has started, port 8080 on localhost will be mapped to the HTTP service exposed by the runtime environment. This can be used to inject serverless applications into the runtime environment and invoke the serverless function handler with event parameters.</p>

<h2>Node.js Remote Debugging</h2>

<p>Modern versions of the Node.js runtime have a command-line flag (<code>--inspect</code>) to expose a <a href="https://nodejs.org/api/debugger.html#debugger_advanced_usage">remote debugging service</a>. This service runs a WebSocket server on localhost which implements the <a href="https://chromedevtools.github.io/devtools-protocol/">Chrome DevTools Protocol</a>.</p>

<p><code>
$ node --inspect index.js
Debugger listening on 127.0.0.1:9229.
</code></p>

<p>External tools can connect to this port to provide debugging capabilities for Node.js code.</p>

<p>Docker images for the OpenWhisk Node.js runtimes use the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejs8Action/Dockerfile#L28">following command</a> to start the internal Node.js process. <strong>Remote debugging is not enabled by default.</strong></p>

<p><code>
node --expose-gc app.js
</code></p>

<p>Docker allows containers to override the default image start command using a <a href="https://docs.docker.com/engine/reference/run/">command line argument</a>.</p>

<p><strong>This command will start the OpenWhisk Node.js runtime container with the remote debugging service enabled.</strong> Binding the HTTP API and WebSocket ports to the host machine allows us to access those services remotely.</p>

<p><code>
docker run -p 8080:8080 -p 9229:9229 -it openwhisk/action-nodejs-v8 node --inspect=0.0.0.0:9229 app.js
</code></p>

<p><em>Once a container from the runtime image has started, we can connect our favourite debugging tools...</em></p>

<h3>Chrome Dev Tools</h3>

<p>To connect <a href="https://developers.google.com/web/tools/chrome-devtools/">Chrome Dev Tools</a> to the remote Node.js debugging service, follow these steps.</p>

<ul>
<li>Open the following page in Chrome: <a href="chrome://inspect/#devices">chrome://inspect/#devices</a></li>
</ul>


<p><img src="/images/debugging/devtools.png" title="Chrome Dev Tools" ></p>

<p>Chrome Dev Tools is configured to open a connection on port 9229 on localhost. If the web socket connection succeeds, the debugging target should be listed in the "Remote Target" section.</p>

<ul>
<li>Click the "<em>Open dedicated DevTools for Node</em>" link.</li>
</ul>


<p>In the "Sources" panel the JavaScript files loaded by the Node.js process are available.</p>

<p><img src="/images/debugging/devtools-debugging.png" title="Chrome Dev Tools Debugging" ></p>

<p>Setting breakpoints in the <code>runner.js</code> file will allow you to halt execution for debugging upon invocations.</p>

<h3>VSCode</h3>

<p><a href="https://code.visualstudio.com/">Visual Studio Code</a> supports remote debugging of Node.js code using the Chrome Dev Tools protocol. Follow these steps to connect the editor to the remote debugging service.</p>

<ul>
<li>Click the menu item "<em>Debug -> Add Configuration</em>"</li>
<li>Select the "<em>Node.js: Attach to Remote Program</em>" from the Intellisense menu.</li>
<li>Edit the default configuration to have the following values.</li>
</ul>


<p><code>json
{
  "type": "node",
  "request": "attach",
  "name": "Attach to Remote",
  "address": "127.0.0.1",
  "port": 9229,
  "localRoot": "${workspaceFolder}"
}
</code></p>

<p><img src="/images/debugging/vscode.png" title="Visual Studio Code" ></p>

<ul>
<li>Choose the new "<em>attach to remote</em>" debugging profile and click the Run button.</li>
</ul>


<p>The "<em>Loaded Scripts</em>" window will show all the JavaScript files loaded by the Node.js process.</p>

<p><img src="/images/debugging/vscode-debugging.png" title="Visual Studio Code Debugging" ></p>

<p>Setting breakpoints in the <code>runner.js</code> file will allow you to halt execution for debugging upon invocations.</p>

<h3>Breakpoint Locations</h3>

<p>Here are some useful locations to set breakpoints to catch errors in your serverless functions for the OpenWhisk Node.js runtime environments.</p>

<h4>Initialisation Errors - Source Actions</h4>

<p>If you are creating OpenWhisk actions from JavaScript source files, the code is dynamically evaluated during  the <code>/init</code> request at <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L76">this location</a>. Putting a breakpoint here will allow you to catch errors thrown during that <code>eval()</code> call.</p>

<h4>Initialisation Errors - Binary Actions</h4>

<p>If you are creating OpenWhisk actions from a zip file containing JavaScript modules, <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L54">this location</a> is where the archive is extracted in the runtime filesystem. Putting a breakpoint here will catch errors from the extraction call and runtime checks for a valid JavaScript module.</p>

<p><a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L61">This code</a> is where the JavaScript module is imported once it has been extracted. Putting a breakpoint here will catch errors thrown importing the module into the Node.js environment.</p>

<h4>Action Handler Errors</h4>

<p>For both source file and zipped module actions, <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs/blob/master/core/nodejsActionBase/runner.js#L95">this location</a> is where the action handler is invoked on each <code>/run</code> request. Putting a breakpoint here will catch errors thrown from within action handlers.</p>

<h2>Invoking OpenWhisk Actions</h2>

<p>Once you have attached the debugger to the remote Node.js process, you need to send the API requests to simulate the platform invocations. Runtime containers use separate HTTP endpoints to import the action source code into the runtime environment (<code>/init</code>)  and then fire the invocation requests (<code>/run</code>).</p>

<h4>Generating Init Request Body - Source Files</h4>

<p>If you are creating OpenWhisk actions from JavaScript source files, send the following JSON body in the HTTP POST to the <code>/init</code> endpoint.</p>

<p>```json
{
  "value": {</p>

<pre><code>"main": "&lt;FUNCTION NAME IN SOURCE FILE&gt;",
"code": "&lt;INSERT SOURCE HERE&gt;"
</code></pre>

<p>  }
}
```</p>

<p><code>code</code> is the JavaScript source to be evaluated which contains the action handler. <code>main</code> is the function name in the source file used for the action handler.</p>

<p>Using the <code>jq</code> <a href="https://stedolan.github.io/jq/">command-line tool</a>, we can create the JSON body for the source code in <code>file.js</code>.</p>

<p><code>sh
$ cat file.js | jq -sR  '{value: {main: "main", code: .}}'
</code></p>

<h4>Generating Init Request Body - Zipped Modules</h4>

<p>If you are creating OpenWhisk actions from a zip file containing JavaScript modules, send the following JSON body in the HTTP POST to the <code>/init</code> endpoint.</p>

<p>```json
{
  "value": {</p>

<pre><code>"main": "&lt;FUNCTION NAME ON JS MODULE&gt;",
"code": "&lt;INSERT BASE64 ENCODED STRING FROM ZIP FILE HERE&gt;",
"binary": true
</code></pre>

<p>  }
}
```</p>

<p><code>code</code> must be a Base64 encoded string for the zip file. <code>main</code> is the function name returned in the imported JavaScript module to call as the action handler.</p>

<p>Using the <code>jq</code> <a href="https://stedolan.github.io/jq/">command-line tool</a>, we can create the JSON body for the zip file in <code>action.zip</code>.</p>

<p><code>sh
$ base64 action.zip | tr -d '\n' | jq -sR '{value: {main: "main", binary: true, code: .}}'
</code></p>

<h4>Sending Init Request</h4>

<p>The <a href="https://httpie.org/">HTTPie</a> tool makes it simple to send HTTP requests from the command-line.</p>

<p>Using this tool, the following command will initialise the runtime container with an OpenWhisk action.</p>

<p>```sh
$ http post localhost:8080/init &lt; init.json
HTTP/1.1 200 OK
...
{</p>

<pre><code>"OK": true
</code></pre>

<p>}
```</p>

<p>If this HTTP request returns without an error, the action is ready to be invoked.</p>

<p><em>No further initialisation requests are needed unless you want to modify the action deployed.</em></p>

<h4>Generating Run Request Body</h4>

<p>Invocations of the action handler functions are triggered from a HTTP POST to the <code>/run</code> API endpoint.</p>

<p>Invocations parameters are sent in the JSON request body, using a JSON object with a <code>value</code> field.</p>

<p>```json
{
  "value": {</p>

<pre><code>"some-param-name": "some-param-value",
"another-param-name": "another-param-value",
</code></pre>

<p>  }
}
```</p>

<h4>Sending Run Request</h4>

<p>Using the <a href="https://httpie.org/">HTTPie</a> tool, the following command will invoke the OpenWhisk action.</p>

<p>```sh
$ http post localhost:8080/run &lt; run.json
HTTP/1.1 200 OK
...
{</p>

<pre><code>"msg": "Hello world"
</code></pre>

<p>}
```</p>

<p>Returned values from the action handler are serialised as the JSON body in the HTTP response. Issuing further HTTP POST requests to the <code>/run</code> endpoint allows us to re-invoke the action.</p>

<h2>Conclusion</h2>

<p>Lack of debugging tools is one of the biggest complaints from developers migrating to serverless platforms.</p>

<p>Using an open-source serverless platform helps with this problem, by making it simple to run the same containers locally that are used for the platform's runtime environments. Debugging tools can then be started from inside these local environments to simulate remote access.</p>

<p>In this example, this approach was used to enable the remote debugging service from the OpenWhisk Node.js runtime environment. The same approach could be used for any language and debugging tool needing local access to the runtime environment.</p>

<p>Having access to the Node.js debugger is huge improvement when debugging challenging issues, rather than just being reliant on logs and metrics collected by the platform.</p>
]]></content>
  </entry>
  
</feed>
