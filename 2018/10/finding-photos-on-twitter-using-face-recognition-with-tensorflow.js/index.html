<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.69.2" />

    
    
    

<title>Finding photos on Twitter using face recognition with TensorFlow.js ‚Ä¢ notes on software.</title>
<meta name="description" content="James Thomas&#39; blog about software. Posts about serverless, cloud platforms, openwhisk, node.js and more...">
<meta name="keywords" content="blog,serverless,openwhisk,ibm,nodejs,tensorflow,serverless framework,open-source,conferences,developer advocacy,github">
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Finding photos on Twitter using face recognition with TensorFlow.js"/>
<meta name="twitter:description" content="Using face recognition to find unlabelled photos on twitter with machine learning. TensorFlow.js and Serverless Cloud Platforms used to search for all tweets for a hashtag and compare images to user&#39;s profile photo. Uses IBM Cloud Functions (Apache OpenWhisk)"/>

<meta property="og:title" content="Finding photos on Twitter using face recognition with TensorFlow.js" />
<meta property="og:description" content="Using face recognition to find unlabelled photos on twitter with machine learning. TensorFlow.js and Serverless Cloud Platforms used to search for all tweets for a hashtag and compare images to user&#39;s profile photo. Uses IBM Cloud Functions (Apache OpenWhisk)" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jamesthom.as/2018/10/finding-photos-on-twitter-using-face-recognition-with-tensorflow.js/" />
<meta property="article:published_time" content="2018-10-30T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-10-30T00:00:00+00:00" />


    






<link rel="stylesheet" href="/scss/hyde-hyde.3081c4981fb69a2783dd36ecfdd0e6ba7a158d4cbfdd290ebce8f78ba0469fc6.css" integrity="sha256-MIHEmB&#43;2mieD3Tbs/dDmunoVjUy/3SkOvOj3i6BGn8Y=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    <style type="text/css">
.content {
  max-width: 48rem;
}

.sidebar .container {
  padding-right: 0rem;
  padding-left: 0rem;
}

h1 {
  margin-top: 0;
}
</style>

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://jamesthom.as">notes on software.</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://jamesthom.asprofile.jpg" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
         by james thomas serverless aficionado. 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">notes on software.</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/thomasj" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/jthomas" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="https://stackoverflow.com/users/1427084" rel="me"><i class="fab fa-stack-overflow fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://medium.com/@jamesthom.as" rel="me"><i class="fab fa-medium fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
        
	<a href="https://www.youtube.com/user/jthomasuk" rel="me"><i class="fab fa-youtube fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="mailto:blog@jamesthom.as" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Finding photos on Twitter using face recognition with TensorFlow.js</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Oct 30, 2018
    
    
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/serverless">serverless</a>
           
      
          <a class="badge badge-tag" href="/tags/openwhisk">openwhisk</a>
           
      
          <a class="badge badge-tag" href="/tags/node.js">node.js</a>
           
      
          <a class="badge badge-tag" href="/tags/machine-learning">machine learning</a>
           
      
          <a class="badge badge-tag" href="/tags/tensorflow">tensorflow</a>
           
      
          <a class="badge badge-tag" href="/tags/javascript">javascript</a>
          
      
    
    
    <br/>
    
    <i class="fas fa-clock"></i> 13 min read
    
</div>


  </header>
  
  
  <div class="post">
    <p>As a developer advocate, I spend a lot of time at developer conferences (talking about serverless üòé). Upon returning from each trip, I need to compile a &ldquo;trip report&rdquo; on the event for my bosses. This helps demonstrate the value in attending events and that I&rsquo;m not just accruing air miles and hotel points for fun&hellip; üõ´üè®</p>
<p>I always include any social media content people post about my talks in the trip report. This is usually tweets with photos of me on stage. If people are tweeting about your session, I assume they enjoyed it and wanted to share with their followers.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">&quot;Servers kill your productivity&quot; <a href="https://twitter.com/thomasj?ref_src=twsrc%5Etfw">@thomasj</a> at <a href="https://twitter.com/hashtag/CodeMobileUK?src=hash&amp;ref_src=twsrc%5Etfw">#CodeMobileUK</a> <a href="https://t.co/Y4NsiBBSxT">pic.twitter.com/Y4NsiBBSxT</a></p>&mdash; Mihai C√ÆrlƒÉnaru (@MCirlanaru) <a href="https://twitter.com/MCirlanaru/status/981170555834441729?ref_src=twsrc%5Etfw">April 3, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p><strong>Finding tweets with photos about your talk from attendees is surprisingly challenging.</strong></p>
<p>Attendees often forget to include your twitter username in their tweets. This means the only way to find those photos is to manually scroll through all the results from the conference hashtag. This is problematic at conferences with thousands of attendees all tweeting during the event. <em>#devrelproblems</em>.</p>
<p>Having become bored of manually trawling through all the tweets for each conference, I had a thought&hellip;</p>
<blockquote>
<p><em>&ldquo;Can&rsquo;t I write some code to do this for me?&quot;</em></p>
</blockquote>
<p>This didn&rsquo;t seem like too ridiculous an idea. Twitter has <a href="https://developer.twitter.com/en/docs/tweets/search/overview">an API</a>, which would allow me to retrieve all tweets for a conference hashtag. Once I had all the tweet photos, couldn&rsquo;t I run some magic AI algorithm over the images to tell me if I was in them? ü§î</p>
<p>After a couple of weeks of hacking around (and overcoming numerous challenges) I had (to my own amazement) managed to <a href="https://github.com/jthomas/findme">build a serverless application</a> which can find unlabelled photos of a person on twitter using machine learning with <a href="https://github.com/tensorflow/tfjs">TensorFlow.js</a>.</p>
<p><img src="/images/face-recog/find-me-demo.gif" alt="FindMe Example"></p>
<p><em>If you just want to try this application yourself, follow the instructions in the Github repo: <a href="https://github.com/jthomas/findme">https://github.com/jthomas/findme</a></em></p>
<h2 id="architecture">architecture</h2>
<p><img src="/images/face-recog/architecture.png" alt="FindMe Architecture Diagram"></p>
<p>This application has four <a href="https://github.com/jthomas/findme/blob/master/serverless.yml">serverless functions</a> (two API handlers and two backend services) and a client-side application from a static web page. Users log into the <a href="https://github.com/jthomas/findme/tree/master/public">client-side application</a> using Auth0 with their Twitter account. This provides the backend application with the user&rsquo;s profile image and Twitter API credentials.</p>
<p>When the user invokes a search query, the client-side application invokes the API endpoint for the <code>register_search</code> <a href="https://github.com/jthomas/findme/blob/master/schedule_search.js">function</a> with the query terms and twitter credentials. This function registers a new search job in Redis and fires a new <code>search_request</code> trigger event with the query and job id. This job identifier is returned to the client to poll for real-time status updates.</p>
<p>The <code>twitter_search</code> <a href="https://github.com/jthomas/findme/blob/master/twitter_search.js">function</a> is connected to the <code>search_request</code> trigger and invoked for each event. It uses the Twitter Search API to retrieve all tweets for the search terms. If tweets retrieved from the API contain photos, those tweet ids (with photo urls) are fired as new <code>tweet_image</code> trigger events.</p>
<p>The <code>compare_images</code> <a href="https://github.com/jthomas/findme/blob/master/compare_images.js">function</a> is connected to the <code>tweet_image</code> trigger. When invoked, it downloads the user&rsquo;s twitter profile image along with the tweet image and runs face detection against both images, using the <code>face-api.js</code> <a href="https://github.com/justadudewhohacks/face-api.js">library</a>. If any faces in the tweet photo match the face in the user&rsquo;s profile image, tweet ids are written to Redis before exiting.</p>
<p>The client-side web page polls for real-time search results by polling the API endpoint for the <code>search_status</code>  <a href="https://github.com/jthomas/findme/blob/master/search_status.js">function</a> with the search job id. Tweets with matching faces are displayed on the web page using the <a href="https://developer.twitter.com/en/docs/twitter-for-websites/embedded-tweets/overview">Twitter JS library</a>.</p>
<h2 id="challenges">challenges</h2>
<p>Since I had found an <a href="https://github.com/justadudewhohacks/face-api.js">NPM library to handle face detection</a>, I could just use this on a serverless platform by including the library within the zip file used to create my serverless application? Sounds easy, right?!</p>
<p><strong>ahem - not so faas-t&hellip;. ‚úã</strong></p>
<p>As discussed in <a href="http://jamesthom.as/blog/2018/08/13/serverless-machine-learning-with-tensorflow-dot-js/">previous blog posts</a>, there are numerous challenges in using TF.js-based libraries on serverless platforms. Starting with making the packages available in the runtime and loading model files to converting images for classification, these libraries are not like using normal NPM modules.</p>
<p><em>Here are the main challenges I had to overcome to make this serverless application work&hellip;</em></p>
<h3 id="using-tfjs-libraries-on-a-serverless-platform">using tf.js libraries on a serverless platform</h3>
<p>The <a href="https://github.com/tensorflow/tfjs-node">Node.js backend drivers</a> for TensorFlow.js use a native shared C++ library  (<code>libtensorflow.so</code>) to execute models on the CPU or GPU. This native dependency is compiled for the platform during the <code>npm install</code> process. The shared library file is around 142MB, which is too large to include in the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">deployment package</a> for most serverless platforms.</p>
<p>Normal workarounds for this issue store large dependencies in an object store. These files are dynamically retrieved during cold starts and stored in the runtime filesystem, as shown in this pseudo-code. This workaround does add an additional delay to cold start invocations.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">let</span> <span style="color:#a6e22e">cold_start</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">false</span>

<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">library</span> <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;libtensorflow.so&#39;</span>

<span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">cold_start</span>) {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">data</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">from_object_store</span>(<span style="color:#a6e22e">library</span>)
  <span style="color:#a6e22e">write_to_fs</span>(<span style="color:#a6e22e">library</span>, <span style="color:#a6e22e">data</span>)
  <span style="color:#a6e22e">cold_start</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>
}

<span style="color:#75715e">// rest of function code‚Ä¶
</span></code></pre></div><p><strong>Fortunately, I had a better solution using Apache OpenWhisk&rsquo;s support for custom Docker runtimes!</strong></p>
<p>This feature allows serverless applications to use <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md#creating-native-actions">custom Docker images</a> as the runtime environment. Creating custom images with <a href="http://jamesthom.as/blog/2017/08/04/large-applications-on-openwhisk/">large libraries pre-installed</a> means they can be excluded from deployment packages. üíØ</p>
<p>Apache OpenWhisk publishes all existing <a href="https://hub.docker.com/r/openwhisk/">runtime images</a> on Docker Hub. Using existing runtime images as base images means Dockerfiles for custom runtimes are minimal. Here&rsquo;s the Dockerfile needed to build a custom runtime with the TensorFlow.js Node.js backend drivers pre-installed.</p>
<pre><code>FROM openwhisk/action-nodejs-v8:latest

RUN npm install @tensorflow/tfjs-node
</code></pre><p>Once this image has been built and published on Dockerhub, you can use it when creating new functions.</p>
<p><em>I used this approach to build a <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/tags/">custom TensorFlow.js runtime</a> which is available on Docker Hub: <code>jamesthomas/action-nodejs-v8:tfjs-faceapi</code></em></p>
<p>OpenWhisk actions created using the <code>wsk</code> command-line use a configuration flag (<code>--docker</code>) to specify custom runtime images.</p>
<pre><code>wsk action create classify source.js --docker jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></pre><p>The OpenWhisk provider plugin for The Serverless Framework also supports <a href="https://github.com/serverless/serverless-openwhisk#custom-runtime-images">custom runtime images</a> through a configuration parameter (<code>image</code>) under the function configuration.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-yaml" data-lang="yaml"><span style="color:#66d9ef">service</span>: machine-learning

<span style="color:#66d9ef">provider</span>:
  <span style="color:#66d9ef">name</span>: openwhisk

<span style="color:#66d9ef">functions</span>:
  <span style="color:#66d9ef">classify</span>:
    <span style="color:#66d9ef">handler</span>: source.main
    <span style="color:#66d9ef">image</span>: jamesthomas/action-nodejs-v8:tfjs-faceapi
</code></pre></div><p>Having fixed the issue of library loading on serverless platforms, I could move onto the next problem, loading the pre-trained models&hellip; üíΩ</p>
<h3 id="loading-pre-trained-models">loading pre-trained models</h3>
<p>Running the <a href="https://github.com/justadudewhohacks/face-api.js#usage-loading-models">example code</a> to load the pre-trained models for face recognition gave me this error:</p>
<pre><code>ReferenceError: fetch is not defined
</code></pre><p>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I discovered how to manually load TensorFlow.js models from the filesystem using the <code>file://</code> URI prefix. Unfortunately, the <code>face-api.js</code> library doesn&rsquo;t support this feature. Models are <a href="https://github.com/justadudewhohacks/tfjs-image-recognition-base/blob/4a7d981dbb37e0d3dabc962e1cbfb6122e535263/src/dom/loadWeightMap.ts#L12">automatically loaded</a> using the <code>fetch</code> HTTP client. This HTTP client is available into modern browsers but not in the Node.js runtime.</p>
<p>Overcoming this issue relies on providing an instance of a compatible HTTP client in the runtime. The <code>node-fetch</code> library is a <a href="https://www.npmjs.com/package/node-fetch">implementation of the fetch client</a> API for the Node.js runtime. By manually installing this module and exporting as a global variable, the library can then use the HTTP client as expected.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#75715e">// Make HTTP client available in runtime
</span><span style="color:#75715e"></span><span style="color:#a6e22e">global</span>.<span style="color:#a6e22e">fetch</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">require</span>(<span style="color:#e6db74">&#39;node-fetch&#39;</span>)
</code></pre></div><p>Model configuration and weight files can then be loaded from the library&rsquo;s Github repository using this URL:</p>
<p><a href="https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/">https://raw.githubusercontent.com/justadudewhohacks/face-api.js/master/weights/</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-js" data-lang="js"><span style="color:#a6e22e">faceapi</span>.<span style="color:#a6e22e">loadFaceDetectionModel</span>(<span style="color:#e6db74">&#39;&lt;GITHUB_URL&gt;&#39;</span>)
</code></pre></div><h3 id="face-detection-in-images">face detection in images</h3>
<p>The <code>face-api.js</code> library has a <a href="https://github.com/justadudewhohacks/face-api.js#detecting-faces">utility function</a> (<code>models.allFaces</code>) to automatically detect and calculate descriptors for all faces found in an image. Descriptors are a feature vector (of 128 32-bit float values) which uniquely describes the characteristics of a persons face.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">results</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">await</span> <span style="color:#a6e22e">models</span>.<span style="color:#a6e22e">allFaces</span>(<span style="color:#a6e22e">input</span>, <span style="color:#a6e22e">minConfidence</span>)
</code></pre></div><p><em>The input to this function is the input tensor with the RGB values from an image. In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I explained how to convert an image from the filesystem in Node.js to the input tensor needed by the model.</em></p>
<p>Finding a user by comparing their twitter profile against photos from tweets starts by running face detection against both images. By comparing computed descriptor values, a measure of similarity can be established between faces from the images.</p>
<h3 id="face-comparison">face comparison</h3>
<p>Once the face descriptors have been calculated the library provides a utility function to compute the euclidian distance between two descriptors vectors. If the difference between two face descriptors is less than a threshold value, this is used to identify the same person in both images.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">distance</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">faceapi</span>.<span style="color:#a6e22e">euclideanDistance</span>(<span style="color:#a6e22e">descriptor1</span>, <span style="color:#a6e22e">descriptor2</span>)

<span style="color:#66d9ef">if</span> (<span style="color:#a6e22e">distance</span> <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0.6</span>)
  <span style="color:#a6e22e">console</span>.<span style="color:#a6e22e">log</span>(<span style="color:#e6db74">&#39;match&#39;</span>)
<span style="color:#66d9ef">else</span>
  <span style="color:#a6e22e">console</span>.<span style="color:#a6e22e">log</span>(<span style="color:#e6db74">&#39;no match&#39;</span>)
</code></pre></div><p>I&rsquo;ve no idea why 0.6 is chosen as the threshold value but this seemed to work for me! Even small changes to this value dramatically reduced the precision and recall rates for my test data. I&rsquo;m calling it the Goldilocks value, just use it&hellip;</p>
<h2 id="performance">performance</h2>
<p>Once I had the end to end application working, I wanted to make it was fast as possible. By optimising the performance, I could improve the application responsiveness and reduce compute costs for my backend. Time is literally money with serverless platforms.</p>
<h3 id="baseline-performance">baseline performance</h3>
<p>Before attempting to optimise my application, I needed to understand the baseline performance. Setting up experiments to record invocation durations gave me the following average test results.</p>
<ul>
<li><em>Warm invocations</em>: ~5 seconds</li>
<li><em>Cold invocations</em>: ~8 seconds</li>
</ul>
<p>Instrumenting the code with <code>console.time</code> statements revealed execution time was comprised of five main sections.</p>
<table>
<thead>
<tr>
<th>Cold Starts</th>
<th>Warm Starts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation</td>
<td>1200 ms</td>
</tr>
<tr>
<td>Model Loading</td>
<td>3200 ms</td>
</tr>
<tr>
<td>Image Loading</td>
<td>500 ms x 2</td>
</tr>
<tr>
<td>Face Detection</td>
<td>700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else</td>
<td>1000 ms</td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td><strong>~ 8 seconds</strong></td>
</tr>
</tbody>
</table>
<p><em>Initialisation</em> was the delay during cold starts to create the runtime environment and load all the library files and application code. <em>Model Loading</em> recorded the time spent instantiating the TF.js models from the source files. <em>Image Loading</em> was the time spent converting the RGB values from images into input tensors, this happened twice, once for the twitter profile picture and again for the tweet photo. <em>Face Detection</em> is the elapsed time to execute the <code>models.allFaces</code> method and <code>faceapi.euclideanDistance</code> methods for all the detected faces. <em>Everything else</em> is well&hellip; everything else.</p>
<p>Since model loading was the largest section, this seemed like an obvious place to start optimising. üìàüìâ</p>
<h3 id="loading-model-files-from-disk">loading model files from disk</h3>
<p>Overcoming the initial model loading issue relied on manually exposing the expected HTTP client in the Node.js runtime. This allowed models to be dynamically loaded (over HTTP) from the external Github repository. Models files were about 36MB.</p>
<p>My first idea was to load these model files from the filesystem, which should be much faster than downloading from Github. Since I was already building a custom Docker runtime, it was a one-line change to include the model files within the runtime filesystem.</p>
<pre><code>FROM openwhisk/action-nodejs-v8:latest

RUN npm install @tensorflow/tfjs-node

COPY weights weights
</code></pre><p>Having re-built the image and pushed to Docker Hub, the classification function&rsquo;s runtime environment now included models files in the filesystem.</p>
<p><strong>But how do we make the <code>face-api.js</code> library load models files from the filesystem when it is using a HTTP client?</strong></p>
<p>My solution was to write a <code>fetch</code> client that proxied calls to retrieve files from a HTTP endpoint to the local filesystem. üò± I&rsquo;d let you decide whether this is a brilliant or terrible idea!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#a6e22e">global</span>.<span style="color:#a6e22e">fetch</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">async</span> (<span style="color:#a6e22e">file</span>) =&gt; {
  <span style="color:#66d9ef">return</span> {
    <span style="color:#a6e22e">json</span><span style="color:#f92672">:</span> () =&gt; <span style="color:#a6e22e">JSON</span>.<span style="color:#a6e22e">parse</span>(<span style="color:#a6e22e">fs</span>.<span style="color:#a6e22e">readFileSync</span>(<span style="color:#a6e22e">file</span>, <span style="color:#e6db74">&#39;utf8&#39;</span>)),
    <span style="color:#a6e22e">arrayBuffer</span><span style="color:#f92672">:</span> () =&gt; <span style="color:#a6e22e">fs</span>.<span style="color:#a6e22e">readFileSync</span>(<span style="color:#a6e22e">file</span>)
  }
}

<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">model</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">await</span> <span style="color:#a6e22e">models</span>.<span style="color:#a6e22e">load</span>(<span style="color:#e6db74">&#39;/weights&#39;</span>)
</code></pre></div><p>The <code>face-api.js</code> library only used two methods (<code>json()</code> &amp; <code>arrayBuffer()</code>) from the HTTP client. Stubbing out these methods to proxy <code>fs.readFileSync</code> meant files paths were loaded from the filesystem. Amazingly, this seemed to just work, hurrah!</p>
<p><strong>Implementing this feature and re-running performance tests revealed this optimisation saved about 500 ms from the Model Loading section.</strong></p>
<table>
<thead>
<tr>
<th>Cold Starts</th>
<th>Warm Starts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation</td>
<td>1200 ms</td>
</tr>
<tr>
<td>Model Loading</td>
<td>2700 ms</td>
</tr>
<tr>
<td>Image Loading</td>
<td>500 ms x 2</td>
</tr>
<tr>
<td>Face Detection</td>
<td>700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else</td>
<td>1000 ms</td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td><strong>~ 7.5 seconds</strong></td>
</tr>
</tbody>
</table>
<p>This was less of an improvement than I&rsquo;d expected. Parsing all the model files and instantiating the internal objects was more computationally intensive than I realised. This performance improvement did improve both cold and warm invocations, which was a bonus.</p>
<p><em>Despite this optimisation, model loading was still the largest section in the classification function&hellip;</em></p>
<h3 id="caching-loaded-models">caching loaded models</h3>
<p>There&rsquo;s a good strategy to use when optimising serverless functions&hellip;</p>
<p><img src="/images/face-recog/cache-all-the-things.jpg" alt="CACHE ALL THE THINGS"></p>
<p>Serverless runtimes re-use runtime containers for consecutive requests, known as warm environments. Using local state, like global variables or the runtime filesystem, to cache data between requests can be used to improve performance during those invocations.</p>
<p>Since model loading was such an expensive process, I wanted to cache initialised models. Using a global variable, I could control whether to trigger model loading or return the pre-loaded models. Warm environments would re-use pre-loaded models and remove model loading delay.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">faceapi</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">require</span>(<span style="color:#e6db74">&#39;face-api.js&#39;</span>)

<span style="color:#66d9ef">let</span> <span style="color:#a6e22e">LOADED</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">false</span>

<span style="color:#a6e22e">exports</span>.<span style="color:#a6e22e">load</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">async</span> <span style="color:#a6e22e">location</span> =&gt; {
  <span style="color:#66d9ef">if</span> (<span style="color:#f92672">!</span><span style="color:#a6e22e">LOADED</span>) {
    <span style="color:#a6e22e">await</span> <span style="color:#a6e22e">faceapi</span>.<span style="color:#a6e22e">loadFaceDetectionModel</span>(<span style="color:#a6e22e">location</span>)
    <span style="color:#a6e22e">await</span> <span style="color:#a6e22e">faceapi</span>.<span style="color:#a6e22e">loadFaceRecognitionModel</span>(<span style="color:#a6e22e">location</span>)
    <span style="color:#a6e22e">await</span> <span style="color:#a6e22e">faceapi</span>.<span style="color:#a6e22e">loadFaceLandmarkModel</span>(<span style="color:#a6e22e">location</span>)

    <span style="color:#a6e22e">LOADED</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">true</span>
  }

  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">faceapi</span>
}
</code></pre></div><p><strong>This performance improvement had a significant impact of the performance for warm invocations. Model loading became &ldquo;free&rdquo;.</strong> üëç</p>
<table>
<thead>
<tr>
<th>Cold Starts</th>
<th>Warm Starts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation</td>
<td>1200 ms</td>
</tr>
<tr>
<td>Model Loading</td>
<td>2700 ms</td>
</tr>
<tr>
<td>Image Loading</td>
<td>500 ms x 2</td>
</tr>
<tr>
<td>Face Detection</td>
<td>700 ms - 900 ms x 2</td>
</tr>
<tr>
<td>Everything Else</td>
<td>1000 ms</td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td><strong>~ 7.5 seconds</strong></td>
</tr>
</tbody>
</table>
<h3 id="caching-face-descriptors">caching face descriptors</h3>
<p>In the initial implementation, the face comparison function was executing face detection against both the user&rsquo;s twitter profile image and tweet photo for comparison. Since the twitter profile image was the same in each search request, running face detection against this image would always return the same results.</p>
<p>Rather than having this work being redundantly computed in each function, caching the results of the computed face descriptor for the profile image meant it could re-used across invocations. This would reduce by 50% the work necessary in the Image &amp; Model Loading sections.</p>
<p>The <code>face-api.js</code> library returns the face descriptor as a typed array with 128 32-bit float values. Encoding this values as a hex string allows them to be stored and retrieved from Redis. This code was used to convert float values to hex strings, whilst maintaining the exact precision of those float values.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-javascript" data-lang="javascript"><span style="color:#66d9ef">const</span> <span style="color:#a6e22e">encode</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">typearr</span> =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">encoded</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">Buffer</span>.<span style="color:#a6e22e">from</span>(<span style="color:#a6e22e">typearr</span>.<span style="color:#a6e22e">buffer</span>).<span style="color:#a6e22e">toString</span>(<span style="color:#e6db74">&#39;hex&#39;</span>)  
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">encoded</span>
}

<span style="color:#66d9ef">const</span> <span style="color:#a6e22e">decode</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">encoded</span> =&gt; {
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">decoded</span> <span style="color:#f92672">=</span> <span style="color:#a6e22e">Buffer</span>.<span style="color:#a6e22e">from</span>(<span style="color:#a6e22e">encoded</span>, <span style="color:#e6db74">&#39;hex&#39;</span>)
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">uints</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#a6e22e">Uint8Array</span>(<span style="color:#a6e22e">decoded</span>)
  <span style="color:#66d9ef">const</span> <span style="color:#a6e22e">floats</span> <span style="color:#f92672">=</span> <span style="color:#66d9ef">new</span> <span style="color:#a6e22e">Float32Array</span>(<span style="color:#a6e22e">uints</span>.<span style="color:#a6e22e">buffer</span>)
  <span style="color:#66d9ef">return</span> <span style="color:#a6e22e">floats</span>
}
</code></pre></div><p>This optimisation improves the performance of most cold invocations and all warm invocations, removing over 1200 ms of computation time to compute the results.</p>
<table>
<thead>
<tr>
<th></th>
<th align="center">Cold Starts (Cached)</th>
<th align="center">Warm Starts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation</td>
<td align="center">1200 ms</td>
<td align="center">0 ms</td>
</tr>
<tr>
<td>Model Loading</td>
<td align="center">2700 ms</td>
<td align="center">1500 ms</td>
</tr>
<tr>
<td>Image Loading</td>
<td align="center">500 ms</td>
<td align="center">500 ms</td>
</tr>
<tr>
<td>Face Detection</td>
<td align="center">700 ms - 900 ms</td>
<td align="center">700 ms - 900 ms</td>
</tr>
<tr>
<td>Everything Else</td>
<td align="center">1000 ms</td>
<td align="center">500 ms</td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td align="center"><strong>~ 6 seconds</strong></td>
<td align="center"><strong>~ 2.5 seconds</strong></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Cold Starts</th>
<th>Warm Starts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Initialisation</td>
<td>1200 ms</td>
</tr>
<tr>
<td>Model Loading</td>
<td>2700 ms</td>
</tr>
<tr>
<td>Image Loading</td>
<td>500 ms</td>
</tr>
<tr>
<td>Face Detection</td>
<td>700 ms - 900 ms</td>
</tr>
<tr>
<td>Everything Else</td>
<td>1000 ms</td>
</tr>
<tr>
<td><strong>Total Duration</strong></td>
<td><strong>~ 7.5 seconds</strong></td>
</tr>
</tbody>
</table>
<h3 id="final-results--cost">final results + cost</h3>
<p>Application performance was massively improved with all these optimisations. As demonstrated in the video above, the application could process tweets in real-time, returning almost instant results. Average invocation durations were now.</p>
<ul>
<li><em>Warm invocations</em>: ~2.5 seconds</li>
<li><em>Cold invocations (Cached)</em>: ~6 seconds</li>
</ul>
<p>Serverless platforms charge for compute time by the millisecond, so these improvements led to cost savings of 25% for cold invocations (apart the first classification for a user) and 50% for warm invocations.</p>
<p>Classification functions used 512MB of RAM which meant IBM Cloud Functions would provide 320,000 &ldquo;warm&rdquo; classifications or 133,333 &ldquo;cold&rdquo; classifications within the free tier each month. Ignoring the free tier, 100,000 &ldquo;warm&rdquo; classifications would cost $5.10 and 100,000 &ldquo;cold&rdquo; classifications $2.13.</p>
<h2 id="conclusion">conclusion</h2>
<p>Using TensorFlow.js with serverless cloud platforms makes it easy to build scalable machine learning applications in the cloud. Using the horizontal scaling capabilities of serverless platforms, thousands of model classifications can be ran in parallel. This can be more performant than having dedicated hardware with a GPU, especially with compute costs for serverless applications being so cheap.</p>
<p>TensorFlow.js is ideally suited to serverless application due to the JS interface, (relatively) small library size and availability of pre-trained models. Despite having no prior experience in Machine Learning, I was able to use the library to build a face recognition pipeline, processing 100s of images in parallel, for real-time results. This amazing library opens up machine learning to a whole new audience!</p>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/2018/08/serverless-machine-learning-with-tensorflow.js/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Serverless Machine Learning With TensorFlow.js</span>
    </a>
    
    
    <a href="/2018/12/using-custom-domains-with-ibm-cloud-functions/" class="navigation-next">
      <span class="navigation-tittle">Using Custom Domains With IBM Cloud Functions</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>
<script data-goatcounter="https://jamesthomas.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


    
    
    <script type="text/javascript">
        

    </script>
    




    



    </body>
</html>
