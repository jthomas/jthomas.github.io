<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.74.3" />

    
    
    

<title>Serverless Machine Learning With TensorFlow.js • notes on software.</title>
<meta name="description" content="James Thomas&#39; blog about software. Posts about serverless, cloud platforms, openwhisk, node.js and more... Founder of JT Consulting Services.">
<meta name="keywords" content="blog,serverless,openwhisk,ibm,nodejs,tensorflow,serverless framework,open-source,conferences,developer advocacy,github">
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Serverless Machine Learning With TensorFlow.js"/>
<meta name="twitter:description" content="Using TensorFlow.js for Machine Learning on Serverless Cloud Platforms. Use serverless Node.js functions for visual recognition on image files using IBM Cloud Functions (Apache OpenWhisk)."/>

<meta property="og:title" content="Serverless Machine Learning With TensorFlow.js" />
<meta property="og:description" content="Using TensorFlow.js for Machine Learning on Serverless Cloud Platforms. Use serverless Node.js functions for visual recognition on image files using IBM Cloud Functions (Apache OpenWhisk)." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jamesthom.as/2018/08/serverless-machine-learning-with-tensorflow.js/" />
<meta property="article:published_time" content="2018-08-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-08-13T00:00:00+00:00" />


    






<link rel="stylesheet" href="/scss/hyde-hyde.3081c4981fb69a2783dd36ecfdd0e6ba7a158d4cbfdd290ebce8f78ba0469fc6.css" integrity="sha256-MIHEmB&#43;2mieD3Tbs/dDmunoVjUy/3SkOvOj3i6BGn8Y=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
    <style type="text/css">
.content {
  max-width: 48rem;
}

.sidebar .container {
  padding-right: 0rem;
  padding-left: 0rem;
}

h1 {
  margin-top: 0;
}
</style>

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://jamesthom.as/">notes on software.</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://jamesthom.as/profile_new.png" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
         by james thomas serverless aficionado. 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">notes on software.</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="mailto:consultancy@jamesthom.as">
						<span>Hire Me?</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://twitter.com/thomasj" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://github.com/jthomas" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	<a href="https://stackoverflow.com/users/1427084" rel="me"><i class="fab fa-stack-overflow fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://medium.com/@jamesthom.as" rel="me"><i class="fab fa-medium fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
        
	<a href="https://www.youtube.com/user/jthomasuk" rel="me"><i class="fab fa-youtube fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="mailto:blog@jamesthom.as" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    


  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Serverless Machine Learning With TensorFlow.js</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Aug 13, 2018
    
    
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/serverless">serverless</a>
           
      
          <a class="badge badge-tag" href="/tags/openwhisk">openwhisk</a>
           
      
          <a class="badge badge-tag" href="/tags/node.js">node.js</a>
           
      
          <a class="badge badge-tag" href="/tags/machine-learning">machine learning</a>
           
      
          <a class="badge badge-tag" href="/tags/tensorflow">tensorflow</a>
           
      
          <a class="badge badge-tag" href="/tags/javascript">javascript</a>
          
      
    
    
    <br/>
    
    <i class="fas fa-clock"></i> 9 min read
    
</div>


  </header>
  
  
  <div class="post">
    <p>In a <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to use <a href="https://js.tensorflow.org/">TensorFlow.js</a> on Node.js to run <a href="https://gist.github.com/jthomas/145610bdeda2638d94fab9a397eb1f1d#file-script-js">visual recognition on images from the local filesystem</a>. TensorFlow.js is a JavaScript version of the open-source machine learning library from Google.</p>
<p>Once I had this working with a local Node.js script, my next idea was to convert it into a serverless function. Running this function on <a href="https://console.bluemix.net/openwhisk/">IBM Cloud Functions</a> (<a href="https://openwhisk.incubator.apache.org/">Apache OpenWhisk</a>) would turn the script into my own visual recognition microservice.</p>
<p>![ Function]( /images/tfjs-serverless/tf-js-example.gif Serverless TensorFlow.js)</p>
<p>Sounds easy, right? It&rsquo;s just a JavaScript library? So, zip it up and away we go&hellip; <em><strong>ahem</strong></em> 👊</p>
<p><em>Converting the image classification script to run in a serverless environment had the following challenges&hellip;</em></p>
<ul>
<li><strong>TensorFlow.js libraries need to be available in the runtime.</strong></li>
<li><strong>Native bindings for the library must be compiled against the platform architecture.</strong></li>
<li><strong>Models files need to be loaded from the filesystem.</strong></li>
</ul>
<p>Some of these issues were more challenging than others to fix! Let&rsquo;s start by looking at the details of each issue, before explaining how <a href="http://jamesthom.as/blog/2017/01/16/openwhisk-docker-actions/">Docker support</a> in Apache OpenWhisk can be used to resolve them all.</p>
<h2 id="challenges">Challenges</h2>
<h3 id="tensorflowjs-libraries">TensorFlow.js Libraries</h3>
<p>TensorFlow.js libraries are not included in the <a href="https://github.com/apache/incubator-openwhisk-runtime-nodejs">Node.js runtimes</a> provided by the Apache OpenWhisk.</p>
<p>External libraries <a href="http://jamesthom.as/blog/2016/11/28/npm-modules-in-openwhisk/">can be imported</a> into the runtime by deploying applications from a zip file. Custom <code>node_modules</code> folders included in the zip file will be extracted in the runtime. Zip files are limited to a <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/reference.md#actions">maximum size of 48MB</a>.</p>
<h4 id="library-size">Library Size</h4>
<p>Running <code>npm install</code> for the TensorFlow.js libraries used revealed the first problem&hellip; the resulting <code>node_modules</code> directory was 175MB. 😱</p>
<p>Looking at the contents of this folder, the <code>tfjs-node</code> module compiles a <a href="https://github.com/tensorflow/tfjs-node/tree/master/src">native shared library</a> (<code>libtensorflow.so</code>) that is 135M. This means no amount of JavaScript minification is going to get those external dependencies under the magic 48 MB limit. 👎</p>
<h4 id="native-dependencies">Native Dependencies</h4>
<p>The <code>libtensorflow.so</code> native shared library must be compiled using the platform runtime. Running <code>npm install</code>  locally automatically compiles native dependencies against the host platform. Local environments may use different CPU architectures (Mac vs Linux) or link against shared libraries not available in the serverless runtime.</p>
<h3 id="mobilenet-model-files">MobileNet Model Files</h3>
<p>TensorFlow models files <a href="https://js.tensorflow.org/tutorials/model-save-load.html">need loading from the filesystem</a> in Node.js. Serverless runtimes do provide a temporary filesystem inside the runtime environment. Files from deployment zip files are automatically extracted into this environment before invocations. There is no external access to this filesystem outside the lifecycle of the serverless function.</p>
<p>Models files for the MobileNet model were 16MB. If these files are included in the deployment package, it leaves 32MB for the rest of the application source code. Although the model files are small enough to include in the zip file, what about the TensorFlow.js libraries? Is this the end of the blog post? Not so fast&hellip;.</p>
<p><strong>Apache OpenWhisk&rsquo;s support for custom runtimes provides a simple solution to all these issues!</strong></p>
<h2 id="custom-runtimes">Custom Runtimes</h2>
<p>Apache OpenWhisk uses Docker containers as the runtime environments for serverless functions (actions). All platform runtime images are <a href="https://hub.docker.com/r/openwhisk/">published on Docker Hub</a>, allowing developers to start these environments locally.</p>
<p>Developers can also <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-docker.md">specify custom runtime images</a> when creating actions. These images must be publicly available on Docker Hub. Custom runtimes have to expose the <a href="https://github.com/apache/incubator-openwhisk/blob/master/docs/actions-new.md#action-interface">same HTTP API</a> used by the platform for invoking actions.</p>
<p>Using platform runtime images as <a href="https://docs.docker.com/glossary/?term=parent%20image">parent images</a> makes it simple to build custom runtimes. Users can run commands during the Docker build to install additional libraries and other dependencies. The parent image already contains source files with the HTTP API service handling platform requests.</p>
<h3 id="tensorflowjs-runtime">TensorFlow.js Runtime</h3>
<p>Here is the Docker build file for the Node.js action runtime with additional TensorFlow.js dependencies.</p>
<pre><code>FROM openwhisk/action-nodejs-v8:latest

RUN npm install @tensorflow/tfjs @tensorflow-models/mobilenet @tensorflow/tfjs-node jpeg-js

COPY mobilenet mobilenet
</code></pre><p><code>openwhisk/action-nodejs-v8:latest</code> is the Node.js action runtime image <a href="https://hub.docker.com/r/openwhisk/action-nodejs-v8/">published by OpenWhisk</a>.</p>
<p>TensorFlow libraries and other dependencies are installed using <code>npm install</code> in the build process. Native dependencies for the <code>@tensorflow/tfjs-node</code> library are automatically compiled for the correct platform by installing during the build process.</p>
<p>Since I&rsquo;m building a new runtime, I&rsquo;ve also added the <a href="https://github.com/tensorflow/tfjs-models/tree/master/mobilenet">MobileNet model files</a> to the image. Whilst not strictly necessary, removing them from the action zip file reduces deployment times.</p>
<p><em><strong>Want to skip the next step? Use this image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/"><code>jamesthomas/action-nodejs-v8:tfjs</code></a> rather than building your own.</strong></em></p>
<h3 id="building-the-runtime">Building The Runtime</h3>
<p><em>In the <a href="http://jamesthom.as/blog/2018/08/07/machine-learning-in-node-dot-js-with-tensorflow-dot-js/">previous blog post</a>, I showed how to download model files from the public storage bucket.</em></p>
<ul>
<li>Download a version of the MobileNet model and place all files in the <code>mobilenet</code> directory.</li>
<li>Copy the Docker build file from above to a local file named <code>Dockerfile</code>.</li>
<li>Run the Docker <a href="https://docs.docker.com/engine/reference/commandline/build/">build command</a> to generate a local image.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">docker build -t tfjs .
</code></pre></div><ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/tag/">Tag the local image</a> with a remote username and repository.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">docker tag tfjs &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></pre></div><p><em>Replace <code>&lt;USERNAME&gt;</code> with your Docker Hub username.</em></p>
<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/push/">Push the local image</a> to Docker Hub</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"> docker push &lt;USERNAME&gt;/action-nodejs-v8:tfjs
</code></pre></div><p>Once the image <a href="https://hub.docker.com/r/jamesthomas/action-nodejs-v8/">is available</a> on Docker Hub, actions can be created using that runtime image. 😎</p>
<h2 id="example-code">Example Code</h2>
<p>This source code implements image classification as an OpenWhisk action. Image files are provided as a Base64 encoded string using the <code>image</code> property on the event parameters. Classification results are returned as the <code>results</code> property in the response.</p>
<!-- raw HTML omitted -->
<h3 id="caching-loaded-models">Caching Loaded Models</h3>
<p>Serverless platforms initialise runtime environments on-demand to handle invocations. Once a runtime environment has been created, it will be <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">re-used for further invocations</a> with some limits. This improves performance by removing the initialisation delay (&ldquo;cold start&rdquo;) from request processing.</p>
<p>Applications can exploit this behaviour by using global variables to maintain state across requests. This is often use to <a href="https://blog.rowanudell.com/database-connections-in-lambda/">cache opened database connections</a> or store initialisation data loaded from external systems.</p>
<p>I have used this pattern to <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L80-L82">cache the MobileNet model</a> used for classification. During cold invocations, the model is loaded from the filesystem and stored in a global variable. Warm invocations then use the existence of that global variable to skip the model loading process with further requests.</p>
<p>Caching the model reduces the time (and therefore cost) for classifications on warm invocations.</p>
<h3 id="memory-leak">Memory Leak</h3>
<p>Running the Node.js script from blog post on IBM Cloud Functions was possible with minimal modifications. Unfortunately, performance testing revealed a memory leak in the handler function. 😢</p>
<p><em>Reading more about <a href="https://js.tensorflow.org/tutorials/core-concepts.html">how TensorFlow.js works</a> on Node.js uncovered the issue&hellip;</em></p>
<p>TensorFlow.js&rsquo;s Node.js extensions use a native C++ library to execute the Tensors on a CPU or GPU engine. Memory allocated for Tensor objects in the native library is retained until the application explicitly releases it or the process exits. TensorFlow.js provides a <code>dispose</code> method on the individual objects to free allocated memory. There is also a <code>tf.tidy</code> method to automatically clean up all allocated objects within a frame.</p>
<p>Reviewing the code, tensors were being created as <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L51-L59">model input from images</a> on each request. These objects were not disposed before returning from the request handler. This meant native memory grew unbounded. Adding an explicit <code>dispose</code> call to free these objects before returning <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L91">fixed the issue</a>.</p>
<h3 id="profiling--performance">Profiling &amp; Performance</h3>
<p>Action code records memory usage and elapsed time at different stages in classification process.</p>
<p>Recording <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L12-L20">memory usage</a> allows me to modify the maximum memory allocated to the function for optimal performance and cost. Node.js provides a <a href="https://nodejs.org/docs/v0.4.11/api/all.html#process.memoryUsage">standard library API</a> to retrieve memory usage for the current process. Logging these values allows me to inspect memory usage at different stages.</p>
<p>Timing <a href="https://gist.github.com/jthomas/e7c78bbfe4091ed6ace93d1b53cbf6e5#file-index-js-L71">different tasks</a> in the classification process, i.e. model loading, image classification, gives me an insight into how efficient classification is compared to other methods. Node.js has a <a href="https://nodejs.org/api/console.html#console_console_time_label">standard library API</a> for timers to record and print elapsed time to the console.</p>
<h2 id="demo">Demo</h2>
<h3 id="deploy-action">Deploy Action</h3>
<ul>
<li>Run the following command with the <a href="https://console.bluemix.net/openwhisk/learn/cli">IBM Cloud CLI</a> to create the action.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">ibmcloud fn action create classify --docker &lt;IMAGE_NAME&gt; index.js
</code></pre></div><p><em>Replace <code>&lt;IMAGE_NAME&gt;</code> with the public Docker Hub image identifier for the custom runtime. Use <code>jamesthomas/action-nodejs-v8:tfjs</code> if you haven&rsquo;t built this manually.</em></p>
<h3 id="testing-it-out">Testing It Out</h3>
<ul>
<li>Download <a href="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG">this image</a> of a Panda from Wikipedia.</li>
</ul>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/f/fe/Giant_Panda_in_Beijing_Zoo_1.JPG" alt=""></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">wget http://bit.ly/2JYSal9 -O panda.jpg
</code></pre></div><ul>
<li>Invoke the action with the Base64 encoded image as an input parameter.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh"> ibmcloud fn action invoke classify -r -p image <span style="color:#66d9ef">$(</span>base64 panda.jpg<span style="color:#66d9ef">)</span>
</code></pre></div><ul>
<li>Returned JSON message contains classification probabilities. 🐼🐼🐼</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-json" data-lang="json">{
  <span style="color:#f92672">&#34;results&#34;</span>:  [{
    <span style="color:#960050;background-color:#1e0010">className:</span> <span style="color:#960050;background-color:#1e0010">&#39;giant</span> <span style="color:#960050;background-color:#1e0010">panda,</span> <span style="color:#960050;background-color:#1e0010">panda,</span> <span style="color:#960050;background-color:#1e0010">panda</span> <span style="color:#960050;background-color:#1e0010">bear,</span> <span style="color:#960050;background-color:#1e0010">coon</span> <span style="color:#960050;background-color:#1e0010">bear&#39;,</span>
    <span style="color:#960050;background-color:#1e0010">probability:</span> <span style="color:#960050;background-color:#1e0010">0.9993536472320557</span>
  }]
}
</code></pre></div><h3 id="activation-details">Activation Details</h3>
<ul>
<li>Retrieve logging output for the last activation to show performance data.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">ibmcloud fn activation logs --last
</code></pre></div><p><em><strong>Profiling and memory usage details are logged to stdout</strong></em></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">prediction <span style="color:#66d9ef">function</span> called.
memory used: rss<span style="color:#f92672">=</span>150.46 MB, heapTotal<span style="color:#f92672">=</span>32.83 MB, heapUsed<span style="color:#f92672">=</span>20.29 MB, external<span style="color:#f92672">=</span>67.6 MB
loading image and model...
decodeImage: 74.233ms
memory used: rss<span style="color:#f92672">=</span>141.8 MB, heapTotal<span style="color:#f92672">=</span>24.33 MB, heapUsed<span style="color:#f92672">=</span>19.05 MB, external<span style="color:#f92672">=</span>40.63 MB
imageByteArray: 5.676ms
memory used: rss<span style="color:#f92672">=</span>141.8 MB, heapTotal<span style="color:#f92672">=</span>24.33 MB, heapUsed<span style="color:#f92672">=</span>19.05 MB, external<span style="color:#f92672">=</span>45.51 MB
imageToInput: 5.952ms
memory used: rss<span style="color:#f92672">=</span>141.8 MB, heapTotal<span style="color:#f92672">=</span>24.33 MB, heapUsed<span style="color:#f92672">=</span>19.06 MB, external<span style="color:#f92672">=</span>45.51 MB
mn_model.classify: 274.805ms
memory used: rss<span style="color:#f92672">=</span>149.83 MB, heapTotal<span style="color:#f92672">=</span>24.33 MB, heapUsed<span style="color:#f92672">=</span>20.57 MB, external<span style="color:#f92672">=</span>45.51 MB
classification results: <span style="color:#f92672">[</span>...<span style="color:#f92672">]</span>
main: 356.639ms
memory used: rss<span style="color:#f92672">=</span>144.37 MB, heapTotal<span style="color:#f92672">=</span>24.33 MB, heapUsed<span style="color:#f92672">=</span>20.58 MB, external<span style="color:#f92672">=</span>45.51 MB
</code></pre></div><p><code>main</code> is the total elapsed time for the action handler. <code>mn_model.classify</code> is the elapsed time for the image classification. Cold start requests print an extra log message with model loading time, <code>loadModel: 394.547ms</code>.</p>
<h2 id="performance-results">Performance Results</h2>
<p>Invoking the <code>classify</code> action 1000 times for both cold and warm activations (using 256MB memory) generated the following performance results.</p>
<h3 id="warm-invocations">warm invocations</h3>
<p><img src="/images/tfjs-serverless/warm-activations.png" alt=" Warm Activation Performance Results"></p>
<p>Classifications took an average of <strong>316 milliseconds to process when using warm environments</strong>. Looking at the timing data, converting the Base64 encoded JPEG into the input tensor took around 100 milliseconds. Running the model classification task was in the 200 - 250 milliseconds range.</p>
<h3 id="cold-invocations">cold invocations</h3>
<p><img src="/images/tfjs-serverless/cold-activations.png" alt=" Cold Activation Performance Results"></p>
<p>Classifications took an average of <strong>1260 milliseconds to process when using cold environments</strong>. These requests incur penalties for initialising new runtime containers and loading models from the filesystem. Both of these tasks took around 400 milliseconds each.</p>
<p><em>One disadvantage of using custom runtime images in Apache OpenWhisk is the lack of <a href="https://medium.com/openwhisk/squeezing-the-milliseconds-how-to-make-serverless-platforms-blazing-fast-aea0e9951bd0">pre-warmed containers</a>. Pre-warming is used to reduce cold start times by starting runtime containers before they are needed. This is not supported for non-standard runtime images.</em></p>
<h3 id="classification-cost">classification cost</h3>
<p>IBM Cloud Functions <a href="https://console.bluemix.net/openwhisk/learn/pricing">provides a free tier</a> of 400,000 GB/s per month. Each further second of execution is charged at $0.000017 per GB of memory allocated. Execution time is rounded up to the nearest 100ms.</p>
<p>If all activations were warm, a user could execute <strong>more than 4,000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 600,000 further invocations would cost just over $1.</p>
<p>If all activations were cold, a user could execute <strong>more than 1,2000,000 classifications per month in the free tier</strong> using an action with 256MB. Once outside the free tier, around 180,000 further invocations would cost just over $1.</p>
<h2 id="conclusion">Conclusion</h2>
<p>TensorFlow.js brings the power of deep learning to JavaScript developers. Using pre-trained models with the TensorFlow.js library makes it simple to extend JavaScript applications with complex machine learning tasks with minimal effort and code.</p>
<p>Getting a local script to run image classification was relatively simple, but converting to a serverless function came with more challenges! Apache OpenWhisk restricts the maximum application size to 50MB and native libraries dependencies were much larger than this limit.</p>
<p>Fortunately, Apache OpenWhisk&rsquo;s custom runtime support allowed us to resolve all these issues. By building a custom runtime with native dependencies and models files, those libraries can be used on the platform without including them in the deployment package.</p>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/2018/08/machine-learning-in-node.js-with-tensorflow.js/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Machine Learning In Node.js With TensorFlow.js</span>
    </a>
    
    
    <a href="/2018/10/finding-photos-on-twitter-using-face-recognition-with-tensorflow.js/" class="navigation-next">
      <span class="navigation-tittle">Finding photos on Twitter using face recognition with TensorFlow.js</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>
<script data-goatcounter="https://jamesthomas.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>




    



    </body>
</html>
